{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27951\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/proeng/thermo/mixed_split.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...</td>\n",
       "      <td>37.962947</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...</td>\n",
       "      <td>54.425342</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MRICFLLLAFLVAETFANELTRCCAGGTRHFKNSNTCSSIKSEGTS...</td>\n",
       "      <td>49.459216</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIRVALPTTASAIPRSISTSPGETISKNHEEEVKRVWRKADAVCFD...</td>\n",
       "      <td>42.593131</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNGDWSRAFVLSKVKNLYFFVIIDKGFSAILNDPREPVQVGGFFEV...</td>\n",
       "      <td>37.999478</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence     target    set  \\\n",
       "0  MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...  37.962947  train   \n",
       "1  MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...  54.425342  train   \n",
       "2  MRICFLLLAFLVAETFANELTRCCAGGTRHFKNSNTCSSIKSEGTS...  49.459216  train   \n",
       "3  MIRVALPTTASAIPRSISTSPGETISKNHEEEVKRVWRKADAVCFD...  42.593131  train   \n",
       "4  MNGDWSRAFVLSKVKNLYFFVIIDKGFSAILNDPREPVQVGGFFEV...  37.999478  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for running pytorch models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent import futures\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "\n",
    "from scr.preprocess.data_process import split_protrain_loader\n",
    "from scr.encoding.encoding_classes import get_emb_info, ESMEncoder, CARPEncoder\n",
    "from scr.model.pytorch_model import LinearRegression\n",
    "from scr.model.train_test import train, test\n",
    "from scr.vis.learning_vis import plot_lc\n",
    "from scr.utils import get_folder_file_names, pickle_save, get_default_output_path\n",
    "\n",
    "\n",
    "class Run_Pytorch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        loader_batch_size: int = 64,\n",
    "        worker_seed: int = RAND_SEED,\n",
    "        if_encode_all: bool = True,\n",
    "        learning_rate: float = 1e-4,\n",
    "        lr_decay: float = 0.1,\n",
    "        epochs: int = 100,\n",
    "        early_stop: bool = True,\n",
    "        tolerance: int = 10,\n",
    "        min_epoch: int = 5,\n",
    "        device: torch.device | str = DEVICE,\n",
    "        all_plot_folder: str = \"results/learning_curves\",\n",
    "        all_result_folder: str = \"results/train_val_test\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        self._dataset_path = dataset_path\n",
    "        self._encoder_name = encoder_name\n",
    "        self._reset_param = reset_param\n",
    "        self._resample_param = resample_param\n",
    "        self._embed_batch_size = embed_batch_size\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._lr_decay = lr_decay\n",
    "        self._epochs = epochs\n",
    "        self._early_stop = early_stop\n",
    "        self._tolerance = tolerance\n",
    "        self._min_epoch = min_epoch\n",
    "        self._device = device\n",
    "        self._all_plot_folder = all_plot_folder\n",
    "        self._all_result_folder = all_result_folder\n",
    "        self._encoder_params = encoder_params\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running pytorch model\n",
    "\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        \n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "        - worker_seed: int, the seed for dataloader\n",
    "        - learning_rate: float\n",
    "        - lr_decay: float, factor by which to decay LR on plateau\n",
    "        - epochs: int, number of epochs to train for\n",
    "        - device: torch.device or str\n",
    "        - all_plot_folder: str, the parent folder path for saving all the learning curves\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "\n",
    "        Returns:\n",
    "        - result_dict: dict, with the keys and dict values\n",
    "            \"losses\": {\"train_losses\": np.ndarray, \"val_losses\": np.ndarray}\n",
    "            \"train\": {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._train_loader, self._val_loader, self._test_loader = split_protrain_loader(\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            embed_folder=embed_folder,\n",
    "            seq_start_idx=seq_start_idx,\n",
    "            seq_end_idx=seq_end_idx,\n",
    "            subset_list=[\"train\", \"val\", \"test\"],\n",
    "            loader_batch_size=loader_batch_size,\n",
    "            worker_seed=worker_seed,\n",
    "            if_encode_all=if_encode_all,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        encoder_name, encoder_class, total_emb_layer = get_emb_info(encoder_name)\n",
    "\n",
    "        if encoder_class == ESMEncoder:\n",
    "            self._encoder_info_dict = TRANSFORMER_INFO\n",
    "        elif encoder_class == CARPEncoder:\n",
    "            self._encoder_info_dict = CARP_INFO\n",
    "\n",
    "        future_path = {}\n",
    "        # add the thredpool max_workers=None\n",
    "        with futures.ProcessPoolExecutor() as pool:\n",
    "            # for each layer train the model and save the model\n",
    "            for embed_layer in tqdm(range(total_emb_layer)):\n",
    "                pool.submit(\n",
    "                    self.run_pytorch_layer,\n",
    "                    embed_layer\n",
    "                )\n",
    "\n",
    "    def run_pytorch_layer(self, embed_layer):\n",
    "        model = LinearRegression(\n",
    "            input_dim=self._encoder_info_dict[self._encoder_name][0], output_dim=1\n",
    "        )\n",
    "        model.to(self._device, non_blocking=True)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        criterion.to(self._device, non_blocking=True)\n",
    "\n",
    "        train_losses, val_losses = train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=self._train_loader,\n",
    "            val_loader=self._val_loader,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            device=self._device,\n",
    "            learning_rate=self._learning_rate,\n",
    "            lr_decay=self._lr_decay,\n",
    "            epochs=self._epochs,\n",
    "            early_stop=self._early_stop,\n",
    "            tolerance=self._tolerance,\n",
    "            min_epoch=self._min_epoch,\n",
    "            **self._encoder_params,\n",
    "        )\n",
    "\n",
    "        # record the losses\n",
    "        result_dict = {\n",
    "            \"losses\": {\"train_losses\": train_losses, \"val_losses\": val_losses}\n",
    "        }\n",
    "\n",
    "        plot_lc(\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            all_plot_folder=self._all_plot_folder,\n",
    "        )\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, loader in zip(\n",
    "            [\"train\", \"val\", \"test\"],\n",
    "            [self._train_loader, self._val_loader, self._test_loader],\n",
    "        ):\n",
    "            mse, pred, true = test(\n",
    "                model=model, loader=loader, device=self._device, criterion=criterion\n",
    "            )\n",
    "\n",
    "            result_dict[subset] = {\n",
    "                \"mse\": mse,\n",
    "                \"pred\": pred,\n",
    "                \"true\": true,\n",
    "                \"ndcg\": ndcg_score(true[None, :], pred[None, :]),\n",
    "                \"rho\": spearmanr(true, pred),\n",
    "            }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=get_default_output_path(self._all_result_folder),\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )\n",
    "\n",
    "\n",
    "def run_pytorch(\n",
    "    dataset_path: str,\n",
    "    encoder_name: str,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 128,\n",
    "    flatten_emb: bool | str = False,\n",
    "    embed_folder: str | None = None,\n",
    "    seq_start_idx: bool | int = False,\n",
    "    seq_end_idx: bool | int = False,\n",
    "    loader_batch_size: int = 64,\n",
    "    worker_seed: int = RAND_SEED,\n",
    "    if_encode_all: bool = True,\n",
    "    learning_rate: float = 1e-4,\n",
    "    lr_decay: float = 0.1,\n",
    "    epochs: int = 100,\n",
    "    early_stop: bool = True,\n",
    "    tolerance: int = 10,\n",
    "    min_epoch: int = 5,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    all_plot_folder: str = \"results/learning_curves\",\n",
    "    all_result_folder: str = \"results/train_val_test\",\n",
    "    **encoder_params,\n",
    ") -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    A function for running pytorch model\n",
    "\n",
    "    Args:\n",
    "    - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "        columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "    - encoder_name: str, the name of the encoder\n",
    "    \n",
    "    - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "    - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "    - embed_folder: str = None, path to presaved embedding\n",
    "    - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "    - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "    - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "    - worker_seed: int, the seed for dataloader\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float, factor by which to decay LR on plateau\n",
    "    - epochs: int, number of epochs to train for\n",
    "    - device: torch.device or str\n",
    "    - all_plot_folder: str, the parent folder path for saving all the learning curves\n",
    "    - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "    - encoder_params: kwarg, additional parameters for encoding\n",
    "\n",
    "    Returns:\n",
    "    - result_dict: dict, with the keys and dict values\n",
    "        \"losses\": {\"train_losses\": np.ndarray, \"val_losses\": np.ndarray}\n",
    "        \"train\": {\"mse\": float, \n",
    "                  \"pred\": np.ndarray,\n",
    "                  \"true\": np.ndarray,\n",
    "                  \"ndcg\": float,\n",
    "                  \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"val\":   {\"mse\": float, \n",
    "                  \"pred\": np.ndarray,\n",
    "                  \"true\": np.ndarray,\n",
    "                  \"ndcg\": float,\n",
    "                  \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"test\":  {\"mse\": float, \n",
    "                  \"pred\": np.ndarray,\n",
    "                  \"true\": np.ndarray,\n",
    "                  \"ndcg\": float,\n",
    "                  \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    train_loader, val_loader, test_loader = split_protrain_loader(\n",
    "        dataset_path=dataset_path,\n",
    "        encoder_name=encoder_name,\n",
    "        reset_param=reset_param,\n",
    "        resample_param=resample_param,\n",
    "        embed_batch_size=embed_batch_size,\n",
    "        flatten_emb=flatten_emb,\n",
    "        embed_folder=embed_folder,\n",
    "        seq_start_idx=seq_start_idx,\n",
    "        seq_end_idx=seq_end_idx,\n",
    "        subset_list=[\"train\", \"val\", \"test\"],\n",
    "        loader_batch_size=loader_batch_size,\n",
    "        worker_seed=worker_seed,\n",
    "        if_encode_all=if_encode_all,\n",
    "        **encoder_params,\n",
    "    )\n",
    "\n",
    "    encoder_name, encoder_class, total_emb_layer = get_emb_info(encoder_name)\n",
    "\n",
    "    if encoder_class == ESMEncoder:\n",
    "        encoder_info_dict = TRANSFORMER_INFO\n",
    "    elif encoder_class == CARPEncoder:\n",
    "        encoder_info_dict = CARP_INFO\n",
    "\n",
    "    # for each layer train the model and save the model\n",
    "    for embed_layer in range(total_emb_layer):\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1b_t33_650M_UR50S upto 33 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1b_t33_650M_UR50S upto 33 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1b_t33_650M_UR50S upto 33 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "100%|██████████| 34/34 [00:02<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for layer 13\n",
      "Running model for layer 14Running model for layer 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m Run_Pytorch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=1'>2</a>\u001b[0m     dataset_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/proeng/thermo/mixed_split.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=2'>3</a>\u001b[0m     encoder_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mesm1b_t33_650M_UR50S\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=3'>4</a>\u001b[0m     reset_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=4'>5</a>\u001b[0m     resample_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=5'>6</a>\u001b[0m     embed_batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=6'>7</a>\u001b[0m     flatten_emb\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=7'>8</a>\u001b[0m     embed_folder\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39membeddings/proeng/thermo/mixed_split\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m     seq_start_idx\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m     seq_end_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=10'>11</a>\u001b[0m     loader_batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=11'>12</a>\u001b[0m     worker_seed \u001b[39m=\u001b[39;49m RAND_SEED,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=12'>13</a>\u001b[0m     if_encode_all \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=13'>14</a>\u001b[0m     learning_rate \u001b[39m=\u001b[39;49m \u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=14'>15</a>\u001b[0m     lr_decay \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=15'>16</a>\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=16'>17</a>\u001b[0m     early_stop \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=17'>18</a>\u001b[0m     tolerance \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=18'>19</a>\u001b[0m     min_epoch \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=19'>20</a>\u001b[0m     device \u001b[39m=\u001b[39;49m DEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=20'>21</a>\u001b[0m     all_plot_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest/learning_curves\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=21'>22</a>\u001b[0m     all_result_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest/train_val_test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=22'>23</a>\u001b[0m     \u001b[39m# **encoder_params,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=23'>24</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb Cell 7\u001b[0m in \u001b[0;36mRun_Pytorch.__init__\u001b[0;34m(self, dataset_path, encoder_name, reset_param, resample_param, embed_batch_size, flatten_emb, embed_folder, seq_start_idx, seq_end_idx, loader_batch_size, worker_seed, if_encode_all, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, device, all_plot_folder, all_result_folder, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=140'>141</a>\u001b[0m \u001b[39mwith\u001b[39;00m futures\u001b[39m.\u001b[39mProcessPoolExecutor() \u001b[39mas\u001b[39;00m pool:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=141'>142</a>\u001b[0m     \u001b[39m# for each layer train the model and save the model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=142'>143</a>\u001b[0m     \u001b[39mfor\u001b[39;00m embed_layer \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(total_emb_layer)):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=143'>144</a>\u001b[0m         pool\u001b[39m.\u001b[39msubmit(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=144'>145</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_pytorch_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=145'>146</a>\u001b[0m             embed_layer\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/thermo.ipynb#ch0000006vscode-remote?line=146'>147</a>\u001b[0m         )\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/concurrent/futures/_base.py:628\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 628\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/concurrent/futures/process.py:740\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_manager_thread_wakeup\u001b[39m.\u001b[39mwakeup()\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_manager_thread \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m wait:\n\u001b[0;32m--> 740\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor_manager_thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m    741\u001b[0m \u001b[39m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_manager_thread \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/threading.py:1033\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1034\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/threading.py:1049\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m lock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1049\u001b[0m \u001b[39melif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1050\u001b[0m     lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1051\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Run_Pytorch(\n",
    "    dataset_path=\"data/proeng/thermo/mixed_split.csv\",\n",
    "    encoder_name=\"esm1b_t33_650M_UR50S\",\n",
    "    reset_param = False,\n",
    "    resample_param = False,\n",
    "    embed_batch_size = 128,\n",
    "    flatten_emb= \"mean\",\n",
    "    embed_folder= \"embeddings/proeng/thermo/mixed_split\",\n",
    "    seq_start_idx= False,\n",
    "    seq_end_idx = False,\n",
    "    loader_batch_size = 64,\n",
    "    worker_seed = RAND_SEED,\n",
    "    if_encode_all = False,\n",
    "    learning_rate = 1e-4,\n",
    "    lr_decay = 0.1,\n",
    "    epochs = 100,\n",
    "    early_stop = True,\n",
    "    tolerance = 10,\n",
    "    min_epoch = 5,\n",
    "    device = DEVICE,\n",
    "    all_plot_folder = \"test/learning_curves\",\n",
    "    all_result_folder = \"test/train_val_test\",\n",
    "    # **encoder_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
