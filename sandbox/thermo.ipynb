{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27951\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/proeng/thermo/mixed_split.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...</td>\n",
       "      <td>37.962947</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...</td>\n",
       "      <td>54.425342</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MRICFLLLAFLVAETFANELTRCCAGGTRHFKNSNTCSSIKSEGTS...</td>\n",
       "      <td>49.459216</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIRVALPTTASAIPRSISTSPGETISKNHEEEVKRVWRKADAVCFD...</td>\n",
       "      <td>42.593131</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNGDWSRAFVLSKVKNLYFFVIIDKGFSAILNDPREPVQVGGFFEV...</td>\n",
       "      <td>37.999478</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence     target    set  \\\n",
       "0  MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...  37.962947  train   \n",
       "1  MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...  54.425342  train   \n",
       "2  MRICFLLLAFLVAETFANELTRCCAGGTRHFKNSNTCSSIKSEGTS...  49.459216  train   \n",
       "3  MIRVALPTTASAIPRSISTSPGETISKNHEEEVKRVWRKADAVCFD...  42.593131  train   \n",
       "4  MNGDWSRAFVLSKVKNLYFFVIIDKGFSAILNDPREPVQVGGFFEV...  37.999478  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for running pytorch models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent import futures\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "\n",
    "from scr.preprocess.data_process import split_protrain_loader\n",
    "from scr.encoding.encoding_classes import get_emb_info, ESMEncoder, CARPEncoder\n",
    "from scr.model.pytorch_model import LinearRegression\n",
    "from scr.model.train_test import train, test\n",
    "from scr.vis.learning_vis import plot_lc\n",
    "from scr.utils import get_folder_file_names, pickle_save, get_default_output_path\n",
    "\n",
    "\n",
    "class Run_Pytorch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        loader_batch_size: int = 64,\n",
    "        worker_seed: int = RAND_SEED,\n",
    "        if_encode_all: bool = True,\n",
    "        learning_rate: float = 1e-4,\n",
    "        lr_decay: float = 0.1,\n",
    "        epochs: int = 100,\n",
    "        early_stop: bool = True,\n",
    "        tolerance: int = 10,\n",
    "        min_epoch: int = 5,\n",
    "        device: torch.device | str = DEVICE,\n",
    "        all_plot_folder: str = \"results/learning_curves\",\n",
    "        all_result_folder: str = \"results/train_val_test\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        self._dataset_path = dataset_path\n",
    "        self._encoder_name = encoder_name\n",
    "        self._reset_param = reset_param\n",
    "        self._resample_param = resample_param\n",
    "        self._embed_batch_size = embed_batch_size\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._lr_decay = lr_decay\n",
    "        self._epochs = epochs\n",
    "        self._early_stop = early_stop\n",
    "        self._tolerance = tolerance\n",
    "        self._min_epoch = min_epoch\n",
    "        self._device = device\n",
    "        self._all_plot_folder = all_plot_folder\n",
    "        self._all_result_folder = all_result_folder\n",
    "        self._encoder_params = encoder_params\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running pytorch model\n",
    "\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        \n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "        - worker_seed: int, the seed for dataloader\n",
    "        - learning_rate: float\n",
    "        - lr_decay: float, factor by which to decay LR on plateau\n",
    "        - epochs: int, number of epochs to train for\n",
    "        - device: torch.device or str\n",
    "        - all_plot_folder: str, the parent folder path for saving all the learning curves\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "\n",
    "        Returns:\n",
    "        - result_dict: dict, with the keys and dict values\n",
    "            \"losses\": {\"train_losses\": np.ndarray, \"val_losses\": np.ndarray}\n",
    "            \"train\": {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._train_loader, self._val_loader, self._test_loader = split_protrain_loader(\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            embed_folder=embed_folder,\n",
    "            seq_start_idx=seq_start_idx,\n",
    "            seq_end_idx=seq_end_idx,\n",
    "            subset_list=[\"train\", \"val\", \"test\"],\n",
    "            loader_batch_size=loader_batch_size,\n",
    "            worker_seed=worker_seed,\n",
    "            if_encode_all=if_encode_all,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        encoder_name, encoder_class, total_emb_layer = get_emb_info(encoder_name)\n",
    "\n",
    "        if encoder_class == ESMEncoder:\n",
    "            self._encoder_info_dict = TRANSFORMER_INFO\n",
    "        elif encoder_class == CARPEncoder:\n",
    "            self._encoder_info_dict = CARP_INFO\n",
    "\n",
    "        future_path = {}\n",
    "        # add the thredpool max_workers=None\n",
    "        with futures.ProcessPoolExecutor(max_workers=os.cpu_count() - 1) as pool:\n",
    "            # for each layer train the model and save the model\n",
    "            for embed_layer in tqdm(range(total_emb_layer)):\n",
    "                pool.submit(self.run_pytorch_layer, embed_layer)\n",
    "\n",
    "    def run_pytorch_layer(self, embed_layer):\n",
    "        model = LinearRegression(\n",
    "            input_dim=self._encoder_info_dict[self._encoder_name][0], output_dim=1\n",
    "        )\n",
    "        model.to(self._device, non_blocking=True)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        criterion.to(self._device, non_blocking=True)\n",
    "\n",
    "        train_losses, val_losses = train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=self._train_loader,\n",
    "            val_loader=self._val_loader,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            device=self._device,\n",
    "            learning_rate=self._learning_rate,\n",
    "            lr_decay=self._lr_decay,\n",
    "            epochs=self._epochs,\n",
    "            early_stop=self._early_stop,\n",
    "            tolerance=self._tolerance,\n",
    "            min_epoch=self._min_epoch,\n",
    "            **self._encoder_params,\n",
    "        )\n",
    "\n",
    "        # record the losses\n",
    "        result_dict = {\n",
    "            \"losses\": {\"train_losses\": train_losses, \"val_losses\": val_losses}\n",
    "        }\n",
    "\n",
    "        plot_lc(\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            all_plot_folder=self._all_plot_folder,\n",
    "        )\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, loader in zip(\n",
    "            [\"train\", \"val\", \"test\"],\n",
    "            [self._train_loader, self._val_loader, self._test_loader],\n",
    "        ):\n",
    "            mse, pred, true = test(\n",
    "                model=model, loader=loader, device=self._device, criterion=criterion\n",
    "            )\n",
    "\n",
    "            result_dict[subset] = {\n",
    "                \"mse\": mse,\n",
    "                \"pred\": pred,\n",
    "                \"true\": true,\n",
    "                \"ndcg\": ndcg_score(true[None, :], pred[None, :]),\n",
    "                \"rho\": spearmanr(true, pred),\n",
    "            }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=get_default_output_path(self._all_result_folder),\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1b_t33_650M_UR50S upto 33 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1b_t33_650M_UR50S upto 33 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1b_t33_650M_UR50S upto 33 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:04<00:00,  7.33it/s]\n"
     ]
    }
   ],
   "source": [
    "Run_Pytorch(\n",
    "    dataset_path=\"data/proeng/thermo/mixed_split.csv\",\n",
    "    encoder_name=\"esm1b_t33_650M_UR50S\",\n",
    "    reset_param = False,\n",
    "    resample_param = False,\n",
    "    embed_batch_size = 128,\n",
    "    flatten_emb= \"mean\",\n",
    "    embed_folder= \"embeddings/proeng/thermo/mixed_split\",\n",
    "    seq_start_idx= False,\n",
    "    seq_end_idx = False,\n",
    "    loader_batch_size = 64,\n",
    "    worker_seed = RAND_SEED,\n",
    "    if_encode_all = False,\n",
    "    learning_rate = 1e-4,\n",
    "    lr_decay = 0.1,\n",
    "    epochs = 100,\n",
    "    early_stop = True,\n",
    "    tolerance = 10,\n",
    "    min_epoch = 5,\n",
    "    device = DEVICE,\n",
    "    all_plot_folder = \"test/learning_curves\",\n",
    "    all_result_folder = \"test/train_val_test\",\n",
    "    # **encoder_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
