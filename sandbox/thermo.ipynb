{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27951\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/proeng/thermo/mixed_split.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...</td>\n",
       "      <td>37.962947</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...</td>\n",
       "      <td>54.425342</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MRICFLLLAFLVAETFANELTRCCAGGTRHFKNSNTCSSIKSEGTS...</td>\n",
       "      <td>49.459216</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIRVALPTTASAIPRSISTSPGETISKNHEEEVKRVWRKADAVCFD...</td>\n",
       "      <td>42.593131</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNGDWSRAFVLSKVKNLYFFVIIDKGFSAILNDPREPVQVGGFFEV...</td>\n",
       "      <td>37.999478</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence     target    set  \\\n",
       "0  MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...  37.962947  train   \n",
       "1  MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...  54.425342  train   \n",
       "2  MRICFLLLAFLVAETFANELTRCCAGGTRHFKNSNTCSSIKSEGTS...  49.459216  train   \n",
       "3  MIRVALPTTASAIPRSISTSPGETISKNHEEEVKRVWRKADAVCFD...  42.593131  train   \n",
       "4  MNGDWSRAFVLSKVKNLYFFVIIDKGFSAILNDPREPVQVGGFFEV...  37.999478  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_505257/1677256276.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Sequence, defaultdict\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pre-processing the dataset\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Sequence, defaultdict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scr.utils import pickle_save, pickle_load, replace_ext\n",
    "from scr.params.sys import RAND_SEED\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO, MAX_SEQ_LEN\n",
    "from scr.preprocess.seq_loader import SeqLoader\n",
    "from scr.encoding.encoding_classes import (\n",
    "    AbstractEncoder,\n",
    "    ESMEncoder,\n",
    "    CARPEncoder,\n",
    "    OnehotEncoder,\n",
    ")\n",
    "\n",
    "\n",
    "def get_mut_name(mut_seq: str, parent_seq: str) -> str:\n",
    "    \"\"\"\n",
    "    A function for returning the mutant name\n",
    "\n",
    "    Args:\n",
    "    - mut_seq: str, the full mutant sequence\n",
    "    - parent_seq: str, the full parent sequence\n",
    "\n",
    "    Returns:\n",
    "    - str, parent, indel, or mutant name in the format of\n",
    "        ParentAAMutLocMutAA:ParentAAMutLocMutAA:..., ie. W39W:D40G:G41C:V54Q\n",
    "    \"\"\"\n",
    "\n",
    "    mut_list = []\n",
    "    if parent_seq == mut_seq:\n",
    "        return \"parent\"\n",
    "    elif len(parent_seq) == len(mut_seq):\n",
    "        for i, (p, m) in enumerate(zip(list(parent_seq), list(mut_seq))):\n",
    "            if p != m:\n",
    "                mut_list.append(f\"{p}{i+1}{m}\")\n",
    "        return \":\".join(mut_list)\n",
    "    else:\n",
    "        return \"indel\"\n",
    "\n",
    "\n",
    "class AddMutInfo:\n",
    "    \"\"\"A class for appending mutation info for mainly protein engineering tasks\"\"\"\n",
    "\n",
    "    def __init__(self, parent_seq_path: str, csv_path: str):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - parent_seq_path: str, path for the parent sequence\n",
    "        - csv_path: str, path for the fitness csv file\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the parent sequence from the fasta file\n",
    "        self._parent_seq = SeqLoader(parent_seq_path=parent_seq_path)\n",
    "\n",
    "        # load the dataframe\n",
    "        self._init_df = pd.read_csv(csv_path)\n",
    "\n",
    "        self._df = self._init_df.copy()\n",
    "        # add a column with the mutant names\n",
    "        self._df[\"mut_name\"] = self._init_df[\"sequence\"].apply(\n",
    "            get_mut_name, parent_seq=self._parent_seq\n",
    "        )\n",
    "        # add a column with the number of mutations\n",
    "        self._df[\"mut_numb\"] = (\n",
    "            self._df[\"mut_name\"].str.split(\":\").map(len, na_action=\"ignore\")\n",
    "        )\n",
    "\n",
    "        # get the pickle file path\n",
    "        self._pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "\n",
    "        pickle_save(what2save=self._df, where2save=self._pkl_path)\n",
    "\n",
    "    @property\n",
    "    def parent_seq(self) -> str:\n",
    "        \"\"\"Return the parent sequence\"\"\"\n",
    "        return self._parent_seq\n",
    "\n",
    "    @property\n",
    "    def pkl_path(self) -> str:\n",
    "        \"\"\"Return the pkl file path for the processed dataframe\"\"\"\n",
    "        return self._pkl_path\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the processed dataframe\"\"\"\n",
    "        return self._df\n",
    "\n",
    "\n",
    "class TaskProcess:\n",
    "    \"\"\"A class for handling different downstream tasks\"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_folder: str, a folder path with all the tasks as subfolders where\n",
    "            all the subfolders have datasets as the subsubfolders, ie\n",
    "\n",
    "            {data_folder}/\n",
    "                proeng/\n",
    "                    aav/\n",
    "                        one_vs_many.csv\n",
    "                        two_vs_many.csv\n",
    "                        P03135.fasta\n",
    "                    thermo/\n",
    "                        mixed.csv\n",
    "        \"\"\"\n",
    "\n",
    "        if data_folder[-1] == \"/\":\n",
    "            self._data_folder = data_folder\n",
    "        else:\n",
    "            self._data_folder = data_folder + \"/\"\n",
    "\n",
    "        # sumamarize all files i nthe data folder\n",
    "        self._sum_file_df = self.sum_files()\n",
    "\n",
    "    def sum_files(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Summarize all files in the data folder\n",
    "\n",
    "        Returns:\n",
    "        - A dataframe with \"task\", \"dataset\", \"split\",\n",
    "            \"csv_path\", \"fasta_path\", \"pkl_path\" as columns, ie.\n",
    "            (proeng, gb1, low_vs_high, data/proeng/gb1/low_vs_high.csv,\n",
    "            data/proeng/gb1/5LDE_1.fasta)\n",
    "            note that csv_path is the list of lmdb files for the structure task\n",
    "        \"\"\"\n",
    "        dataset_folders = glob(f\"{self._data_folder}*/*\")\n",
    "        # need a list of tuples in the order of:\n",
    "        # (task, dataset, split, csv_path, fasta_path)\n",
    "        list_for_df = []\n",
    "        for dataset_folder in dataset_folders:\n",
    "            _, task, dataset = dataset_folder.split(\"/\")\n",
    "            if task == \"structure\":\n",
    "                structure_file_list = [\n",
    "                    file_path\n",
    "                    for file_path in glob(f\"{dataset_folder}/*.*\")\n",
    "                    if os.path.basename(os.path.splitext(file_path)[0]).split(\"_\")[-1]\n",
    "                    in [\"train\", \"valid\", \"cb513\"]\n",
    "                ]\n",
    "                list_for_df.append(\n",
    "                    tuple([task, dataset, \"cb513\", structure_file_list, \"\", \"\"])\n",
    "                )\n",
    "            else:\n",
    "                csv_paths = glob(f\"{dataset_folder}/*.csv\")\n",
    "                fasta_paths = glob(f\"{dataset_folder}/*.fasta\")\n",
    "                pkl_paths = glob(f\"{dataset_folder}/*.pkl\")\n",
    "\n",
    "                assert len(csv_paths) >= 1, \"Less than one csv\"\n",
    "                assert len(fasta_paths) <= 1, \"More than one fasta\"\n",
    "\n",
    "                for csv_path in csv_paths:\n",
    "                    # if parent seq fasta exists\n",
    "                    if len(fasta_paths) == 1:\n",
    "                        fasta_path = fasta_paths[0]\n",
    "\n",
    "                        # if no existing pkl file, generate and save\n",
    "                        if len(pkl_paths) == 0:\n",
    "                            print(f\"Adding mutation info to {csv_path}...\")\n",
    "                            pkl_path = AddMutInfo(\n",
    "                                parent_seq_path=fasta_path, csv_path=csv_path\n",
    "                            ).pkl_path\n",
    "                        # pkl file exits\n",
    "                        else:\n",
    "                            pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "                    # no parent fasta no pkl file\n",
    "                    else:\n",
    "                        fasta_path = \"\"\n",
    "                        pkl_path = \"\"\n",
    "\n",
    "                    list_for_df.append(\n",
    "                        tuple(\n",
    "                            [\n",
    "                                task,\n",
    "                                dataset,\n",
    "                                os.path.basename(os.path.splitext(csv_path)[0]),\n",
    "                                csv_path,\n",
    "                                fasta_path,\n",
    "                                pkl_path,\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            list_for_df,\n",
    "            columns=[\"task\", \"dataset\", \"split\", \"csv_path\", \"fasta_path\", \"pkl_path\"],\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def sum_file_df(self) -> pd.DataFrame:\n",
    "        \"\"\"A summary table for all files in the data folder\"\"\"\n",
    "        return self._sum_file_df\n",
    "\n",
    "class ProtranDataset(Dataset):\n",
    "\n",
    "    \"\"\"A dataset class for processing protein transfer data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        subset: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_path: str = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        if_encode_all: bool = True,\n",
    "        **encoder_params,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - subset: str, train, val, test\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_path: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        # with additional info mut_name, mut_numb\n",
    "        if os.path.splitext(dataset_path)[-1] in [\".pkl\", \".PKL\", \"\"]:\n",
    "            self._df = pickle_load(dataset_path)\n",
    "            self._add_mut_info = True\n",
    "        # without such info\n",
    "        else:\n",
    "            self._df = pd.read_csv(dataset_path)\n",
    "            self._add_mut_info = False\n",
    "\n",
    "        assert \"set\" in self._df.columns, f\"set is not a column in {dataset_path}\"\n",
    "        assert (\n",
    "            \"validation\" in self._df.columns\n",
    "        ), f\"validation is not a column in {dataset_path}\"\n",
    "\n",
    "        self._df_train = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] != True)\n",
    "        ]\n",
    "        self._df_val = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] == True)\n",
    "        ]\n",
    "        self._df_test = self._df.loc[(self._df[\"set\"] == \"test\")]\n",
    "\n",
    "        self._df_dict = {\n",
    "            \"train\": self._df_train,\n",
    "            \"val\": self._df_val,\n",
    "            \"test\": self._df_test,\n",
    "        }\n",
    "\n",
    "        assert subset in list(\n",
    "            self._df_dict.keys()\n",
    "        ), \"split can only be 'train', 'val', or 'test'\"\n",
    "        self._subset = subset\n",
    "\n",
    "        self._subdf_len = len(self._df_dict[self._subset])\n",
    "\n",
    "        # not specified seq start will be from 0\n",
    "        if seq_start_idx == False:\n",
    "            self._seq_start_idx = 0\n",
    "        else:\n",
    "            self._seq_start_idx = int(seq_start_idx)\n",
    "        # not specified seq end will be the full sequence length\n",
    "        if seq_end_idx == False:\n",
    "            self._seq_end_idx = -1\n",
    "            self._max_seq_len = self._df.sequence.str.len().max()\n",
    "        else:\n",
    "            self._seq_end_idx = int(seq_end_idx)\n",
    "            self._max_seq_len = self._seq_end_idx - self._seq_start_idx\n",
    "\n",
    "        # get unencoded string of input sequence\n",
    "        # will need to convert data type\n",
    "        self.sequence = self._get_column_value(\"sequence\")\n",
    "\n",
    "        self.if_encode_all = if_encode_all\n",
    "        if self.if_encode_all:\n",
    "            # get the encoder class\n",
    "            if encoder_name in TRANSFORMER_INFO.keys():\n",
    "                encoder_class = ESMEncoder\n",
    "            elif encoder_name in CARP_INFO.keys():\n",
    "                encoder_class = CARPEncoder\n",
    "            else:\n",
    "                encoder_class = OnehotEncoder\n",
    "                encoder_params[\"max_seq_len\"] = self._max_seq_len\n",
    "\n",
    "            # get the encoder\n",
    "            self._encoder = encoder_class(\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                **encoder_params,\n",
    "            )\n",
    "            self._total_emb_layer = self._encoder.total_emb_layer\n",
    "\n",
    "            # check if pregenerated embedding\n",
    "            if embed_path is not None:\n",
    "                print(f\"Loading pregenerated embeddings from {embed_path}\")\n",
    "                encoded_dict = pickle_load(embed_path)\n",
    "\n",
    "            # encode the sequences without the mut_name\n",
    "            else:\n",
    "                # init an empty dict with empty list to append emb\n",
    "                encoded_dict = defaultdict(list)\n",
    "\n",
    "                # use the encoder generator for batch emb\n",
    "                # assume no labels included\n",
    "                for encoded_batch_dict in self._encoder.encode(\n",
    "                    mut_seqs=self.sequence,\n",
    "                    batch_size=embed_batch_size,\n",
    "                    flatten_emb=flatten_emb,\n",
    "                ):\n",
    "\n",
    "                    for layer, emb in encoded_batch_dict.items():\n",
    "                        encoded_dict[layer].append(emb)\n",
    "\n",
    "            # assign each layer as its own variable\n",
    "            for layer, emb in encoded_dict.items():\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"layer\" + str(layer),\n",
    "                    np.vstack(emb)\n",
    "                    # torch.tensor(np.vstack(emb), dtype=torch.float32),\n",
    "                )\n",
    "\n",
    "        # get and format the fitness or secondary structure values\n",
    "        # can be numbers or string\n",
    "        # will need to convert data type\n",
    "        # make 1D tensor 2D\n",
    "        self.y = np.expand_dims(self._get_column_value(\"target\"), 1)\n",
    "\n",
    "        # add mut_name and mut_numb for relevant proeng datasets\n",
    "        if self._add_mut_info:\n",
    "            self.mut_name = self._get_column_value(\"mut_name\")\n",
    "            self.mut_numb = self._get_column_value(\"mut_numb\")\n",
    "        else:\n",
    "            self.mut_name = [\"\"] * self._subdf_len\n",
    "            self.mut_numb = [np.nan] * self._subdf_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the selected subset of the dataframe\"\"\"\n",
    "        return self._subdf_len\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Return the item in the order of\n",
    "        target (y), sequence, mut_name (optional), mut_numb (optional),\n",
    "        embedding per layer upto the max number of layer for the encoder\n",
    "\n",
    "        Args:\n",
    "        - idx: int\n",
    "        \"\"\"\n",
    "        if self.if_encode_all:\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "                *(\n",
    "                    getattr(self, \"layer\" + str(layer))[idx]\n",
    "                    for layer in range(self._total_emb_layer)\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "            )\n",
    "\n",
    "\n",
    "    def _get_column_value(self, column_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Check and return the column values of the selected dataframe subset\n",
    "\n",
    "        Args:\n",
    "        - column_name: str, the name of the dataframe column\n",
    "        \"\"\"\n",
    "        if column_name in self._df.columns:\n",
    "            if column_name == \"sequence\":\n",
    "                return (\n",
    "                    self._df_dict[self._subset][\"sequence\"]\n",
    "                    .astype(str)\n",
    "                    .str[self._seq_start_idx : self._seq_end_idx]\n",
    "                    .apply(\n",
    "                        lambda x: x[: int(MAX_SEQ_LEN // 2)]\n",
    "                        + x[-int(MAX_SEQ_LEN // 2) :]\n",
    "                        if len(x) > MAX_SEQ_LEN\n",
    "                        else x\n",
    "                    )\n",
    "                    .values\n",
    "                )\n",
    "            else:\n",
    "                return self._df_dict[self._subset][column_name].values\n",
    "\n",
    "    @property\n",
    "    def df_full(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the full loaded dataset\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def df_train(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_train\n",
    "\n",
    "    @property\n",
    "    def df_val(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for validation only\"\"\"\n",
    "        return self._df_val\n",
    "\n",
    "    @property\n",
    "    def df_test(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_test\n",
    "\n",
    "    @property\n",
    "    def max_seq_len(self) -> int:\n",
    "        \"\"\"Longest sequence length\"\"\"\n",
    "        return self._max_seq_len\n",
    "\n",
    "\n",
    "def split_protrain_loader(\n",
    "    dataset_path: str,\n",
    "    encoder_name: str,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 128,\n",
    "    flatten_emb: bool | str = False,\n",
    "    embed_path: str | None = None,\n",
    "    seq_start_idx: bool | int = False,\n",
    "    seq_end_idx: bool | int = False,\n",
    "    subset_list: list[str] = [\"train\", \"val\", \"test\"],\n",
    "    loader_batch_size: int = 64,\n",
    "    worker_seed: int = RAND_SEED,\n",
    "    if_encode_all: bool = True,\n",
    "    **encoder_params,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    A function encode and load the data from a path\n",
    "\n",
    "    Args:\n",
    "    - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "        columns include: sequence, target, set, validation,\n",
    "        mut_name (optional), mut_numb (optional)\n",
    "    - encoder_name: str, the name of the encoder\n",
    "    - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "    - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "    - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "    - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "    - embed_path: str = None, path to presaved embedding\n",
    "    - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "    - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "    - subset_list: list of str, train, val, test\n",
    "    - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "    - worker_seed: int, the seed for dataloader\n",
    "    - encoder_params: kwarg, additional parameters for encoding\n",
    "    \"\"\"\n",
    "\n",
    "    assert set(subset_list) <= set(\n",
    "        [\"train\", \"val\", \"test\"]\n",
    "    ), \"subset_list can only contain terms with in be 'train', 'val', or 'test'\"\n",
    "\n",
    "    # specify no shuffling for validation and test\n",
    "    if_shuffle_list = [True if subset == \"train\" else False for subset in subset_list]\n",
    "\n",
    "    return (\n",
    "        DataLoader(\n",
    "            dataset=ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_path=embed_path,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                if_encode_all=if_encode_all,\n",
    "                **encoder_params,\n",
    "            ),\n",
    "            batch_size=loader_batch_size,\n",
    "            shuffle=if_shuffle,\n",
    "            worker_init_fn=worker_seed,\n",
    "        )\n",
    "        for subset, if_shuffle in zip(subset_list, if_shuffle_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(349, 39, 49)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_protrain_loader(\n",
    "    dataset_path=\"data/proeng/thermo/mixed_split.csv\",\n",
    "    encoder_name=\"\",\n",
    "    reset_param= False,\n",
    "    resample_param= False,\n",
    "    embed_batch_size= 0,\n",
    "    flatten_emb = \"flatten\",\n",
    "    embed_path = None,\n",
    "    seq_start_idx = False,\n",
    "    seq_end_idx = False,\n",
    "    subset_list= [\"train\", \"val\", \"test\"],\n",
    "    loader_batch_size = 64,\n",
    "    worker_seed = RAND_SEED,\n",
    "    if_encode_all = False,\n",
    "    #**encoder_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for running pytorch models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "from scr.params.emb import TRANSFORMER_INFO\n",
    "\n",
    "from scr.preprocess.data_process import split_protrain_loader\n",
    "from scr.model.pytorch_model import LinearRegression\n",
    "from scr.model.train_test import train, test\n",
    "from scr.vis.learning_vis import plot_lc\n",
    "from scr.utils import get_folder_file_names, pickle_save, get_default_output_path\n",
    "\n",
    "def run_pytorch(\n",
    "    dataset_path: str,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    embed_batch_size: int = 128,\n",
    "    flatten_emb: bool | str = False,\n",
    "    embed_path: str | None = None,\n",
    "    seq_start_idx: bool | int = False,\n",
    "    seq_end_idx: bool | int = False,\n",
    "    loader_batch_size: int = 64,\n",
    "    worker_seed: int = RAND_SEED,\n",
    "    learning_rate: float = 1e-4,\n",
    "    lr_decay: float = 0.1,\n",
    "    epochs: int = 100,\n",
    "    early_stop: bool = True,\n",
    "    tolerance: int = 10,\n",
    "    min_epoch: int = 5,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    all_plot_folder: str = \"results/learning_curves\",\n",
    "    all_result_folder: str = \"results/train_val_test\",\n",
    "    **encoder_params,\n",
    ") -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    A function for running pytorch model\n",
    "\n",
    "    Args:\n",
    "    - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "        columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "    - encoder_name: str, the name of the encoder\n",
    "    - embed_layer: int, the layer number of the embedding\n",
    "    - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "    - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "    - embed_path: str = None, path to presaved embedding\n",
    "    - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "    - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "    - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "    - worker_seed: int, the seed for dataloader\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float, factor by which to decay LR on plateau\n",
    "    - epochs: int, number of epochs to train for\n",
    "    - device: torch.device or str\n",
    "    - all_plot_folder: str, the parent folder path for saving all the learning curves\n",
    "    - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "    - encoder_params: kwarg, additional parameters for encoding\n",
    "\n",
    "    Returns:\n",
    "    - result_dict: dict, with the keys and dict values\n",
    "        \"losses\": {\"train_losses\": np.ndarray, \"val_losses\": np.ndarray}\n",
    "        \"train\": {\"mse\": float, \n",
    "                  \"pred\": np.ndarray,\n",
    "                  \"true\": np.ndarray,\n",
    "                  \"ndcg\": float,\n",
    "                  \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"val\":   {\"mse\": float, \n",
    "                  \"pred\": np.ndarray,\n",
    "                  \"true\": np.ndarray,\n",
    "                  \"ndcg\": float,\n",
    "                  \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"test\":  {\"mse\": float, \n",
    "                  \"pred\": np.ndarray,\n",
    "                  \"true\": np.ndarray,\n",
    "                  \"ndcg\": float,\n",
    "                  \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader, val_loader, test_loader = split_protrain_loader(\n",
    "        dataset_path=dataset_path,\n",
    "        encoder_name=encoder_name,\n",
    "        embed_layer=embed_layer,\n",
    "        embed_batch_size=embed_batch_size,\n",
    "        flatten_emb=flatten_emb,\n",
    "        embed_path=embed_path,\n",
    "        seq_start_idx=seq_start_idx,\n",
    "        seq_end_idx=seq_end_idx,\n",
    "        subset_list=[\"train\", \"val\", \"test\"],\n",
    "        loader_batch_size=loader_batch_size,\n",
    "        worker_seed=worker_seed,\n",
    "        **encoder_params,\n",
    "    )\n",
    "\n",
    "    model = LinearRegression(input_dim=TRANSFORMER_INFO[encoder_name][0], output_dim=1)\n",
    "    model.to(device, non_blocking=True)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    criterion.to(device, non_blocking=True)\n",
    "\n",
    "    train_losses, val_losses = train(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_decay=lr_decay,\n",
    "        epochs=epochs,\n",
    "        early_stop=early_stop,\n",
    "        tolerance=tolerance,\n",
    "        min_epoch=min_epoch,\n",
    "    )\n",
    "\n",
    "    # record the losses\n",
    "    result_dict = {\"losses\": {\"train_losses\": train_losses, \"val_losses\": val_losses}}\n",
    "\n",
    "    plot_lc(\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "        dataset_path=dataset_path,\n",
    "        encoder_name=encoder_name,\n",
    "        embed_layer=embed_layer,\n",
    "        flatten_emb=flatten_emb,\n",
    "        all_plot_folder=all_plot_folder,\n",
    "    )\n",
    "\n",
    "    # now test the model with the test data\n",
    "    for subset, loader in zip(\n",
    "        [\"train\", \"val\", \"test\"], [train_loader, val_loader, test_loader]\n",
    "    ):\n",
    "        mse, pred, true = test(\n",
    "            model=model, loader=loader, device=device, criterion=criterion\n",
    "        )\n",
    "\n",
    "        result_dict[subset] = {\n",
    "            \"mse\": mse,\n",
    "            \"pred\": pred,\n",
    "            \"true\": true,\n",
    "            \"ndcg\": ndcg_score(true[None, :], pred[None, :]),\n",
    "            \"rho\": spearmanr(true, pred),\n",
    "        }\n",
    "\n",
    "    dataset_subfolder, file_name = get_folder_file_names(\n",
    "        parent_folder=get_default_output_path(all_result_folder),\n",
    "        dataset_path=dataset_path,\n",
    "        encoder_name=encoder_name,\n",
    "        embed_layer=embed_layer,\n",
    "        flatten_emb=flatten_emb,\n",
    "    )\n",
    "\n",
    "    print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "    pickle_save(\n",
    "        what2save=result_dict,\n",
    "        where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "    )\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A script with model training and testing details assuming\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "\n",
    "# seed everything\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "torch.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed_all(RAND_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def run_epoch(\n",
    "        model: nn.Module,\n",
    "        loader: DataLoader,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        if_encode_all: bool = True,\n",
    "        device: torch.device | str = DEVICE,\n",
    "        criterion: nn.Module | None = None,\n",
    "        optimizer: torch.optim.Optimizer | None = None,\n",
    "        **encoder_params,\n",
    "        ) -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs one epoch.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - device: torch.device or str\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - optimizer: optional torch.optim.Optimizer, must also provide criterion,\n",
    "        only provided for training\n",
    "\n",
    "    Returns: \n",
    "    - float, average loss over batches\n",
    "    \"\"\"\n",
    "    if optimizer is not None:\n",
    "        assert criterion is not None\n",
    "        model.train()\n",
    "        is_train = True\n",
    "    else:\n",
    "        model.eval()\n",
    "        is_train = False\n",
    "\n",
    "    cum_loss = 0.\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        if not if_encode_all:\n",
    "            for (y, x, _, _) in loader:\n",
    "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "                # encode x\n",
    "                # get the encoder class\n",
    "                if encoder_name in TRANSFORMER_INFO.keys():\n",
    "                    encoder_class = ESMEncoder\n",
    "                elif encoder_name in CARP_INFO.keys():\n",
    "                    encoder_class = CARPEncoder\n",
    "                else:\n",
    "                    encoder_class = OnehotEncoder\n",
    "                    encoder_params[\"max_seq_len\"] = len(max(x, key = len))\n",
    "\n",
    "                # get the encoder\n",
    "                encoder = encoder_class(\n",
    "                    encoder_name=encoder_name,\n",
    "                    reset_param=reset_param,\n",
    "                    resample_param=resample_param,\n",
    "                    **encoder_params,\n",
    "                )\n",
    "                total_emb_layer = encoder.total_emb_layer\n",
    "\n",
    "                # init an empty dict with empty list to append emb\n",
    "                encoded_dict = defaultdict(list)\n",
    "\n",
    "                # use the encoder generator for batch emb\n",
    "                # assume no labels included\n",
    "                for encoded_batch_dict in encoder.encode(\n",
    "                    mut_seqs=x,\n",
    "                    batch_size=embed_batch_size,\n",
    "                    flatten_emb=flatten_emb,\n",
    "                ):\n",
    "\n",
    "                    for layer, emb in encoded_batch_dict.items():\n",
    "                        encoded_dict[layer].append(emb)\n",
    "\n",
    "                # assign each layer as its own variable\n",
    "                for layer, emb in encoded_dict.items():\n",
    "                    setattr(\n",
    "                        self,\n",
    "                        \"layer\" + str(layer),\n",
    "                        np.vstack(emb)\n",
    "                        # torch.tensor(np.vstack(emb), dtype=torch.float32),\n",
    "                    )\n",
    "\n",
    "                outputs = model(x)\n",
    "\n",
    "                if criterion is not None:\n",
    "                    loss = criterion(outputs, y.float())\n",
    "\n",
    "                    if optimizer is not None:\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    cum_loss += loss.item()\n",
    "\n",
    "        return cum_loss / len(loader)\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          train_loader: DataLoader,\n",
    "          val_loader: DataLoader,\n",
    "          criterion: nn.Module,\n",
    "          device: torch.device | str = DEVICE,\n",
    "          learning_rate: float = 1e-4,\n",
    "          lr_decay: float = 0.1,\n",
    "          epochs: int = 100,\n",
    "          early_stop: bool = True,\n",
    "          tolerance: int = 10,\n",
    "          min_epoch: int = 5,\n",
    "          ) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - train_loader: torch.utils.data.DataLoader, \n",
    "    - val_loader: torch.utils.data.DataLoader, \n",
    "    - criterion: nn.Module, loss function, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float, factor by which to decay LR on plateau\n",
    "    - epochs: int, number of epochs to train for\n",
    "    - early_stop: bool = True,\n",
    "\n",
    "    Returns: \n",
    "    - tuple of np.ndarray, (train_losses, val_losses)\n",
    "        train/val_losses: np.ndarray, shape [epochs], entries are average loss\n",
    "        over batches for that epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=lr_decay)\n",
    "\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "\n",
    "    # init for early stopping\n",
    "    counter = 0\n",
    "    min_val_loss = np.Inf\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_losses[epoch] = run_epoch(\n",
    "            model=model, loader=train_loader, device=device,\n",
    "            criterion=criterion, optimizer=optimizer)\n",
    "\n",
    "        val_loss = run_epoch(\n",
    "            model=model, loader=val_loader, device=device,\n",
    "            criterion=criterion)\n",
    "        val_losses[epoch] = val_loss\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if early_stop:\n",
    "            # when val loss decrease, reset min loss and counter\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if epoch > min_epoch and counter == tolerance:\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def test(model: nn.Module,\n",
    "         loader: DataLoader,\n",
    "         criterion: nn.Module | None,\n",
    "         device: torch.device | str = DEVICE,\n",
    "         print_every: int = 1000,\n",
    "         ) -> tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Runs one epoch of testing, returning predictions and labels.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - print_every: int, how often (number of batches) to print avg loss\n",
    "    \n",
    "    Returns: tuple (avg_loss, preds, labels)\n",
    "    - avg_loss: float, average loss per training example \n",
    "    - preds: np.ndarray, shape [num_examples, ...], predictions over dataset\n",
    "    - labels: np.ndarray, shape [num_examples, ...], dataset labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    msg = \"[{step:5d}] loss: {loss:.3f}\"\n",
    "\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (x, y, _, _, _) in enumerate(tqdm(loader)):\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x)\n",
    "            preds.append(outputs.detach().cpu().squeeze().numpy())\n",
    "            labels.append(y.detach().cpu().squeeze().numpy())\n",
    "\n",
    "            if criterion is not None:\n",
    "                loss = criterion(outputs, y)\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "                if ((i + 1) % print_every == 0) or (i + 1 == len(loader)):\n",
    "                    tqdm.write(msg.format(step=i + 1, loss=cum_loss / len(loader)))\n",
    "\n",
    "    avg_loss = cum_loss / len(loader)\n",
    "    return avg_loss, np.concatenate(preds), np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
