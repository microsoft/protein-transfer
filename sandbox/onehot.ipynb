{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add encoding classes with class methods\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import Iterable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from sequence_models.pretrained import load_model_and_alphabet\n",
    "\n",
    "from scr.params.aa import AA_NUMB, AA_TO_IND\n",
    "from scr.params.emb import TRANSFORMER_INFO, TRANSFORMER_MAX_SEQ_LEN, CARP_INFO\n",
    "from scr.params.sys import DEVICE\n",
    "\n",
    "\n",
    "class AbstractEncoder(ABC):\n",
    "    \"\"\"\n",
    "    An abstract encoder class to fill in for different kinds of encoders\n",
    "\n",
    "    All encoders will have an \"encode\" function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_name: str = \"\", reset_param: bool = False):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - encoder_name: str, the name of the encoder, default empty for onehot\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform\n",
    "        \"\"\"\n",
    "\n",
    "        self._encoder_name = encoder_name\n",
    "        self._reset_param = reset_param\n",
    "\n",
    "    def reset_parameters(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initiate parameters in the PyTorch model. Following:\n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer\n",
    "\n",
    "        Args:\n",
    "        - model: torch.nn.Module, the input model\n",
    "\n",
    "        Returns:\n",
    "        - torch.nn.Module, the model with all params set with xavier_uniform\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Reinit params for {self._encoder_name} ...\")\n",
    "\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> Iterable[np.ndarray]:\n",
    "        \"\"\"\n",
    "        A function takes a list of sequences to yield a batch of encoded elements\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - generator: dict with layer number as keys and\n",
    "            encoded flattened sequence with or without labels as value\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_seqs, str):\n",
    "            mut_seqs = [mut_seqs]\n",
    "\n",
    "        # If the batch size is 0, then encode all at once in a single batch\n",
    "        if batch_size == 0:\n",
    "            yield self._encode_batch(\n",
    "                mut_seqs=mut_seqs, flatten_emb=flatten_emb, mut_names=mut_names\n",
    "            )\n",
    "\n",
    "        # Otherwise, yield chunks of encoded sequence\n",
    "        else:\n",
    "\n",
    "            for i in tqdm(range(0, len(mut_seqs), batch_size)):\n",
    "\n",
    "                # figure out what mut_names to feed in\n",
    "                if mut_names is None:\n",
    "                    mut_name_batch = mut_names\n",
    "                else:\n",
    "                    mut_name_batch = mut_names[i : i + batch_size]\n",
    "\n",
    "                yield self._encode_batch(\n",
    "                    mut_seqs=mut_seqs[i : i + batch_size],\n",
    "                    flatten_emb=flatten_emb,\n",
    "                    mut_names=mut_name_batch,\n",
    "                )\n",
    "\n",
    "    def flatten_encode(\n",
    "        self, encoded_mut_seqs: np.ndarray, flatten_emb: bool | str\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Flatten the embedding or just return the encoded mutants.\n",
    "\n",
    "        Args:\n",
    "        - encoded_mut_seqs: np.ndarray, shape [batch_size, seq_len, embed_dim]\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "            - True -> shape [batch_size, seq_len * embed_dim]\n",
    "            - \"max\" or \"mean\" -> shape [batch_size, embed_dim]\n",
    "            - False or everything else -> [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, shape depends on flatten_emb parameter\n",
    "        \"\"\"\n",
    "        assert encoded_mut_seqs.shape[2] == self._embed_dim, \"Wrong embed dim\"\n",
    "\n",
    "        if (\n",
    "            flatten_emb in [True, \"flatten\", \"flattened\"]\n",
    "            or self._encoder_name == \"onehot\"\n",
    "        ):\n",
    "            # shape [batch_size, seq_len * embed_dim]\n",
    "            return encoded_mut_seqs.reshape(encoded_mut_seqs.shape[0], -1)\n",
    "\n",
    "        elif isinstance(flatten_emb, str):\n",
    "            if flatten_emb == \"mean\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.mean(axis=1)\n",
    "            elif flatten_emb == \"max\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.max(axis=1)\n",
    "\n",
    "        else:\n",
    "            print(\"No embedding flattening\")\n",
    "            # [batch_size, seq_len, embed_dim]\n",
    "            return encoded_mut_seqs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _encode_batch(\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode a single batch of mut_seqs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def embed_dim(self) -> int:\n",
    "        \"\"\"The dim of the embedding\"\"\"\n",
    "        return self._embed_dim\n",
    "\n",
    "    @property\n",
    "    def max_emb_layer(self) -> int:\n",
    "        \"\"\"The max layer nubmer of the embedding\"\"\"\n",
    "        return self._max_emb_layer\n",
    "\n",
    "    @property\n",
    "    def include_input_layer(self) -> bool:\n",
    "        \"\"\"If include the input layer when counting the max layer number\"\"\"\n",
    "        return self._include_input_layer\n",
    "\n",
    "    @property\n",
    "    def total_emb_layer(self) -> int:\n",
    "        \"\"\"Total embedding layer number\"\"\"\n",
    "        return self._max_emb_layer + self._include_input_layer\n",
    "\n",
    "    @property\n",
    "    def encoder_name(self) -> str:\n",
    "        \"\"\"The name of the encoding method\"\"\"\n",
    "        return self._encoder_name\n",
    "\n",
    "\n",
    "class OnehotEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build a onehot encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_name: str = \"\", reset_param: bool = False):\n",
    "\n",
    "        super().__init__(encoder_name, reset_param)\n",
    "\n",
    "        if encoder_name not in (TRANSFORMER_INFO.keys() and CARP_INFO.keys()):\n",
    "            self._encoder_name = \"onehot\"\n",
    "            self._embed_dim, self._max_emb_layer = AA_NUMB, 0\n",
    "            self._include_input_layer = False\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(\n",
    "            f\"Generating {self._encoder_name} upto {self._max_emb_layer} layer embedding ...\"\n",
    "        )\n",
    "\n",
    "        if reset_param:\n",
    "            self._reset_param = False\n",
    "            print(\n",
    "                f\"Onehot encoding reset param not allowed. /n \\\n",
    "                    Setting reset_param to {self._reset_param} ...\"\n",
    "            )\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        encoded_mut_seqs = []\n",
    "\n",
    "        for mut_seq in mut_seqs:\n",
    "            encoded_mut_seqs.append(np.eye(AA_NUMB)[[AA_TO_IND[aa] for aa in mut_seq]])\n",
    "        return {0: self.flatten_encode(np.array(encoded_mut_seqs), flatten_emb)}\n",
    "\n",
    "\n",
    "class ESMEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build an ESM encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        iftrimCLS: bool = True,\n",
    "        iftrimEOS: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of TRANSFORMER_INFO\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform\n",
    "        - iftrimCLS: bool, whether to trim the first classifification token\n",
    "        - iftrimEOS: bool, whether to trim the end of sequence token, if exists\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, reset_param)\n",
    "\n",
    "        self._iftrimCLS = iftrimCLS\n",
    "        self._iftrimEOS = iftrimEOS\n",
    "\n",
    "        # get transformer dim and layer info\n",
    "        self._embed_dim, self._max_emb_layer, _ = TRANSFORMER_INFO[self._encoder_name]\n",
    "\n",
    "        # esm has the input representation\n",
    "        self._include_input_layer = True\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(\n",
    "            f\"Generating {self._encoder_name} upto {self._max_emb_layer} layer embedding ...\"\n",
    "        )\n",
    "\n",
    "        self.model, self.alphabet = torch.hub.load(\n",
    "            \"facebookresearch/esm:main\", model=self._encoder_name\n",
    "        )\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "\n",
    "        # if reset weights\n",
    "        if self._reset_param:\n",
    "            self.model = self.reset_parameters(model=self.model)\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        expected_num_layers = int(self._encoder_name.split(\"_\")[-3][1:])\n",
    "        assert (\n",
    "            expected_num_layers == self._max_emb_layer\n",
    "        ), \"Wrong ESM model name or layer\"\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_names, str):\n",
    "            mut_names = [mut_names]\n",
    "\n",
    "        # pair the mut_names and mut_seqs\n",
    "        if mut_names is not None:\n",
    "            assert len(mut_names) == len(\n",
    "                mut_seqs\n",
    "            ), \"mutant_name and mut_seqs different length\"\n",
    "            mut_seqs = [(n, m) for (n, m) in zip(mut_names, mut_seqs)]\n",
    "        else:\n",
    "            mut_seqs = [(\"\", m) for m in mut_seqs]\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        batch_labels, _, batch_tokens = self.batch_converter(mut_seqs)\n",
    "        batch_tokens = batch_tokens.to(DEVICE)\n",
    "\n",
    "        # Turn off gradients and pass the batch through\n",
    "        with torch.no_grad():\n",
    "            # shape [batch_size, seq_len + pad, embed_dim]\n",
    "            if batch_tokens.shape[1] > TRANSFORMER_MAX_SEQ_LEN:\n",
    "                print(f\"Sequence exceeds {TRANSFORMER_MAX_SEQ_LEN}, chopping the end\")\n",
    "                batch_tokens = batch_tokens[:, :TRANSFORMER_MAX_SEQ_LEN]\n",
    "\n",
    "            dict_encoded_mut_seqs = self.model(\n",
    "                batch_tokens, repr_layers=list(range(self._max_emb_layer + 1))\n",
    "            )[\"representations\"]\n",
    "\n",
    "        for layer, encoded_mut_seqs in dict_encoded_mut_seqs.items():\n",
    "\n",
    "            encoded_mut_seqs = encoded_mut_seqs.cpu().numpy()\n",
    "            # https://github.com/facebookresearch/esm/blob/main/esm/data.py\n",
    "            # from_architecture\n",
    "\n",
    "            # trim off initial classification token [CLS]\n",
    "            # both \"ESM-1\" and \"ESM-1b\" have prepend_bos = True\n",
    "            if self._iftrimCLS and self._encoder_name.split(\"_\")[0] in [\n",
    "                \"esm1\",\n",
    "                \"esm1b\",\n",
    "            ]:\n",
    "                encoded_mut_seqs = encoded_mut_seqs[:, 1:, :]\n",
    "\n",
    "            # trim off end-of-sequence token [EOS]\n",
    "            # only \"ESM-1b\" has append_eos = True\n",
    "            if self._iftrimEOS and self._encoder_name.split(\"_\")[0] == \"esm1b\":\n",
    "                encoded_mut_seqs = encoded_mut_seqs[:, :-1, :]\n",
    "\n",
    "            if mut_names is not None:\n",
    "                dict_encoded_mut_seqs[layer] = (\n",
    "                    self.flatten_encode(encoded_mut_seqs, flatten_emb),\n",
    "                    batch_labels,\n",
    "                )\n",
    "            else:\n",
    "                dict_encoded_mut_seqs[layer] = self.flatten_encode(\n",
    "                    encoded_mut_seqs, flatten_emb\n",
    "                )\n",
    "\n",
    "        return dict_encoded_mut_seqs\n",
    "\n",
    "\n",
    "class CARPEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build a CARP encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of CARP_INFO\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, reset_param)\n",
    "\n",
    "        self.model, self.collater = load_model_and_alphabet(self._encoder_name)\n",
    "\n",
    "        # if reset weights\n",
    "        if self._reset_param:\n",
    "            self.model = self.reset_parameters(model=self.model)\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self._embed_dim, self._max_emb_layer = CARP_INFO[self._encoder_name]\n",
    "\n",
    "        # carp does not have the input representation\n",
    "        self._include_input_layer = False\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(\n",
    "            f\"Generating {self._encoder_name} upto {self._max_emb_layer} layer embedding ...\"\n",
    "        )\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        mut_seqs = [[m] for m in mut_seqs]\n",
    "\n",
    "        x = self.collater(mut_seqs)[0]\n",
    "\n",
    "        # alternatively check out the article called:\n",
    "        # The One PyTorch Trick Which You Should Know\n",
    "        # How hooks can improve your workflow significantly\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        for layer_numb in list(range(self._max_emb_layer)):\n",
    "            self.model.model.embedder.layers[layer_numb].register_forward_hook(\n",
    "                get_activation(layer_numb)\n",
    "            )\n",
    "\n",
    "        rep = self.model(x)\n",
    "\n",
    "        for layer_numb, encoded_mut_seqs in activation.items():\n",
    "            activation[layer_numb] = self.flatten_encode(\n",
    "                encoded_mut_seqs.cpu().numpy(), flatten_emb\n",
    "            )\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 43, 8309, 8733)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scr.utils import pickle_load\n",
    "\n",
    "df = pickle_load(\"data/proeng/gb1/two_vs_rest.pkl\")\n",
    "df_train = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] != True)]\n",
    "df_val = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] == True)]\n",
    "df_test = df.loc[(df[\"set\"] == \"test\")]\n",
    "\n",
    "len(df_train), len(df_val), len(df_test), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, 5300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_flat_encoder = OnehotEncoder(\n",
    "    encoder_name=\"\",\n",
    "    reset_param=False,\n",
    ").encode(mut_seqs=list(df_val.sequence), mut_names=list(df_val.mut_name))\n",
    "one_emb = next(no_flat_encoder)\n",
    "one_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, 5300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_flat_encoder = OnehotEncoder(\n",
    "    encoder_name=\"\",\n",
    "    reset_param=False,\n",
    ").encode(mut_seqs=list(df_val.sequence), mut_names=list(df_val.mut_name),flatten_emb=\"flatten\", batch_size=0)\n",
    "one_mean_emb = next(mean_flat_encoder)\n",
    "one_mean_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5300/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 5300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_mean_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
