{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence_models.pretrained import load_model_and_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, collater = load_model_and_alphabet('carp_600k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [df_val.sequence.astype(str).str[0 : 56].values[0]]\n",
    "x = collater(seqs)[0]  # (n, max_len)\n",
    "# rep = model(x)  # (n, max_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f446f7dc280>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embedder.layers[0].register_forward_hook(get_activation(\"layer0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1794e-01,  1.1626e-01,  6.0831e-02,  7.6150e-02,  7.5652e-02,\n",
       "           1.4893e-01,  1.9834e-02, -3.0836e-01,  2.5717e-01, -3.4265e-01,\n",
       "           7.3477e-01,  1.3106e+00,  8.5214e-01, -2.4842e-01,  1.0407e+00,\n",
       "           2.3907e+00,  2.6350e-01,  7.8502e-01,  1.1730e-01,  7.7125e-02,\n",
       "          -1.5943e-01, -1.7217e-01, -7.6545e-01, -3.2751e+00, -4.3581e-01,\n",
       "           1.2820e+00, -2.3568e-01,  6.7061e-02,  8.5233e-01,  1.5238e-01,\n",
       "           3.1420e-01,  3.3563e-01,  7.9613e-02, -6.2080e-01,  2.9033e-01,\n",
       "           5.0830e-01, -3.7001e-01, -2.1508e-01, -8.2633e-01,  5.3360e-01,\n",
       "           2.3634e-02, -5.6023e-01, -1.3095e-01,  1.3029e-01, -6.4150e+00,\n",
       "          -3.3860e-01, -3.2213e-02,  5.3964e-01, -5.5691e-01, -9.4326e-02,\n",
       "          -4.9920e-01, -7.1323e-01, -2.7627e-01, -2.5626e-01, -4.8774e-02,\n",
       "          -2.0168e+00, -3.7258e-01,  5.3224e-01, -1.0354e-02, -1.3404e+00,\n",
       "          -1.0159e+00, -3.6293e-01,  1.7592e-01, -9.3403e-02, -4.4597e+00,\n",
       "          -9.5616e-02, -1.2394e-01, -4.9033e-01, -1.9430e-01, -2.4922e-01,\n",
       "          -3.0100e-01, -3.4951e-01,  1.4495e-01, -2.8574e-01,  3.7177e-02,\n",
       "           5.2442e-01,  2.1758e-01,  1.9034e-01, -5.2843e-01,  6.1772e-01,\n",
       "           5.2984e-01,  4.5948e-01,  8.0302e-01, -6.0084e-01, -5.9663e-01,\n",
       "          -5.1359e-02,  1.2151e+00,  1.8258e+00,  9.3524e-02,  1.2994e-05,\n",
       "           6.7404e-01,  2.0095e-01, -5.2243e-03,  2.7663e-01,  2.3089e-02,\n",
       "          -7.9048e+00,  7.0561e-02, -3.9194e-02, -5.5982e-01, -8.6481e-01,\n",
       "           1.4348e-01, -3.7274e-02,  1.5259e-01, -1.1406e-01,  3.8537e-01,\n",
       "           3.4082e-01, -1.4117e-01, -3.5772e-01, -4.3148e-01, -1.6409e-01,\n",
       "          -2.4632e-02,  5.3751e-01,  1.1220e-01, -3.9713e-01, -1.2768e-01,\n",
       "          -7.6133e-01,  3.2180e-01,  1.2555e-01, -2.2908e-01, -7.0453e-01,\n",
       "           1.5259e+00, -3.3994e-01,  4.9425e-01,  2.5796e-01,  7.1763e-02,\n",
       "          -4.4537e-01, -5.2255e-02,  6.0656e-01]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation[\"layer0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add encoding classes with class methods\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Iterable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from sequence_models.pretrained import load_model_and_alphabet\n",
    "\n",
    "from scr.params.emb import TRANSFORMER_INFO, TRANSFORMER_MAX_SEQ_LEN, CARP_INFO\n",
    "from scr.params.sys import DEVICE\n",
    "\n",
    "\n",
    "class AbstractEncoder(ABC):\n",
    "    \"\"\"\n",
    "    An abstract encoder class to fill in for different kinds of encoders\n",
    "\n",
    "    All encoders will have an \"encode\" function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_name: str, embed_layer: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        self._encoder_name = encoder_name\n",
    "        self._embed_layer = embed_layer\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> Iterable[np.ndarray]:\n",
    "        \"\"\"\n",
    "        A function takes a list of sequences to yield a batch of encoded elements\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_seqs, str):\n",
    "            mut_seqs = [mut_seqs]\n",
    "\n",
    "        # If the batch size is 0, then encode all at once in a single batch\n",
    "        if batch_size == 0:\n",
    "            yield self._encode_batch(\n",
    "                mut_seqs=mut_seqs, flatten_emb=flatten_emb, mut_names=mut_names\n",
    "            )\n",
    "\n",
    "        # Otherwise, yield chunks of encoded sequence\n",
    "        else:\n",
    "\n",
    "            for i in tqdm(range(0, len(mut_seqs), batch_size)):\n",
    "\n",
    "                # figure out what mut_names to feed in\n",
    "                if mut_names is None:\n",
    "                    mut_name_batch = mut_names\n",
    "                else:\n",
    "                    mut_name_batch = mut_names[i : i + batch_size]\n",
    "\n",
    "                yield self._encode_batch(\n",
    "                    mut_seqs=mut_seqs[i : i + batch_size],\n",
    "                    flatten_emb=flatten_emb,\n",
    "                    mut_names=mut_name_batch,\n",
    "                )\n",
    "\n",
    "    def flatten_encode(\n",
    "        self, encoded_mut_seqs: np.ndarray, flatten_emb: bool | str\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Flatten the embedding or just return the encoded mutants.\n",
    "\n",
    "        Args:\n",
    "        - encoded_mut_seqs: np.ndarray, shape [batch_size, seq_len, embed_dim]\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "            - True -> shape [batch_size, seq_len * embed_dim]\n",
    "            - \"max\" or \"mean\" -> shape [batch_size, embed_dim]\n",
    "            - False or everything else -> [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, shape depends on flatten_emb parameter\n",
    "        \"\"\"\n",
    "        assert encoded_mut_seqs.shape[2] == self._embed_dim, \"Wrong embed dim\"\n",
    "\n",
    "        if flatten_emb is True:\n",
    "            # shape [batch_size, seq_len * embed_dim]\n",
    "            return encoded_mut_seqs.reshape(encoded_mut_seqs.shape[0], -1)\n",
    "\n",
    "        elif isinstance(flatten_emb, str):\n",
    "            if flatten_emb == \"mean\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.mean(axis=1)\n",
    "            elif flatten_emb == \"max\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.max(axis=1)\n",
    "\n",
    "        else:\n",
    "            print(\"No embedding flattening\")\n",
    "            # [batch_size, seq_len, embed_dim]\n",
    "            return encoded_mut_seqs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _encode_batch(\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode a single batch of mut_seqs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def embed_dim(self) -> int:\n",
    "        \"\"\"The dim of the embedding\"\"\"\n",
    "        return self._embed_dim\n",
    "\n",
    "    @property\n",
    "    def embed_layer(self) -> int:\n",
    "        \"\"\"The layer nubmer of the embedding\"\"\"\n",
    "        return self._embed_layer\n",
    "\n",
    "    @property\n",
    "    def encoder_name(self) -> str:\n",
    "        \"\"\"The name of the encoding method\"\"\"\n",
    "        return self._encoder_name\n",
    "\n",
    "\n",
    "class ESMEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build an ESM encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "        iftrimCLS: bool = True,\n",
    "        iftrimEOS: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of TRANSFORMER_INFO\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        - iftrimCLS: bool, whether to trim the first classifification token\n",
    "        - iftrimEOS: bool, whether to trim the end of sequence token, if exists\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, embed_layer)\n",
    "\n",
    "        self._iftrimCLS = iftrimCLS\n",
    "        self._iftrimEOS = iftrimEOS\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(f\"Loading {self._encoder_name} using {self._embed_layer} layer embedding\")\n",
    "        self.model, self.alphabet = torch.hub.load(\n",
    "            \"facebookresearch/esm:main\", model=self._encoder_name\n",
    "        )\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self._embed_dim, self._max_emb_layer, _ = TRANSFORMER_INFO[self._encoder_name]\n",
    "\n",
    "        assert (\n",
    "            self._embed_layer <= self._max_emb_layer\n",
    "        ), f\"{self._embed_layer} exceeds {self._max_emb_layer}\"\n",
    "\n",
    "        expected_num_layers = int(self._encoder_name.split(\"_\")[-3][1:])\n",
    "        assert (\n",
    "            expected_num_layers == self._max_emb_layer\n",
    "        ), \"Wrong ESM model name or layer\"\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_names, str):\n",
    "            mut_names = [mut_names]\n",
    "\n",
    "        # pair the mut_names and mut_seqs\n",
    "        if mut_names is not None:\n",
    "            assert len(mut_names) == len(\n",
    "                mut_seqs\n",
    "            ), \"mutant_name and mut_seqs different length\"\n",
    "            mut_seqs = [(n, m) for (n, m) in zip(mut_names, mut_seqs)]\n",
    "        else:\n",
    "            mut_seqs = [(\"\", m) for m in mut_seqs]\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        batch_labels, _, batch_tokens = self.batch_converter(mut_seqs)\n",
    "        batch_tokens = batch_tokens.to(DEVICE)\n",
    "\n",
    "        # Turn off gradients and pass the batch through\n",
    "        with torch.no_grad():\n",
    "            # shape [batch_size, seq_len + pad, embed_dim]\n",
    "            if batch_tokens.shape[1] > TRANSFORMER_MAX_SEQ_LEN:\n",
    "                print(f\"Sequence exceeds {TRANSFORMER_MAX_SEQ_LEN}, chopping the end\")\n",
    "                batch_tokens = batch_tokens[:, :TRANSFORMER_MAX_SEQ_LEN]\n",
    "\n",
    "            encoded_mut_seqs = (\n",
    "                self.model(batch_tokens, repr_layers=[self._embed_layer])[\n",
    "                    \"representations\"\n",
    "                ][self._embed_layer]\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        # https://github.com/facebookresearch/esm/blob/main/esm/data.py\n",
    "        # from_architecture\n",
    "\n",
    "        # trim off initial classification token [CLS]\n",
    "        # both \"ESM-1\" and \"ESM-1b\" have prepend_bos = True\n",
    "        if self._iftrimCLS and self._encoder_name.split(\"_\")[0] in [\"esm1\", \"esm1b\"]:\n",
    "            encoded_mut_seqs = encoded_mut_seqs[:, 1:, :]\n",
    "\n",
    "        # trim off end-of-sequence token [EOS]\n",
    "        # only \"ESM-1b\" has append_eos = True\n",
    "        if self._iftrimEOS and self._encoder_name.split(\"_\")[0] == \"esm1b\":\n",
    "            encoded_mut_seqs = encoded_mut_seqs[:, :-1, :]\n",
    "\n",
    "        if mut_names is not None:\n",
    "            return self.flatten_encode(encoded_mut_seqs, flatten_emb), batch_labels\n",
    "        else:\n",
    "            return self.flatten_encode(encoded_mut_seqs, flatten_emb)\n",
    "\n",
    "\n",
    "class CARPEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build a CARP encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of CARP_INFO\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, embed_layer)\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(f\"Loading {self._encoder_name} using {self._embed_layer} layer embedding\")\n",
    "\n",
    "        self.model, self.collater = load_model_and_alphabet(self._encoder_name)\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self._embed_dim, self._max_emb_layer = CARP_INFO[self._encoder_name]\n",
    "\n",
    "        assert (\n",
    "            self._embed_layer <= self._max_emb_layer\n",
    "        ), f\"{self._embed_layer} exceeds {self._max_emb_layer}\"\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        mut_seqs = [[m] for m in mut_seqs]\n",
    "        \n",
    "        x = self.collater(mut_seqs)[0]\n",
    "\n",
    "        layer_name = f\"layer{str(self._embed_layer)}\"\n",
    "\n",
    "        activation = {}\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        self.model.model.embedder.layers[self._embed_layer].register_forward_hook(\n",
    "            get_activation(layer_name)\n",
    "        )\n",
    "\n",
    "        rep = self.model(x)\n",
    "        \n",
    "        encoded_mut_seqs = activation[layer_name].cpu().numpy()\n",
    "\n",
    "        return self.flatten_encode(encoded_mut_seqs, flatten_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.utils import pickle_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle_load(\"data/proeng/gb1/two_vs_rest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 43, 8309, 8733)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] != True)]\n",
    "df_val = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] == True)]\n",
    "df_test = df.loc[(df[\"set\"] == \"test\")]\n",
    "\n",
    "len(df_train), len(df_val), len(df_test), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['carp_600k', 'carp_38M', 'carp_76M', 'carp_640M'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CARP_INFO.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading carp_600k using 0 layer embedding\n",
      "No embedding flattening\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, 56, 128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_flat_encoder = CARPEncoder(\n",
    "    encoder_name=\"carp_600k\",\n",
    "    embed_layer=0,\n",
    ").encode(mut_seqs=list(df_val.sequence.astype(str).str[0 : 56].values))\n",
    "one_emb = next(no_flat_encoder)\n",
    "one_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading carp_600k using 0 layer embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_flat_encoder = CARPEncoder(\n",
    "    encoder_name=\"carp_600k\",\n",
    "    embed_layer=0,\n",
    ").encode(mut_seqs=list(df_val.sequence.astype(str).str[0 : 56].values),flatten_emb=\"mean\")\n",
    "one_mean_emb = next(mean_flat_encoder)\n",
    "one_mean_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11233792, -0.03096455,  0.09588867, ..., -0.2151581 ,\n",
       "        -0.41078562,  0.8417188 ],\n",
       "       [ 0.2050094 , -0.14373404, -0.15035413, ...,  0.13152504,\n",
       "        -0.12934083,  0.50128305],\n",
       "       [ 0.4236132 , -0.09426624,  0.04951383, ...,  0.22084078,\n",
       "        -3.9057856 ,  0.79270667],\n",
       "       ...,\n",
       "       [ 0.31486923, -0.00696398,  0.12247562, ..., -0.05261476,\n",
       "        -4.633561  ,  0.5593643 ],\n",
       "       [ 0.27489296, -0.01615845,  0.14888933, ..., -0.03519906,\n",
       "         0.37774575,  0.5697537 ],\n",
       "       [ 0.34990993,  0.256608  ,  0.21758929, ...,  0.10277098,\n",
       "        -3.0541036 ,  0.31116045]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\"\\x19\\xfc']\n",
      "Bad pipe message: %s [b\"z\\xb5\\xbb\\xa8z\\xf3a\\x80\\xb0\\xa4^/\\xbb\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\", b'#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03']\n",
      "Bad pipe message: %s [b'\\x08\\x08\\x08\\t\\x08\\n\\x08', b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xae}!R\\xf0\\xf1>\\xfc\\x9a\\xecd0\\x08\\x1b\\x8d\\x9a\\xfc\\xd7\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0']\n",
      "Bad pipe message: %s [b\"k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\"]\n",
      "Bad pipe message: %s [b'\\x05g\\x1c\\x1c\\x91\\x82p\\x82+T\\xc5pP\\xdavP\\xc1\\xc1\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0']\n",
      "Bad pipe message: %s [b'\\x16\\x00\\x13\\x00\\x10\\x00\\r']\n",
      "Bad pipe message: %s [b'\\xc4B\\x9f\\x82\\x0c\\x9e\\x94\\x06%\\xfb\\xc6\\x13\\x1f\\xf2\\xec|e\\xd8\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01']\n",
      "Bad pipe message: %s [b'\\n']\n",
      "Bad pipe message: %s [b\"\\xee\\x99\\x1f8\\xed\\xb3\\xbdC\\xf9\\x1a\\xa4\\x89\\xe6I\\xd5?)\\x82\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\", b'\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02\\x04\\x03\\x03\\x01\\x03\\x02\\x03\\x03\\x02']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x03']\n",
      "Bad pipe message: %s [b\"\\x88\\x8f\\xccBY\\xbeD\\xf7/\\xc05\\x01\\x91\\xf3d\\xbc\\xb2\\x02\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00\"]\n",
      "Bad pipe message: %s [b'B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0']\n",
      "Bad pipe message: %s [b'\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00']\n",
      "Bad pipe message: %s [b'\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0']\n",
      "Bad pipe message: %s [b'\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\\x02']\n",
      "Bad pipe message: %s [b'']\n"
     ]
    }
   ],
   "source": [
    "one_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
