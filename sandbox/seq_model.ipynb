{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add encoding classes with class methods\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Iterable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from sequence_models.pretrained import load_model_and_alphabet\n",
    "\n",
    "from scr.params.emb import TRANSFORMER_INFO, TRANSFORMER_MAX_SEQ_LEN, CARP_INFO\n",
    "from scr.params.sys import DEVICE\n",
    "\n",
    "\n",
    "class AbstractEncoder(ABC):\n",
    "    \"\"\"\n",
    "    An abstract encoder class to fill in for different kinds of encoders\n",
    "\n",
    "    All encoders will have an \"encode\" function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_name: str, embed_layer: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        self._encoder_name = encoder_name\n",
    "        self._embed_layer = embed_layer\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> Iterable[np.ndarray]:\n",
    "        \"\"\"\n",
    "        A function takes a list of sequences to yield a batch of encoded elements\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_seqs, str):\n",
    "            mut_seqs = [mut_seqs]\n",
    "\n",
    "        # If the batch size is 0, then encode all at once in a single batch\n",
    "        if batch_size == 0:\n",
    "            yield self._encode_batch(\n",
    "                mut_seqs=mut_seqs, flatten_emb=flatten_emb, mut_names=mut_names\n",
    "            )\n",
    "\n",
    "        # Otherwise, yield chunks of encoded sequence\n",
    "        else:\n",
    "\n",
    "            for i in tqdm(range(0, len(mut_seqs), batch_size)):\n",
    "\n",
    "                # figure out what mut_names to feed in\n",
    "                if mut_names is None:\n",
    "                    mut_name_batch = mut_names\n",
    "                else:\n",
    "                    mut_name_batch = mut_names[i : i + batch_size]\n",
    "\n",
    "                yield self._encode_batch(\n",
    "                    mut_seqs=mut_seqs[i : i + batch_size],\n",
    "                    flatten_emb=flatten_emb,\n",
    "                    mut_names=mut_name_batch,\n",
    "                )\n",
    "\n",
    "    def flatten_encode(\n",
    "        self, encoded_mut_seqs: np.ndarray, flatten_emb: bool | str\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Flatten the embedding or just return the encoded mutants.\n",
    "\n",
    "        Args:\n",
    "        - encoded_mut_seqs: np.ndarray, shape [batch_size, seq_len, embed_dim]\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "            - True -> shape [batch_size, seq_len * embed_dim]\n",
    "            - \"max\" or \"mean\" -> shape [batch_size, embed_dim]\n",
    "            - False or everything else -> [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, shape depends on flatten_emb parameter\n",
    "        \"\"\"\n",
    "        assert encoded_mut_seqs.shape[2] == self._embed_dim, \"Wrong embed dim\"\n",
    "\n",
    "        if flatten_emb is True:\n",
    "            # shape [batch_size, seq_len * embed_dim]\n",
    "            return encoded_mut_seqs.reshape(encoded_mut_seqs.shape[0], -1)\n",
    "\n",
    "        elif isinstance(flatten_emb, str):\n",
    "            if flatten_emb == \"mean\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.mean(axis=1)\n",
    "            elif flatten_emb == \"max\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.max(axis=1)\n",
    "\n",
    "        else:\n",
    "            print(\"No embedding flattening\")\n",
    "            # [batch_size, seq_len, embed_dim]\n",
    "            return encoded_mut_seqs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _encode_batch(\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode a single batch of mut_seqs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def embed_dim(self) -> int:\n",
    "        \"\"\"The dim of the embedding\"\"\"\n",
    "        return self._embed_dim\n",
    "\n",
    "    @property\n",
    "    def embed_layer(self) -> int:\n",
    "        \"\"\"The layer nubmer of the embedding\"\"\"\n",
    "        return self._embed_layer\n",
    "\n",
    "    @property\n",
    "    def encoder_name(self) -> str:\n",
    "        \"\"\"The name of the encoding method\"\"\"\n",
    "        return self._encoder_name\n",
    "\n",
    "\n",
    "class ESMEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build an ESM encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "        iftrimCLS: bool = True,\n",
    "        iftrimEOS: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of TRANSFORMER_INFO\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        - iftrimCLS: bool, whether to trim the first classifification token\n",
    "        - iftrimEOS: bool, whether to trim the end of sequence token, if exists\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, embed_layer)\n",
    "\n",
    "        self._iftrimCLS = iftrimCLS\n",
    "        self._iftrimEOS = iftrimEOS\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(f\"Loading {self._encoder_name} using {self._embed_layer} layer embedding\")\n",
    "        self.model, self.alphabet = torch.hub.load(\n",
    "            \"facebookresearch/esm:main\", model=self._encoder_name\n",
    "        )\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self._embed_dim, self._max_emb_layer, _ = TRANSFORMER_INFO[self._encoder_name]\n",
    "\n",
    "        assert (\n",
    "            self._embed_layer <= self._max_emb_layer\n",
    "        ), f\"{self._embed_layer} exceeds {self._max_emb_layer}\"\n",
    "\n",
    "        expected_num_layers = int(self._encoder_name.split(\"_\")[-3][1:])\n",
    "        assert (\n",
    "            expected_num_layers == self._max_emb_layer\n",
    "        ), \"Wrong ESM model name or layer\"\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_names, str):\n",
    "            mut_names = [mut_names]\n",
    "\n",
    "        # pair the mut_names and mut_seqs\n",
    "        if mut_names is not None:\n",
    "            assert len(mut_names) == len(\n",
    "                mut_seqs\n",
    "            ), \"mutant_name and mut_seqs different length\"\n",
    "            mut_seqs = [(n, m) for (n, m) in zip(mut_names, mut_seqs)]\n",
    "        else:\n",
    "            mut_seqs = [(\"\", m) for m in mut_seqs]\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        batch_labels, _, batch_tokens = self.batch_converter(mut_seqs)\n",
    "        batch_tokens = batch_tokens.to(DEVICE)\n",
    "\n",
    "        # Turn off gradients and pass the batch through\n",
    "        with torch.no_grad():\n",
    "            # shape [batch_size, seq_len + pad, embed_dim]\n",
    "            if batch_tokens.shape[1] > TRANSFORMER_MAX_SEQ_LEN:\n",
    "                print(f\"Sequence exceeds {TRANSFORMER_MAX_SEQ_LEN}, chopping the end\")\n",
    "                batch_tokens = batch_tokens[:, :TRANSFORMER_MAX_SEQ_LEN]\n",
    "\n",
    "            encoded_mut_seqs = (\n",
    "                self.model(batch_tokens, repr_layers=[self._embed_layer])[\n",
    "                    \"representations\"\n",
    "                ][self._embed_layer]\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        # https://github.com/facebookresearch/esm/blob/main/esm/data.py\n",
    "        # from_architecture\n",
    "\n",
    "        # trim off initial classification token [CLS]\n",
    "        # both \"ESM-1\" and \"ESM-1b\" have prepend_bos = True\n",
    "        if self._iftrimCLS and self._encoder_name.split(\"_\")[0] in [\"esm1\", \"esm1b\"]:\n",
    "            encoded_mut_seqs = encoded_mut_seqs[:, 1:, :]\n",
    "\n",
    "        # trim off end-of-sequence token [EOS]\n",
    "        # only \"ESM-1b\" has append_eos = True\n",
    "        if self._iftrimEOS and self._encoder_name.split(\"_\")[0] == \"esm1b\":\n",
    "            encoded_mut_seqs = encoded_mut_seqs[:, :-1, :]\n",
    "\n",
    "        if mut_names is not None:\n",
    "            return self.flatten_encode(encoded_mut_seqs, flatten_emb), batch_labels\n",
    "        else:\n",
    "            return self.flatten_encode(encoded_mut_seqs, flatten_emb)\n",
    "\n",
    "\n",
    "class CARPEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build a CARP encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of CARP_INFO\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, embed_layer)\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(f\"Loading {self._encoder_name} using {self._embed_layer} layer embedding\")\n",
    "\n",
    "        self.model, self.collater = load_model_and_alphabet(self._encoder_name)\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self._embed_dim, self._max_emb_layer = CARP_INFO[self._encoder_name]\n",
    "\n",
    "        assert (\n",
    "            self._embed_layer <= self._max_emb_layer\n",
    "        ), f\"{self._embed_layer} exceeds {self._max_emb_layer}\"\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        mut_seqs = [[m] for m in mut_seqs]\n",
    "\n",
    "        x = self.collater(mut_seqs)[0]\n",
    "\n",
    "        layer_name = f\"layer{str(self._embed_layer)}\"\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        self.model.model.embedder.layers[self._embed_layer].register_forward_hook(\n",
    "            get_activation(layer_name)\n",
    "        )\n",
    "\n",
    "        rep = self.model(x)\n",
    "\n",
    "        encoded_mut_seqs = activation[layer_name].cpu().numpy()\n",
    "\n",
    "        return self.flatten_encode(encoded_mut_seqs, flatten_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.encoding.encoding_classes import CARPEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.utils import pickle_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle_load(\"data/proeng/gb1/two_vs_rest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 43, 8309, 8733)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] != True)]\n",
    "df_val = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] == True)]\n",
    "df_test = df.loc[(df[\"set\"] == \"test\")]\n",
    "\n",
    "len(df_train), len(df_val), len(df_test), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVNGEWTYDDATKTFTVTE']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seqs = [df_val.sequence.astype(str).str[0 : 56].values[0]]\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading carp_600k using 0 layer embedding\n",
      "No embedding flattening\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.11233783, -0.03096454,  0.09588867, ..., -0.21515825,\n",
       "          -0.41078544,  0.8417189 ],\n",
       "         [ 0.20500943, -0.14373404, -0.15035412, ...,  0.13152511,\n",
       "          -0.12934107,  0.50128317],\n",
       "         [ 0.4236132 , -0.09426615,  0.04951378, ...,  0.22084077,\n",
       "          -3.9057853 ,  0.7927066 ],\n",
       "         ...,\n",
       "         [ 0.31486928, -0.00696401,  0.12247568, ..., -0.05261481,\n",
       "          -4.633561  ,  0.55936426],\n",
       "         [ 0.27489296, -0.01615845,  0.14888927, ..., -0.03519902,\n",
       "           0.37774587,  0.5697537 ],\n",
       "         [ 0.34991002,  0.25660813,  0.21758929, ...,  0.10277108,\n",
       "          -3.0541043 ,  0.31116033]]], dtype=float32),\n",
       " (1, 56, 128))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_flat_encoder = CARPEncoder(\n",
    "    encoder_name=\"carp_600k\",\n",
    "    embed_layer=0,\n",
    ").encode(mut_seqs=seqs)\n",
    "one_emb = next(no_flat_encoder)\n",
    "one_emb, one_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading carp_600k using 0 layer embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.64436132e-01,  6.57109246e-02,  8.94564912e-02,\n",
       "          3.09289932e-01,  1.32049993e-01,  1.55501708e-01,\n",
       "          1.28102107e-02,  1.46981403e-01, -9.05814171e-02,\n",
       "         -1.82426244e-01,  1.15854070e-01,  4.80966233e-02,\n",
       "          2.20418856e-01,  2.25005895e-01,  9.72223431e-02,\n",
       "          5.32191932e-01,  7.49111250e-02,  3.54565412e-01,\n",
       "          7.33187646e-02,  1.70479506e-01,  3.80291134e-01,\n",
       "         -8.53402689e-02, -6.54226467e-02,  3.57502520e-01,\n",
       "          1.94397107e-01, -5.21607280e-01, -2.41224188e-02,\n",
       "          3.47265989e-01, -1.94647282e-01,  5.34702502e-02,\n",
       "          3.03542484e-02,  7.43700191e-02,  6.19242154e-02,\n",
       "         -1.18623592e-01,  1.64853826e-01,  9.36362669e-02,\n",
       "         -9.65878293e-02,  1.05360612e-01,  5.56887463e-02,\n",
       "          1.70761794e-01,  1.93100184e-01,  1.66344896e-01,\n",
       "         -5.06169535e-02, -2.64716838e-02, -1.86315969e-01,\n",
       "          9.10815150e-02, -3.17426980e-03, -1.68990362e-02,\n",
       "         -2.34090220e-02,  1.71465054e-01,  9.12062526e-02,\n",
       "          4.91209216e-02, -4.19990672e-03, -1.33241728e-01,\n",
       "         -7.63184354e-02,  2.96926945e-01,  9.24300104e-02,\n",
       "         -1.71542436e-01,  7.09898770e-02, -6.78596079e-01,\n",
       "         -1.58266038e-01,  7.84832984e-02,  5.86537048e-02,\n",
       "         -1.57210715e-02, -5.13545156e-01, -6.56048656e-02,\n",
       "          1.31600901e-01, -1.28430739e-01, -6.75263256e-02,\n",
       "          6.53730929e-02, -1.43184900e-01,  7.61494040e-02,\n",
       "          3.01798195e-01, -1.87858567e-01,  1.87035561e-01,\n",
       "          1.17592596e-01, -1.43412575e-01,  1.82244912e-01,\n",
       "         -5.93267605e-02,  4.63767797e-01,  5.52744232e-02,\n",
       "         -3.08393850e-03,  2.51453727e-01,  7.24663585e-02,\n",
       "          1.48737788e-01,  2.80103356e-01,  7.30815083e-02,\n",
       "         -3.48094881e-01,  5.72386310e-02,  2.79256821e-01,\n",
       "         -7.61862770e-02,  1.36029825e-01, -1.68764926e-02,\n",
       "         -4.73286435e-02,  6.34552836e-02, -3.38880110e+00,\n",
       "         -2.80784518e-02,  1.89122424e-01,  6.50750995e-02,\n",
       "          2.40657330e-01,  7.69611970e-02,  9.61573329e-03,\n",
       "          1.49253473e-01,  7.22483248e-02, -1.34439975e-01,\n",
       "          4.43967670e-01,  1.43314051e-02,  4.29973118e-02,\n",
       "          1.82131559e-01, -3.20435055e-02, -3.27946283e-02,\n",
       "          1.79266483e-01, -9.17897746e-02,  1.89919975e-02,\n",
       "         -1.13470837e-01,  1.10368632e-01,  7.99489766e-02,\n",
       "          1.39933974e-01,  2.49145061e-01, -6.36427328e-02,\n",
       "         -5.23838878e-01,  3.22732478e-01,  4.10858095e-02,\n",
       "          2.18973920e-01,  2.99142511e-03,  7.12214559e-02,\n",
       "         -1.54286826e+00,  4.05713260e-01]], dtype=float32),\n",
       " (1, 128))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_flat_encoder = CARPEncoder(\n",
    "    encoder_name=\"carp_600k\",\n",
    "    embed_layer=0,\n",
    ").encode(mut_seqs=seqs,flatten_emb=\"mean\")\n",
    "one_mean_emb = next(mean_flat_encoder)\n",
    "one_mean_emb, one_mean_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1123, -0.0310,  0.0959,  ..., -0.2152, -0.4108,  0.8417],\n",
       "          [ 0.2050, -0.1437, -0.1504,  ...,  0.1315, -0.1293,  0.5013],\n",
       "          [ 0.4236, -0.0943,  0.0495,  ...,  0.2208, -3.9058,  0.7927],\n",
       "          ...,\n",
       "          [ 0.3149, -0.0070,  0.1225,  ..., -0.0526, -4.6336,  0.5594],\n",
       "          [ 0.2749, -0.0162,  0.1489,  ..., -0.0352,  0.3777,  0.5698],\n",
       "          [ 0.3499,  0.2566,  0.2176,  ...,  0.1028, -3.0541,  0.3112]]]),\n",
       " torch.Size([1, 56, 128]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, collater = load_model_and_alphabet('carp_600k')\n",
    "\n",
    "x = collater([seqs])[0]  # (n, max_len)\n",
    "# rep = model(x)  # (n, max_len, d_model)\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.model.embedder.layers[0].register_forward_hook(get_activation(\"layer0\"))\n",
    "\n",
    "rep = model(x)\n",
    "\n",
    "activation[\"layer0\"], activation[\"layer0\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
