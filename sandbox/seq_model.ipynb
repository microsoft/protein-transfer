{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add encoding classes with class methods\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Iterable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from sequence_models.pretrained import load_model_and_alphabet\n",
    "\n",
    "from scr.params.emb import TRANSFORMER_INFO, TRANSFORMER_MAX_SEQ_LEN, CARP_INFO\n",
    "from scr.params.sys import DEVICE\n",
    "\n",
    "\n",
    "class AbstractEncoder(ABC):\n",
    "    \"\"\"\n",
    "    An abstract encoder class to fill in for different kinds of encoders\n",
    "\n",
    "    All encoders will have an \"encode\" function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_name: str, embed_layer: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        self._encoder_name = encoder_name\n",
    "        self._embed_layer = embed_layer\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> Iterable[np.ndarray]:\n",
    "        \"\"\"\n",
    "        A function takes a list of sequences to yield a batch of encoded elements\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_seqs, str):\n",
    "            mut_seqs = [mut_seqs]\n",
    "\n",
    "        # If the batch size is 0, then encode all at once in a single batch\n",
    "        if batch_size == 0:\n",
    "            yield self._encode_batch(\n",
    "                mut_seqs=mut_seqs, flatten_emb=flatten_emb, mut_names=mut_names\n",
    "            )\n",
    "\n",
    "        # Otherwise, yield chunks of encoded sequence\n",
    "        else:\n",
    "\n",
    "            for i in tqdm(range(0, len(mut_seqs), batch_size)):\n",
    "\n",
    "                # figure out what mut_names to feed in\n",
    "                if mut_names is None:\n",
    "                    mut_name_batch = mut_names\n",
    "                else:\n",
    "                    mut_name_batch = mut_names[i : i + batch_size]\n",
    "\n",
    "                yield self._encode_batch(\n",
    "                    mut_seqs=mut_seqs[i : i + batch_size],\n",
    "                    flatten_emb=flatten_emb,\n",
    "                    mut_names=mut_name_batch,\n",
    "                )\n",
    "\n",
    "    def flatten_encode(\n",
    "        self, encoded_mut_seqs: np.ndarray, flatten_emb: bool | str\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Flatten the embedding or just return the encoded mutants.\n",
    "\n",
    "        Args:\n",
    "        - encoded_mut_seqs: np.ndarray, shape [batch_size, seq_len, embed_dim]\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "            - True -> shape [batch_size, seq_len * embed_dim]\n",
    "            - \"max\" or \"mean\" -> shape [batch_size, embed_dim]\n",
    "            - False or everything else -> [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, shape depends on flatten_emb parameter\n",
    "        \"\"\"\n",
    "        assert encoded_mut_seqs.shape[2] == self._embed_dim, \"Wrong embed dim\"\n",
    "\n",
    "        if flatten_emb is True:\n",
    "            # shape [batch_size, seq_len * embed_dim]\n",
    "            return encoded_mut_seqs.reshape(encoded_mut_seqs.shape[0], -1)\n",
    "\n",
    "        elif isinstance(flatten_emb, str):\n",
    "            if flatten_emb == \"mean\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.mean(axis=1)\n",
    "            elif flatten_emb == \"max\":\n",
    "                # [batch_size, embed_dim]\n",
    "                return encoded_mut_seqs.max(axis=1)\n",
    "\n",
    "        else:\n",
    "            print(\"No embedding flattening\")\n",
    "            # [batch_size, seq_len, embed_dim]\n",
    "            return encoded_mut_seqs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _encode_batch(\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode a single batch of mut_seqs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def embed_dim(self) -> int:\n",
    "        \"\"\"The dim of the embedding\"\"\"\n",
    "        return self._embed_dim\n",
    "\n",
    "    @property\n",
    "    def embed_layer(self) -> int:\n",
    "        \"\"\"The layer nubmer of the embedding\"\"\"\n",
    "        return self._embed_layer\n",
    "\n",
    "    @property\n",
    "    def encoder_name(self) -> str:\n",
    "        \"\"\"The name of the encoding method\"\"\"\n",
    "        return self._encoder_name\n",
    "\n",
    "\n",
    "class ESMEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build an ESM encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "        iftrimCLS: bool = True,\n",
    "        iftrimEOS: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of TRANSFORMER_INFO\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        - iftrimCLS: bool, whether to trim the first classifification token\n",
    "        - iftrimEOS: bool, whether to trim the end of sequence token, if exists\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, embed_layer)\n",
    "\n",
    "        self._iftrimCLS = iftrimCLS\n",
    "        self._iftrimEOS = iftrimEOS\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(f\"Loading {self._encoder_name} using {self._embed_layer} layer embedding\")\n",
    "        self.model, self.alphabet = torch.hub.load(\n",
    "            \"facebookresearch/esm:main\", model=self._encoder_name\n",
    "        )\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self._embed_dim, self._max_emb_layer, _ = TRANSFORMER_INFO[self._encoder_name]\n",
    "\n",
    "        assert (\n",
    "            self._embed_layer <= self._max_emb_layer\n",
    "        ), f\"{self._embed_layer} exceeds {self._max_emb_layer}\"\n",
    "\n",
    "        expected_num_layers = int(self._encoder_name.split(\"_\")[-3][1:])\n",
    "        assert (\n",
    "            expected_num_layers == self._max_emb_layer\n",
    "        ), \"Wrong ESM model name or layer\"\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(mut_names, str):\n",
    "            mut_names = [mut_names]\n",
    "\n",
    "        # pair the mut_names and mut_seqs\n",
    "        if mut_names is not None:\n",
    "            assert len(mut_names) == len(\n",
    "                mut_seqs\n",
    "            ), \"mutant_name and mut_seqs different length\"\n",
    "            mut_seqs = [(n, m) for (n, m) in zip(mut_names, mut_seqs)]\n",
    "        else:\n",
    "            mut_seqs = [(\"\", m) for m in mut_seqs]\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        batch_labels, _, batch_tokens = self.batch_converter(mut_seqs)\n",
    "        batch_tokens = batch_tokens.to(DEVICE)\n",
    "\n",
    "        # Turn off gradients and pass the batch through\n",
    "        with torch.no_grad():\n",
    "            # shape [batch_size, seq_len + pad, embed_dim]\n",
    "            if batch_tokens.shape[1] > TRANSFORMER_MAX_SEQ_LEN:\n",
    "                print(f\"Sequence exceeds {TRANSFORMER_MAX_SEQ_LEN}, chopping the end\")\n",
    "                batch_tokens = batch_tokens[:, :TRANSFORMER_MAX_SEQ_LEN]\n",
    "\n",
    "            encoded_mut_seqs = (\n",
    "                self.model(batch_tokens, repr_layers=[self._embed_layer])[\n",
    "                    \"representations\"\n",
    "                ][self._embed_layer]\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        # https://github.com/facebookresearch/esm/blob/main/esm/data.py\n",
    "        # from_architecture\n",
    "\n",
    "        # trim off initial classification token [CLS]\n",
    "        # both \"ESM-1\" and \"ESM-1b\" have prepend_bos = True\n",
    "        if self._iftrimCLS and self._encoder_name.split(\"_\")[0] in [\"esm1\", \"esm1b\"]:\n",
    "            encoded_mut_seqs = encoded_mut_seqs[:, 1:, :]\n",
    "\n",
    "        # trim off end-of-sequence token [EOS]\n",
    "        # only \"ESM-1b\" has append_eos = True\n",
    "        if self._iftrimEOS and self._encoder_name.split(\"_\")[0] == \"esm1b\":\n",
    "            encoded_mut_seqs = encoded_mut_seqs[:, :-1, :]\n",
    "\n",
    "        if mut_names is not None:\n",
    "            return self.flatten_encode(encoded_mut_seqs, flatten_emb), batch_labels\n",
    "        else:\n",
    "            return self.flatten_encode(encoded_mut_seqs, flatten_emb)\n",
    "\n",
    "\n",
    "class CARPEncoder(AbstractEncoder):\n",
    "    \"\"\"\n",
    "    Build a CARP encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        - encoder_name: str, the name of the encoder, one of the keys of CARP_INFO\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(encoder_name, embed_layer)\n",
    "\n",
    "        # load model from torch.hub\n",
    "        print(f\"Loading {self._encoder_name} using {self._embed_layer} layer embedding\")\n",
    "\n",
    "        self.model, self.collater = load_model_and_alphabet(self._encoder_name)\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        self._embed_dim, self._max_emb_layer = CARP_INFO[self._encoder_name]\n",
    "\n",
    "        assert (\n",
    "            self._embed_layer <= self._max_emb_layer\n",
    "        ), f\"{self._embed_layer} exceeds {self._max_emb_layer}\"\n",
    "\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        mut_seqs: Sequence[str] | str,\n",
    "        flatten_emb: bool | str,\n",
    "        mut_names: Sequence[str] | str | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a batch of mutant sequences.\n",
    "\n",
    "        Args:\n",
    "        - mut_seqs: list of str or str, mutant sequences of the same length\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - mut_names: list of str or str or None, mutant names\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray or a tuple(np.ndarray, list[str]) where the list is batch_labels\n",
    "        \"\"\"\n",
    "\n",
    "        mut_seqs = [[m] for m in mut_seqs]\n",
    "\n",
    "        x = self.collater(mut_seqs)[0]\n",
    "\n",
    "        layer_name = f\"layer{str(self._embed_layer)}\"\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # convert raw mutant sequences to tokens\n",
    "        self.model.model.embedder.layers[self._embed_layer].register_forward_hook(\n",
    "            get_activation(layer_name)\n",
    "        )\n",
    "\n",
    "        rep = self.model(x)\n",
    "\n",
    "        encoded_mut_seqs = activation[layer_name].cpu().numpy()\n",
    "\n",
    "        return self.flatten_encode(encoded_mut_seqs, flatten_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.encoding.encoding_classes import CARPEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.utils import pickle_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle_load(\"data/proeng/gb1/two_vs_rest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 43, 8309, 8733)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] != True)]\n",
    "df_val = df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] == True)]\n",
    "df_test = df.loc[(df[\"set\"] == \"test\")]\n",
    "\n",
    "len(df_train), len(df_val), len(df_test), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVNGEWTYDDATKTFTVTE'],\n",
       " ['MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGEYGEWTYDDATKTFTVTE']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seqs = [[seq] for seq in df_val.sequence.astype(str).str[0 : 56].values[0:2]]\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVNGEWTYDDATKTFTVTE',\n",
       " 'MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGEYGEWTYDDATKTFTVTE']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_val.sequence.astype(str).str[0 : 56].values[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading carp_600k using 0 layer embedding\n",
      "No embedding flattening\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.11233783, -0.03096454,  0.09588867, ..., -0.21515825,\n",
       "          -0.41078544,  0.8417189 ],\n",
       "         [ 0.20500943, -0.14373404, -0.15035412, ...,  0.13152511,\n",
       "          -0.12934107,  0.50128317],\n",
       "         [ 0.4236132 , -0.09426615,  0.04951378, ...,  0.22084077,\n",
       "          -3.9057853 ,  0.7927066 ],\n",
       "         ...,\n",
       "         [ 0.31486923, -0.00696404,  0.12247571, ..., -0.05261491,\n",
       "          -4.633561  ,  0.5593644 ],\n",
       "         [ 0.27489305, -0.0161584 ,  0.14888933, ..., -0.03519905,\n",
       "           0.37774587,  0.5697537 ],\n",
       "         [ 0.3499099 ,  0.2566081 ,  0.21758932, ...,  0.10277104,\n",
       "          -3.0541043 ,  0.31116042]],\n",
       " \n",
       "        [[ 0.11233783, -0.03096454,  0.09588867, ..., -0.21515825,\n",
       "          -0.41078544,  0.8417189 ],\n",
       "         [ 0.20500943, -0.14373404, -0.15035412, ...,  0.13152511,\n",
       "          -0.12934107,  0.50128317],\n",
       "         [ 0.4236132 , -0.09426615,  0.04951378, ...,  0.22084077,\n",
       "          -3.9057853 ,  0.7927066 ],\n",
       "         ...,\n",
       "         [ 0.31486923, -0.00696404,  0.12247571, ..., -0.05261491,\n",
       "          -4.633561  ,  0.5593644 ],\n",
       "         [ 0.27489305, -0.0161584 ,  0.14888933, ..., -0.03519905,\n",
       "           0.37774587,  0.5697537 ],\n",
       "         [ 0.3499099 ,  0.2566081 ,  0.21758932, ...,  0.10277104,\n",
       "          -3.0541043 ,  0.31116042]]], dtype=float32),\n",
       " (2, 56, 128))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_flat_encoder = CARPEncoder(\n",
    "    encoder_name=\"carp_600k\",\n",
    "    embed_layer=0,\n",
    ").encode(mut_seqs=list(df_val.sequence.astype(str).str[0 : 56].values[0:2]))\n",
    "one_emb = next(no_flat_encoder)\n",
    "one_emb, one_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 7168, 6400)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(one_emb[0] == one_emb[1]).all(),len((one_emb[0] == one_emb[1]).flatten()), (one_emb[0] == one_emb[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading carp_600k using 0 layer embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.64436132e-01,  6.57109246e-02,  8.94564912e-02,\n",
       "          3.09289932e-01,  1.32049978e-01,  1.55501693e-01,\n",
       "          1.28102070e-02,  1.46981403e-01, -9.05814171e-02,\n",
       "         -1.82426214e-01,  1.15854070e-01,  4.80966270e-02,\n",
       "          2.20418900e-01,  2.25005895e-01,  9.72223431e-02,\n",
       "          5.32191932e-01,  7.49111325e-02,  3.54565412e-01,\n",
       "          7.33187571e-02,  1.70479506e-01,  3.80291134e-01,\n",
       "         -8.53402615e-02, -6.54226616e-02,  3.57502520e-01,\n",
       "          1.94397107e-01, -5.21607280e-01, -2.41224188e-02,\n",
       "          3.47266048e-01, -1.94647282e-01,  5.34702502e-02,\n",
       "          3.03542484e-02,  7.43700191e-02,  6.19242191e-02,\n",
       "         -1.18623592e-01,  1.64853811e-01,  9.36362743e-02,\n",
       "         -9.65878293e-02,  1.05360612e-01,  5.56887314e-02,\n",
       "          1.70761794e-01,  1.93100184e-01,  1.66344896e-01,\n",
       "         -5.06169535e-02, -2.64716800e-02, -1.86315969e-01,\n",
       "          9.10815224e-02, -3.17427516e-03, -1.68990362e-02,\n",
       "         -2.34090183e-02,  1.71465069e-01,  9.12062600e-02,\n",
       "          4.91209254e-02, -4.19990253e-03, -1.33241728e-01,\n",
       "         -7.63184354e-02,  2.96926945e-01,  9.24300179e-02,\n",
       "         -1.71542451e-01,  7.09898844e-02, -6.78596079e-01,\n",
       "         -1.58266023e-01,  7.84832984e-02,  5.86537048e-02,\n",
       "         -1.57210771e-02, -5.13545156e-01, -6.56048581e-02,\n",
       "          1.31600901e-01, -1.28430724e-01, -6.75263330e-02,\n",
       "          6.53730929e-02, -1.43184900e-01,  7.61494040e-02,\n",
       "          3.01798195e-01, -1.87858567e-01,  1.87035561e-01,\n",
       "          1.17592610e-01, -1.43412575e-01,  1.82244912e-01,\n",
       "         -5.93267605e-02,  4.63767797e-01,  5.52744232e-02,\n",
       "         -3.08393571e-03,  2.51453727e-01,  7.24663585e-02,\n",
       "          1.48737773e-01,  2.80103326e-01,  7.30815157e-02,\n",
       "         -3.48094881e-01,  5.72386272e-02,  2.79256791e-01,\n",
       "         -7.61862621e-02,  1.36029825e-01, -1.68764945e-02,\n",
       "         -4.73286472e-02,  6.34552836e-02, -3.38880110e+00,\n",
       "         -2.80784480e-02,  1.89122424e-01,  6.50751144e-02,\n",
       "          2.40657330e-01,  7.69611970e-02,  9.61573143e-03,\n",
       "          1.49253473e-01,  7.22483248e-02, -1.34439975e-01,\n",
       "          4.43967670e-01,  1.43314051e-02,  4.29973193e-02,\n",
       "          1.82131574e-01, -3.20434980e-02, -3.27946320e-02,\n",
       "          1.79266483e-01, -9.17897746e-02,  1.89919956e-02,\n",
       "         -1.13470830e-01,  1.10368632e-01,  7.99489692e-02,\n",
       "          1.39933944e-01,  2.49145061e-01, -6.36427253e-02,\n",
       "         -5.23838878e-01,  3.22732478e-01,  4.10858057e-02,\n",
       "          2.18973920e-01,  2.99142092e-03,  7.12214634e-02,\n",
       "         -1.54286826e+00,  4.05713260e-01],\n",
       "        [ 1.66712284e-01,  7.06586689e-02,  9.68779251e-02,\n",
       "          3.30310762e-01,  1.30858153e-01,  1.62767664e-01,\n",
       "          2.55236626e-02,  1.52671605e-01, -7.44205043e-02,\n",
       "         -1.76054001e-01,  1.29199535e-01,  6.66932464e-02,\n",
       "          2.22138256e-01,  2.40246326e-01,  1.01717934e-01,\n",
       "          5.59611022e-01,  7.74183124e-02,  3.65471691e-01,\n",
       "          6.78174794e-02,  1.75123289e-01,  3.96152228e-01,\n",
       "         -8.18881169e-02, -6.04284927e-02,  3.66514295e-01,\n",
       "          1.98679522e-01, -5.16414344e-01, -4.37938748e-03,\n",
       "          3.39605063e-01, -1.95204884e-01,  6.08902648e-02,\n",
       "          3.84422913e-02,  7.63345286e-02,  6.22210912e-02,\n",
       "         -1.09308705e-01,  1.69923618e-01,  8.49255994e-02,\n",
       "         -9.06656310e-02,  9.61824805e-02,  3.09454668e-02,\n",
       "          1.69627219e-01,  2.07676545e-01,  1.59769610e-01,\n",
       "         -4.77404408e-02, -1.44066298e-02, -1.88151985e-01,\n",
       "          9.91719738e-02,  5.57765365e-04, -8.67397059e-03,\n",
       "         -1.07149314e-02,  1.72913313e-01,  1.02167025e-01,\n",
       "          4.22972925e-02, -4.64154128e-03, -1.24500461e-01,\n",
       "         -7.27467388e-02,  3.53786677e-01,  9.60156918e-02,\n",
       "         -1.92379922e-01,  7.35781938e-02, -6.64124608e-01,\n",
       "         -1.58412963e-01,  8.52239057e-02,  5.98904975e-02,\n",
       "         -2.01552343e-02, -4.72898066e-01, -7.22851083e-02,\n",
       "          1.39927372e-01, -1.16946474e-01, -7.23059028e-02,\n",
       "          6.78277463e-02, -1.40909404e-01,  8.77089128e-02,\n",
       "          3.04295957e-01, -1.89324304e-01,  1.97317764e-01,\n",
       "          1.35595590e-01, -1.48497686e-01,  1.92473248e-01,\n",
       "         -5.05982675e-02,  4.73875582e-01,  7.05663189e-02,\n",
       "          9.50386282e-03,  2.47689322e-01,  8.17338303e-02,\n",
       "          1.61032304e-01,  2.88912237e-01,  8.00387263e-02,\n",
       "         -3.40339452e-01,  6.38422370e-02,  2.86827743e-01,\n",
       "         -7.34000653e-02,  1.39015034e-01, -1.26962559e-02,\n",
       "         -3.89052294e-02,  7.45301917e-02, -3.39678311e+00,\n",
       "         -2.79379785e-02,  1.97646365e-01,  7.73528144e-02,\n",
       "          2.54833221e-01,  7.74596259e-02,  1.25992801e-02,\n",
       "          1.36963144e-01,  8.85622278e-02, -1.20601363e-01,\n",
       "          4.37776417e-01,  3.74369547e-02,  5.18357567e-02,\n",
       "          1.91807985e-01, -4.56367172e-02, -2.94643529e-02,\n",
       "          1.97546273e-01, -1.04173727e-01,  3.85574102e-02,\n",
       "         -1.21562839e-01,  1.14354953e-01,  9.90007296e-02,\n",
       "          1.45449117e-01,  2.47080222e-01, -4.31968160e-02,\n",
       "         -6.69084668e-01,  3.28419179e-01,  4.49110791e-02,\n",
       "          2.34283656e-01,  2.97746691e-03,  7.22306967e-02,\n",
       "         -1.56095755e+00,  4.16535109e-01]], dtype=float32),\n",
       " (2, 128))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_flat_encoder = CARPEncoder(\n",
    "    encoder_name=\"carp_600k\",\n",
    "    embed_layer=0,\n",
    ").encode(mut_seqs=list(df_val.sequence.astype(str).str[0 : 56].values[0:2]),flatten_emb=\"mean\")\n",
    "one_mean_emb = next(mean_flat_encoder)\n",
    "one_mean_emb, one_mean_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1123, -0.0310,  0.0959,  ..., -0.2152, -0.4108,  0.8417],\n",
       "          [ 0.2050, -0.1437, -0.1504,  ...,  0.1315, -0.1293,  0.5013],\n",
       "          [ 0.4236, -0.0943,  0.0495,  ...,  0.2208, -3.9058,  0.7927],\n",
       "          ...,\n",
       "          [ 0.3149, -0.0070,  0.1225,  ..., -0.0526, -4.6336,  0.5594],\n",
       "          [ 0.2749, -0.0162,  0.1489,  ..., -0.0352,  0.3777,  0.5698],\n",
       "          [ 0.3499,  0.2566,  0.2176,  ...,  0.1028, -3.0541,  0.3112]],\n",
       " \n",
       "         [[ 0.1123, -0.0310,  0.0959,  ..., -0.2152, -0.4108,  0.8417],\n",
       "          [ 0.2050, -0.1437, -0.1504,  ...,  0.1315, -0.1293,  0.5013],\n",
       "          [ 0.4236, -0.0943,  0.0495,  ...,  0.2208, -3.9058,  0.7927],\n",
       "          ...,\n",
       "          [ 0.3149, -0.0070,  0.1225,  ..., -0.0526, -4.6336,  0.5594],\n",
       "          [ 0.2749, -0.0162,  0.1489,  ..., -0.0352,  0.3777,  0.5698],\n",
       "          [ 0.3499,  0.2566,  0.2176,  ...,  0.1028, -3.0541,  0.3112]]]),\n",
       " torch.Size([2, 56, 128]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sequence_models.pretrained import load_model_and_alphabet\n",
    "\n",
    "model, collater = load_model_and_alphabet('carp_600k')\n",
    "\n",
    "x = collater(seqs)[0]  # (n, max_len)\n",
    "# rep = model(x)  # (n, max_len, d_model)\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.model.embedder.layers[0].register_forward_hook(get_activation(\"layer0\"))\n",
    "\n",
    "rep = model(x)\n",
    "\n",
    "activation[\"layer0\"], activation[\"layer0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
