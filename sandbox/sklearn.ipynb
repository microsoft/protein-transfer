{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.preprocess.data_process import split_protrain_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S using 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.59s/it]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S using 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S using 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [02:14<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = split_protrain_loader(\n",
    "    dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\",\n",
    "    encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "    embed_layer=6,\n",
    "    embed_batch_size=128,\n",
    "    flatten_emb=\"mean\",\n",
    "    embed_path=None,\n",
    "    seq_start_idx=0,\n",
    "    seq_end_idx=56,\n",
    "    subset_list=[\"train\", \"val\", \"test\"],\n",
    "    loader_batch_size=64,\n",
    "    worker_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with 0.5 alpha value\n",
    "model = Ridge(alpha = 0.5, normalize = True, tol = 0.001, \\\n",
    "              solver ='auto', random_state = 42)\n",
    "\n",
    "train_preds = []\n",
    "val_preds = []\n",
    "test_preds = []\n",
    "\n",
    "for (x, y, _, _, _) in train_loader:\n",
    "    model.fit(x, y)\n",
    "    \n",
    "for (x, y, _, _, _) in train_loader:\n",
    "    train_preds.append(model.predict(x))\n",
    "\n",
    "for (x, y, _, _, _) in val_loader:\n",
    "    val_preds.append(model.predict(x))\n",
    "\n",
    "# need to add to loop over alpha etc\n",
    "\n",
    "for (x, y, _, _, _) in test_loader:\n",
    "    test_preds.append(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(train_preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes for models\"\"\"\n",
    "\n",
    "# Import third party modules\n",
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as sm\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import torch\n",
    "\n",
    "# Import custom objects\n",
    "from code.params.globals import BATCH_SIZE\n",
    "from code.params.model_info import *\n",
    "# from code.models.utils import pick_best_params\n",
    "\n",
    "\n",
    "# Define an abstract class for holding all models\n",
    "class AbstractModelWrapper(ABC):\n",
    "    \"\"\"\n",
    "    This is an abstract class for wrapping all models used for making\n",
    "    mutant fitness predictions. It will be inherited by a model class\n",
    "    for making both supervised and unsupervised predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization loads the parent sequence\n",
    "    def __init__(self, *constructor_args, **constructor_kwargs):\n",
    "        \"\"\"\n",
    "        Loads the reference sequence that will be used to make predictions and\n",
    "        stores parameters for the model that will be trained.\n",
    "        \n",
    "        Args:\n",
    "        - constructor_args: Positional arguments that will be passed to the model\n",
    "            constructor for this wrapper.\n",
    "        - constructor_kwargs: Keyword arguments that will be passed to the model\n",
    "            constructor for this wrapper.\n",
    "        \"\"\"\n",
    "        # Initialize the sequence loader\n",
    "        super().__init__(fasta_loc)\n",
    "\n",
    "        # print(\"AbstractModelWrapper constructor_kwargs inputs before assigning\")\n",
    "        # print(constructor_kwargs)\n",
    "\n",
    "        # Store the constructor args and kwargs as instance variables\n",
    "        self._constructor_args = constructor_args\n",
    "        self._constructor_kwargs = deepcopy(constructor_kwargs)\n",
    "\n",
    "        # print(\"AbstractModelWrapper constructor_kwargs inputs\")\n",
    "        # print(constructor_kwargs)\n",
    "        # print(\"AbstractModelWrapper self._constructor_kwargs\")\n",
    "        # print(self._constructor_kwargs)\n",
    "\n",
    "        # The default model is \"None\". We need to initialize a model later.\n",
    "        self.model = None\n",
    "\n",
    "    # All classes will be able to make a prediction\n",
    "    def predict(self, mutants, muts_preencoded=False, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Makes predictions given an input of mutants or encoded mutants. If\n",
    "        unencoded mutants are passed in, then they will first be encoded.\n",
    "        Otherwise, encodings passed in will be assumed correct and fed into the\n",
    "        appropriate model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mutants (tuple of tuple of tuples OR ndarray/torch tensor):\n",
    "            If a tuple of tuple of tuples: All mutations to make a prediction\n",
    "            for. Will use the reference sequence loaded during initialization in\n",
    "            combination with the mutants passed in here. Each entry in the\n",
    "            outer tuple is a tuple containing mutation information for one\n",
    "            variant. Each mutation is specified by its own tuple with the format\n",
    "            (Reference AA, Mutation Position, Mutant AA). Mutation positions are\n",
    "            assumed to be 1-indexed relative to the sequence. See an example\n",
    "            below.\n",
    "            If ndarray or torch tensor: This is mutants that have already been\n",
    "            encoded.\n",
    "        Returns\n",
    "        -------\n",
    "        predictions (ndarray): Numpy array containing the predictions for the\n",
    "            input set of mutants.\n",
    "        Examples\n",
    "        --------\n",
    "        An example input for `mutants` when tuple of tuple of tuples:\n",
    "            mutants = (\n",
    "                ((V, 39, D), (D, 40, G)),\n",
    "                ((V, 39, D), (D, 40, G), (G, 41, F))\n",
    "                )\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"\"\"This is the predict inside the AbstractModelWrapper \\\n",
    "        # calling _complete_prediction\"\"\")\n",
    "\n",
    "        # Confirm that we have a model and scaler for making predictions\n",
    "        assert self.model is not None, \"Must train a model before making predictions\"\n",
    "\n",
    "        # Check mutations for accuracy based on whether they are pre-encoded\n",
    "        # or not\n",
    "        self._check_mutants(mutants, muts_preencoded)\n",
    "\n",
    "        # Every child class will need to finish this function\n",
    "        unscaled_preds = self._complete_prediction(\n",
    "            mutants, muts_preencoded=muts_preencoded, **pred_kwargs\n",
    "        )\n",
    "\n",
    "        # print(\"mutants and unscaled_preds dims:\")\n",
    "        # print(mutants.shape, unscaled_preds.shape)\n",
    "\n",
    "        # Confirm that predictions are a numpy array and that they are 2d\n",
    "        assert isinstance(\n",
    "            unscaled_preds, np.ndarray\n",
    "        ), \"Predictions should be numpy array\"\n",
    "        assert len(unscaled_preds.shape) == 2, \"Expect 2D prediction array\"\n",
    "\n",
    "        # Return the unscaled preds. Supervised models will scale them based on\n",
    "        # the standard scaler used at input\n",
    "        return unscaled_preds\n",
    "\n",
    "    def evaluate(self, mutants, true_fitness, muts_preencoded=False, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Wraps making predictions and evaluation of predictions using a number\n",
    "        of evaluation metrics.\n",
    "        mutants (tuple of tuple of tuples): All mutations to make a prediction\n",
    "            for. Will use the reference sequence loaded during initialization in\n",
    "            combination with the mutants passed in here. Each entry in the\n",
    "            outer tuple is a tuple containing mutation information for one\n",
    "            variant. Each mutation is specified by its own tuple with the format\n",
    "            (Reference AA, Mutation Position, Mutant AA). Mutation positions are\n",
    "            assumed to be 1-indexed relative to the sequence. See an example\n",
    "            below.\n",
    "        true_fitness (ndarray): The true fitness values of the mutants.\n",
    "        muts_preencoded (bool): Whether or not the input mutants are pre-encoded.\n",
    "            If not pre-encoded, this function expects a tuple of tuples of tuples.\n",
    "            If it is pre-encoded, then the mutants should be formatted already\n",
    "            to go into whatever model will be making predictions.\n",
    "        Examples\n",
    "        --------\n",
    "        An example input for `mutants`:\n",
    "            mutants = (\n",
    "                ((V, 39, D), (D, 40, G)),\n",
    "                ((V, 39, D), (D, 40, G), (G, 41, F))\n",
    "                )\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(\"This is the evulate inside the AbstractModelWrapper\")\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.predict(\n",
    "            mutants, muts_preencoded=muts_preencoded, **pred_kwargs\n",
    "        )\n",
    "\n",
    "        # Confirm that the predictions and true values are the same shape and\n",
    "        # that they are two dimensional\n",
    "        \n",
    "        # print(\"The kwargs for the evaluate function in AbstractModelWrapper\")\n",
    "        # print(pred_kwargs)\n",
    "        # print(\"After the predict in the evaluate function in AbstractModelWrapper\")\n",
    "        # print(mutants.shape, predictions.shape, true_fitness.shape)\n",
    "        \n",
    "        \"\"\"assert isinstance(true_fitness, np.ndarray), \"True values should be numpy array\"\n",
    "        assert (\n",
    "            predictions.shape == true_fitness.shape\n",
    "        ), \"Mismatch in truth and prediction shapes\"\n",
    "        n_fitness_vals = predictions.shape[1]\n",
    "        # Evaluate the predictions using spearman rho\n",
    "        rho_vals = np.array(\n",
    "            [\n",
    "                ss.spearmanr(true_fitness[:, i], predictions[:, i])[0]\n",
    "                for i in range(n_fitness_vals)\n",
    "            ]\n",
    "        )\n",
    "        # Evaluate predictions using mse. Keep errors separate by label.\n",
    "        mses = sm.mean_squared_error(\n",
    "            true_fitness, predictions, multioutput=\"raw_values\"\n",
    "        )\n",
    "        return [predictions, [rho_vals, mses]]\"\"\"\n",
    "        \n",
    "        return self.evaluate_metrics(predictions, true_fitness, muts_preencoded, **pred_kwargs)\n",
    "    \n",
    "    def evaluate_metrics(self, predictions, true_fitness, muts_preencoded=False, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Wraps making evaluation metrics calculation given predictions.\n",
    "        predictions (ndarray): \n",
    "        true_fitness (ndarray): The true fitness values of the mutants.\n",
    "        muts_preencoded (bool): Whether or not the input mutants are pre-encoded.\n",
    "            If not pre-encoded, this function expects a tuple of tuples of tuples.\n",
    "            If it is pre-encoded, then the mutants should be formatted already\n",
    "            to go into whatever model will be making predictions.\n",
    "        \"\"\"\n",
    "        # print(\"predictions.shape, true_fitness.shape\")\n",
    "        # print(predictions.shape, true_fitness.shape)\n",
    "        assert isinstance(true_fitness, np.ndarray), \"True values should be numpy array\"\n",
    "        assert (\n",
    "            predictions.shape == true_fitness.shape\n",
    "        ), \"Mismatch in truth and prediction shapes\"\n",
    "        n_fitness_vals = predictions.shape[1]\n",
    "\n",
    "        # Evaluate the predictions using spearman rho\n",
    "        rho_vals = np.array(\n",
    "            [\n",
    "                ss.spearmanr(true_fitness[:, i], predictions[:, i])[0]\n",
    "                for i in range(n_fitness_vals)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Evaluate predictions using mse. Keep errors separate by label.\n",
    "        mses = sm.mean_squared_error(\n",
    "            true_fitness, predictions, multioutput=\"raw_values\"\n",
    "        )\n",
    "\n",
    "        return [predictions, [rho_vals, mses]]\n",
    "\n",
    "    @abstractmethod\n",
    "    def _check_mutants(self, mutants, muts_preencoded):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _complete_prediction(self, mutants, muts_preencoded, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Every child class will need to finish up the prediction function.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def constructor_args(self):\n",
    "        return self._constructor_args\n",
    "\n",
    "    @property\n",
    "    def constructor_kwargs(self):\n",
    "        return self._constructor_kwargs\n",
    "\n",
    "\n",
    "# Define an abstract class for unsupervised models\n",
    "class AbstractUnsupervisedModelWrapper(AbstractModelWrapper):\n",
    "    def format_mutants(self, unformatted_mutlists, *reformat_args):\n",
    "\n",
    "        # First confirm that the unformatted mutants are correct\n",
    "        self._check_unformatted_mutants(unformatted_mutlists)\n",
    "\n",
    "        # Complete formatting\n",
    "        reformatted_mutlists = self._complete_reformat(\n",
    "            unformatted_mutlists, *reformat_args\n",
    "        )\n",
    "\n",
    "        # Check the mutants at the end for accuracy\n",
    "        self._check_formatted_mutants(reformatted_mutlists)\n",
    "\n",
    "        return reformatted_mutlists\n",
    "\n",
    "    def _check_mutants(self, mutants, muts_preencoded):\n",
    "\n",
    "        # The function we use for checking depends on whether or not the mutants\n",
    "        # are pre-encoded\n",
    "        if muts_preencoded:\n",
    "            self._check_formatted_mutants(mutants)\n",
    "        else:\n",
    "            self._check_unformatted_mutants(mutants)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _check_formatted_mutants(self, mutants):\n",
    "        \"\"\"\n",
    "        Confirms that the mutants are input to the class in a format expected by\n",
    "        the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # All classes will need a function for formatting mutants. This can mean\n",
    "    # tokenizing them for the NLP models or just rearranging the input mutants\n",
    "    # for EVcouplings or triad.\n",
    "    @abstractmethod\n",
    "    def _complete_reformat(self, mutants):\n",
    "        \"\"\"\n",
    "        When mutations are not pre-encoded, this function handles their processing\n",
    "        so that they can be converted from the standard input format to a format\n",
    "        useful for the model. This must be overwritten by inheriting classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# Define an abstract class for supervised models\n",
    "class AbstractSupervisedModelWrapper(AbstractModelWrapper):\n",
    "    \"\"\"\n",
    "    This is an abstract class for wrapping all supervised models used for making\n",
    "    single-to-multi mutant predictions. This class will be inherited by classes\n",
    "    specific to the different model architectures/types we will be training.\n",
    "    \"\"\"\n",
    "\n",
    "    # We add a fitness scaler to the initialized attributes\n",
    "    def __init__(\n",
    "        self,\n",
    "        fasta_loc,\n",
    "        model_class,\n",
    "        encoder=None,\n",
    "        *constructor_args,\n",
    "        **constructor_kwargs\n",
    "    ):\n",
    "\n",
    "        # Initialize using parent method\n",
    "        super().__init__(fasta_loc, *constructor_args, **constructor_kwargs)\n",
    "\n",
    "        # Record the model class\n",
    "        self.model_class = model_class\n",
    "\n",
    "        # print(\n",
    "        #     \"\"\"initilizing models in AbstractSupervisedModelWrapper \\\n",
    "        #     with constructor_kwargs and self.constructor_kwargs\"\"\"\n",
    "        # )\n",
    "        # print(constructor_kwargs)\n",
    "        # print(self.constructor_kwargs)\n",
    "\n",
    "        # Initialize the model\n",
    "        self._initialize_model()\n",
    "\n",
    "        # Specify the encoder encoding type\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # The default fitness scaler is \"None\". We initialize one during training.\n",
    "        self.fitness_scaler = None\n",
    "\n",
    "    # All classes will have a train method. This method might not do anything\n",
    "    # for some of them though (like triad)\n",
    "    def train(\n",
    "        self,\n",
    "        train_mutants,\n",
    "        train_fitnesses,\n",
    "        test_mutants=None,\n",
    "        test_fitnesses=None,\n",
    "        muts_preencoded=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        _skip_check=False,\n",
    "        **pred_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains a model given input mutants and fitnesses.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mutants (tuple of tuple of tuples OR ndarray/torch tensor):\n",
    "            If a tuple of tuple of tuples: All mutations to make a prediction\n",
    "            for. Will use the reference sequence loaded during initialization in\n",
    "            combination with the mutants passed in here. Each entry in the\n",
    "            outer tuple is a tuple containing mutation information for one\n",
    "            variant. Each mutation is specified by its own tuple with the format\n",
    "            (Reference AA, Mutation Position, Mutant AA). Mutation positions are\n",
    "            assumed to be 1-indexed relative to the sequence. See an example\n",
    "            below.\n",
    "            If ndarray or torch tensor: This is mutants that have already been\n",
    "            encoded.\n",
    "        fitnesses (2D array with same type as `mutants` after they have\n",
    "        been encoded):\n",
    "            Provides the labels against which training will be performed\n",
    "        muts_preencoded (bool): Whether or not the input mutants are pre-encoded.\n",
    "            If not pre-encoded, this function expects a tuple of tuples of tuples.\n",
    "            If it is pre-encoded, then the mutants should be formatted already\n",
    "            to go into whatever model will be making predictions.\n",
    "        _skip_check (bool): Private method. Will skip checking the input data.\n",
    "            Used in conjunction with self.train_cv (self.train_cv handles\n",
    "            checking, so there's no need to do it on every iteration)\n",
    "        \"\"\"\n",
    "        # Confirm training inputs are acceptable\n",
    "        if not _skip_check:\n",
    "            self._check_training_data(\n",
    "                train_mutants, train_fitnesses, None, muts_preencoded\n",
    "            )\n",
    "\n",
    "        # print(\"\"\"initilizing models in the train \\ \n",
    "        #     inside the AbstractSupervisedModelWrapper \\\n",
    "        #     with self.constructor_args and self.constructor_kwargs\"\"\")\n",
    "        \n",
    "        # print(self.constructor_args)\n",
    "        # print(self.constructor_kwargs)\n",
    "        \n",
    "        # Initialize a new model.\n",
    "        self._initialize_model()\n",
    "\n",
    "        # Build a scaler and scale fitnesses\n",
    "        self.fitness_scaler = StandardScaler()\n",
    "        scaled_train_fitnesses = self.fitness_scaler.fit_transform(train_fitnesses)\n",
    "\n",
    "        # Check testing data if provided. Scale it too.\n",
    "        if test_mutants is not None:\n",
    "            assert test_fitnesses is not None, \"Did not provide test_fitnesses\"\n",
    "            self._check_training_data(\n",
    "                test_mutants, test_fitnesses, None, muts_preencoded\n",
    "            )\n",
    "            scaled_test_fitnesses = self.fitness_scaler.transform(\n",
    "                test_fitnesses, copy=True\n",
    "            )\n",
    "        else:\n",
    "            scaled_test_fitnesses = None\n",
    "\n",
    "        # print(\"Now complete training with pred_kwargs\")\n",
    "        # print(pred_kwargs)\n",
    "\n",
    "        return self._complete_training(\n",
    "            train_mutants,\n",
    "            scaled_train_fitnesses,\n",
    "            test_mutants,\n",
    "            scaled_test_fitnesses,\n",
    "            muts_preencoded,\n",
    "            batch_size,\n",
    "            **pred_kwargs\n",
    "        )\n",
    "\n",
    "    # All classes will have a cv-train method.\n",
    "    def train_cv(\n",
    "        self,\n",
    "        mutants,\n",
    "        fitnesses,\n",
    "        muts_preencoded=False,\n",
    "        positions=None,\n",
    "        n_cv=N_CV,\n",
    "        shuffle=True,\n",
    "        random_state=2,\n",
    "        split_type=\"random\",\n",
    "        **pred_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs kfold cross validation to train a model.\n",
    "        \"\"\"\n",
    "        # Check the training data\n",
    "        self._check_training_data(mutants, fitnesses, positions, muts_preencoded)\n",
    "\n",
    "        # Generate splits from the cross-validator\n",
    "        splits = self._build_splits(\n",
    "            fitnesses, positions, n_cv, shuffle, random_state, split_type\n",
    "        )\n",
    "\n",
    "        # Loop over all splits\n",
    "        all_test_errs = np.empty(\n",
    "            [n_cv, N_EVAL_METRICS, fitnesses.shape[1]]\n",
    "        )  # (n_cv, N metrics, n fitness values)\n",
    "\n",
    "        stop_epoch_sum = 0\n",
    "\n",
    "        for i, (train_inds, test_inds) in enumerate(splits):\n",
    "            print(\"Running fold {0}\".format(i+1))\n",
    "            # Make the split. How the split is performed depends on the dtype\n",
    "            # of the mutants\n",
    "            train_muts, test_muts = self._make_split(train_inds, test_inds, mutants)\n",
    "            train_fitness, test_fitness = self._make_split(\n",
    "                train_inds, test_inds, fitnesses\n",
    "            )\n",
    "\n",
    "            # Train\n",
    "            # print(\"\"\"This is the train inside the AbstractSupervisedModelWrapper \\\n",
    "            #     train_cv with pred_kwargs\"\"\")\n",
    "            # print(pred_kwargs)\n",
    "\n",
    "            train_op = self.train(\n",
    "                train_muts,\n",
    "                train_fitness,\n",
    "                test_mutants=test_muts,\n",
    "                test_fitnesses=test_fitness,\n",
    "                muts_preencoded=muts_preencoded,\n",
    "                _skip_check=True,\n",
    "                **pred_kwargs\n",
    "            )\n",
    "\n",
    "            # get the number of early stopping epoch\n",
    "            if train_op is not None:\n",
    "                stop_epoch_sum += train_op\n",
    "\n",
    "            # Evaluate\n",
    "            # print(\"\"\"This is the evaluate inside the AbstractSupervisedModelWrapper \\\n",
    "            # train_cv with pred_kwargs\"\"\")\n",
    "            # print(pred_kwargs)\n",
    "\n",
    "            all_test_errs[i] = self.evaluate(\n",
    "                test_muts, test_fitness, muts_preencoded=muts_preencoded, **pred_kwargs\n",
    "            )[1]\n",
    "\n",
    "        # Return the average test error over the different splits\n",
    "        # print(\"avg epoch after train_cv {0}\".format(np.int(stop_epoch_sum/n_cv)+1))\n",
    "        \n",
    "        \"\"\"print(\"all_test_errs\")\n",
    "        print(all_test_errs)\n",
    "        \n",
    "        print(\"all_test_errs taking the mean\")\n",
    "        print(all_test_errs.mean(axis=0))\"\"\"\n",
    "        \n",
    "        return all_test_errs.mean(axis=0), np.int(stop_epoch_sum / n_cv) + 1\n",
    "\n",
    "    # We extend the prediction method to rescale any predictions made\n",
    "    def predict(\n",
    "        self, mutants, muts_preencoded=False, batch_size=BATCH_SIZE, **pred_kwargs\n",
    "    ):\n",
    "\n",
    "        # print(\n",
    "        #     \"This is the predict inside the AbstractSupervisedModelWrapper with pred_kwargs\"\n",
    "        # )\n",
    "        # print(pred_kwargs)\n",
    "\n",
    "        # Confirm that we have a scaler\n",
    "        assert (\n",
    "            self.fitness_scaler is not None\n",
    "        ), \"Must train model before making predictions\"\n",
    "\n",
    "        # See if we need an encoder\n",
    "        self._check_for_encoder(muts_preencoded)\n",
    "\n",
    "        # Get the unscaled predictions\n",
    "        unscaled_predictions = super().predict(\n",
    "            mutants,\n",
    "            muts_preencoded=muts_preencoded,\n",
    "            batch_size=batch_size,\n",
    "            **pred_kwargs\n",
    "        )\n",
    "\n",
    "        # print(\"mutants and unscaled_predictions dim:\")\n",
    "        # print(mutants.shape, unscaled_predictions.shape)\n",
    "\n",
    "        # Return the rescaled predictions\n",
    "        return self.fitness_scaler.inverse_transform(unscaled_predictions)\n",
    "\n",
    "    def hyperopt_gridsearch(\n",
    "        self,\n",
    "        mutants,\n",
    "        fitnesses,\n",
    "        constructor_args_list,\n",
    "        constructor_kwargs_list,\n",
    "        **cv_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs a grid search to identify the optimal hyperparameters for the\n",
    "        model.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\"\"Inside hyperopt_gridsearch in AbstractSupervisedModelWrapper \\\n",
    "        with constructor_kwargs_list and cv_kwargs\"\"\")\n",
    "        print(constructor_kwargs_list, cv_kwargs)\n",
    "\n",
    "        # Confirm that there are as many constructor args as there are kwargs\n",
    "        n_tests = len(constructor_args_list)\n",
    "        assert n_tests == len(constructor_kwargs_list)\n",
    "\n",
    "        best_cv_rho = 0\n",
    "        best_cv_mse = np.inf\n",
    "        opt_params = np.nan\n",
    "        train_params = {}\n",
    "\n",
    "        # Loop over all combinations of constructor args and kwargs and test\n",
    "        n_fitness_vals = fitnesses.shape[1]\n",
    "        hyperopt_test_errs = np.empty([n_tests, N_EVAL_METRICS, n_fitness_vals])\n",
    "        for i, (constructor_args, constructor_kwargs) in enumerate(\n",
    "            zip(constructor_args_list, constructor_kwargs_list)\n",
    "        ):\n",
    "\n",
    "            # Reassign constructor args and kwargs\n",
    "            self._constructor_args = constructor_args\n",
    "            self._constructor_kwargs = deepcopy(constructor_kwargs)\n",
    "            \n",
    "            # print(\"\"\"Inside hyperopt_gridsearch in AbstractSupervisedModelWrapper \\\n",
    "            # before train_cv constructor_kwargs and self._constructor_kwargs\"\"\")\n",
    "            # print(constructor_kwargs, self._constructor_kwargs)\n",
    "\n",
    "            # Run cross-validation and record error\n",
    "            hyperopt_test_errs[i], stop_epoch_avg = self.train_cv(\n",
    "                mutants, fitnesses, **cv_kwargs\n",
    "            )\n",
    "            # print(\"stop_epoch_avg in hyper tune is {0}\".format(stop_epoch_avg))\n",
    "\n",
    "            current_rho = hyperopt_test_errs[i][0][0]\n",
    "            current_mse = hyperopt_test_errs[i][1][0]\n",
    "            # print(\"current rho: {0}, mse: {1}\".format(current_rho, current_mse))\n",
    "\n",
    "            \"\"\"best_cv_mse, best_cv_rho, opt_params, _ = pick_best_params(current_mse, best_cv_mse, \n",
    "                                                                    current_rho, best_cv_rho, \n",
    "                                                                    self._constructor_kwargs)\"\"\"\n",
    "            \"\"\"# Update the best errors and associated kwargs\n",
    "            if current_mse != np.nan and current_mse < best_cv_mse:\n",
    "                best_cv_mse = current_mse\n",
    "                best_cv_rho = current_rho\n",
    "                opt_params = self._constructor_kwargs\n",
    "            elif (\n",
    "                current_mse != np.nan\n",
    "                and current_rho != np.nan\n",
    "                and current_mse == best_cv_mse\n",
    "                and current_rho > best_cv_rho\n",
    "            ):\n",
    "                best_cv_rho = current_rho\n",
    "                opt_params = self._constructor_kwargs\"\"\"\n",
    "\n",
    "            if (current_mse != np.nan and current_mse < best_cv_mse) or (\n",
    "                current_mse != np.nan\n",
    "                and current_rho != np.nan\n",
    "                and current_mse == best_cv_mse\n",
    "                and current_rho > best_cv_rho\n",
    "            ):\n",
    "                best_cv_mse = current_mse\n",
    "                best_cv_rho = current_rho\n",
    "                opt_params = self._constructor_kwargs\n",
    "\n",
    "            if stop_epoch_avg != 1:\n",
    "                train_params = {\"epochs\": stop_epoch_avg}\n",
    "\n",
    "        print(\n",
    "            \"Chosen best opt_params, train_params: {0} {1}\".format(\n",
    "                opt_params, train_params\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return hyperopt_test_errs, opt_params, train_params, best_cv_mse, best_cv_rho\n",
    "\n",
    "    # Confirm that we have an encoder if mutations are not preencoded\n",
    "    def _check_for_encoder(self, muts_preencoded):\n",
    "        if not muts_preencoded:\n",
    "            assert (\n",
    "                self.encoder is not None\n",
    "            ), \"If muts are not pre-encoded, an encoder is needed\"\n",
    "\n",
    "    def _check_training_data(self, mutants, fitnesses, positions, muts_preencoded):\n",
    "        \"\"\"\n",
    "        Confirms assumptions that we make about the training data as well as\n",
    "        encodes mutants if a tuple of tuple of tuples was passed in.\n",
    "        \"\"\"\n",
    "        # See if we need an encoder\n",
    "        self._check_for_encoder(muts_preencoded)\n",
    "\n",
    "        # Confirm that the input mutants will work. Determine whether the mutants\n",
    "        # are encoded or not.\n",
    "        self._check_mutants(mutants, muts_preencoded)\n",
    "\n",
    "        # Confirm that (1) `fitnesses` is 2D and (2) that the lengths of the\n",
    "        # mutant and fitness arrays match\n",
    "        assert len(fitnesses.shape) == 2, \"Expect 2D array for fitness\"\n",
    "        assert len(mutants) == len(fitnesses), \"Mismatch in training data lengths\"\n",
    "\n",
    "        # If the mutants are pre-encoded, then we expect the fitnesses and\n",
    "        # mutants to have the same datatype\n",
    "        if muts_preencoded:\n",
    "            assert type(fitnesses) is type(\n",
    "                mutants\n",
    "            ), \"Expect x and y to have the same type\"\n",
    "\n",
    "        # If positions are provided, then they should be 1d and and have the\n",
    "        # same length as mutants and fitnesses\n",
    "        if positions is not None:\n",
    "            assert len(positions.shape) == 1, \"Expect 1D array for positions\"\n",
    "            assert len(positions) == len(\n",
    "                fitnesses\n",
    "            ), \"Mismatch in number of positions provided\"\n",
    "\n",
    "    def _build_splits(\n",
    "        self, fitnesses, positions, n_cv, shuffle, random_state, split_type\n",
    "    ):\n",
    "\n",
    "        # Confirm that fitnesses are a numpy array\n",
    "        assert isinstance(fitnesses, np.ndarray), \"Fitnesses expected to be numpy array\"\n",
    "\n",
    "        # Group split kwargs\n",
    "        splargs = {\"n_splits\": n_cv, \"shuffle\": shuffle, \"random_state\": random_state}\n",
    "\n",
    "        # Initialize the appropriate object. If the split type is \"random\", then\n",
    "        # we slit randomly by position\n",
    "        if split_type == \"random\":\n",
    "\n",
    "            # Build the object\n",
    "            kfold_obj = KFold(**splargs)\n",
    "\n",
    "            # Generate splits\n",
    "            splits = kfold_obj.split(fitnesses)\n",
    "\n",
    "        # If the split type is \"even_pos\", then we split such that each position\n",
    "        # is roughly equivalent represented in each split\n",
    "        elif split_type == \"even_pos\":\n",
    "\n",
    "            # Confirm that classes are provided, the positions are a numpy array,\n",
    "            # the positions have the same length as fitnesses, and that the positions\n",
    "            # are a 1d numpy array\n",
    "            assert positions is not None, \"Must provide position information\"\n",
    "            assert isinstance(positions, np.ndarray), \"Positions should be numpy array\"\n",
    "            assert len(positions.shape) == 1, \"Positions should be 1d array\"\n",
    "            assert len(positions) == len(\n",
    "                fitnesses\n",
    "            ), \"Mismatch between fitnesses and positions\"\n",
    "\n",
    "            # Build the object\n",
    "            kfold_obj = StratifiedKFold(**splargs)\n",
    "\n",
    "            # Generate splits\n",
    "            splits = kfold_obj.split(X=fitnesses, y=positions)\n",
    "\n",
    "        # Anything else and we have an unrecognized splitter\n",
    "        else:\n",
    "            raise AssertionError(\"Unrecognized kfold type requested\")\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def _make_split(self, train_inds, test_inds, to_split):\n",
    "\n",
    "        # Confirm that the inputs inds are numpy arrays\n",
    "        assert isinstance(train_inds, np.ndarray)\n",
    "        assert isinstance(test_inds, np.ndarray)\n",
    "\n",
    "        # If the object to be split is a numpy array, just fancy index\n",
    "        if isinstance(to_split, np.ndarray):\n",
    "            return to_split[train_inds], to_split[test_inds]\n",
    "\n",
    "        # If the object to be split is a torch tensor, convert training and\n",
    "        # testing inds to tensors first, then split\n",
    "        elif isinstance(to_split, torch.Tensor):\n",
    "            return (\n",
    "                to_split[torch.from_numpy(train_inds)],\n",
    "                to_split[torch.from_numpy(test_inds)],\n",
    "            )\n",
    "\n",
    "        # If the object to be split is a tuple, then we use list-comp\n",
    "        elif isinstance(to_split, tuple):\n",
    "            return (\n",
    "                tuple(to_split[ind] for ind in train_inds),\n",
    "                tuple(to_split[ind] for ind in test_inds),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown datatype input to cross-validation\")\n",
    "\n",
    "    def _check_mutants(self, mutants, muts_preencoded):\n",
    "        \"\"\"\n",
    "        We only check if muts and not preencoded. There is no good way to check\n",
    "        encoded mutations.\n",
    "        \"\"\"\n",
    "        # The function we use for checking depends on whether or not the mutants\n",
    "        # are pre-encoded\n",
    "        if not muts_preencoded:\n",
    "            self._check_unformatted_mutants(mutants)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"\n",
    "        Builds a new instance of self.model using self.constructor_args and\n",
    "        self.constructor_kwargs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _complete_training(\n",
    "        self,\n",
    "        train_mutants,\n",
    "        scaled_train_fitnesses,\n",
    "        test_mutants,\n",
    "        scaled_test_fitnesses,\n",
    "        muts_preencoded,\n",
    "        batch_size,\n",
    "        **pred_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Finishes the `train` method for each child class.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_model_class_name(self):\n",
    "        \"\"\"\n",
    "        Obtain the model class name of the class as a string\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
