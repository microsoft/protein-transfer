{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [01:52<00:00,  1.73s/it]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_0 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_1 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_2 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_3 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_4 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_5 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_6 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n"
     ]
    }
   ],
   "source": [
    "all_ridge_results = run_ridge(\n",
    "    dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\",\n",
    "    encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "    embed_batch_size=128,\n",
    "    flatten_emb=\"mean\",\n",
    "    embed_path=None,\n",
    "    seq_start_idx=0,\n",
    "    seq_end_idx=56,\n",
    "    loader_batch_size=64,\n",
    "    worker_seed=RAND_SEED,\n",
    "    alphas=SKLEARN_ALPHAS,\n",
    "    ridge_state=RAND_SEED,\n",
    "    ridge_params=None,\n",
    "    all_result_folder=\"results/sklearn\",\n",
    "    # **encoder_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for run sklearn (currently ridge) models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scr.utils import get_folder_file_names, pickle_save\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "from scr.params.sys import RAND_SEED, SKLEARN_ALPHAS\n",
    "from scr.encoding.encoding_classes import ESMEncoder, CARPEncoder\n",
    "from scr.preprocess.data_process import split_protrain_loader\n",
    "\n",
    "# seed\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "\n",
    "\n",
    "class RunRidge:\n",
    "    \"\"\"A class for running ridge regression\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_path: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        loader_batch_size: int = 64,\n",
    "        worker_seed: int = RAND_SEED,\n",
    "        alphas: np.ndarray = SKLEARN_ALPHAS,\n",
    "        ridge_state: int = RAND_SEED,\n",
    "        ridge_params: dict | None = None,\n",
    "        all_result_folder: str = \"results/sklearn\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_path: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "        - worker_seed: int, the seed for dataloader\n",
    "        - alphas: np.ndarray, arrays of alphas to be tested\n",
    "        - ridge_state: int = RAND_SEED, seed the ridge regression\n",
    "        - ridge_params: dict | None = None, other ridge regression args\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.encoder_name = encoder_name\n",
    "        self.flatten_emb = flatten_emb\n",
    "        self.alphas = alphas\n",
    "        self.ridge_state = ridge_state\n",
    "        self.ridge_params = ridge_params\n",
    "        self.all_result_folder = all_result_folder\n",
    "\n",
    "        # loader has ALL embedding layers\n",
    "        self.train_loader, self.val_loader, self.test_loader = split_protrain_loader(\n",
    "            dataset_path=self.dataset_path,\n",
    "            encoder_name=self.encoder_name,\n",
    "            embed_batch_size=embed_batch_size,\n",
    "            flatten_emb=self.flatten_emb,\n",
    "            embed_path=embed_path,\n",
    "            seq_start_idx=seq_start_idx,\n",
    "            seq_end_idx=seq_end_idx,\n",
    "            subset_list=[\"train\", \"val\", \"test\"],\n",
    "            loader_batch_size=loader_batch_size,\n",
    "            worker_seed=worker_seed,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        all_ridge_results = {}\n",
    "\n",
    "        if self.encoder_name in TRANSFORMER_INFO.keys():\n",
    "            total_emb_layer = ESMEncoder(encoder_name=self.encoder_name).total_emb_layer\n",
    "        elif self.encoder_name in CARP_INFO.keys():\n",
    "            total_emb_layer = CARPEncoder(\n",
    "                encoder_name=self.encoder_name\n",
    "            ).total_emb_layer\n",
    "\n",
    "        for layer in range(total_emb_layer):\n",
    "            all_ridge_results[layer] = self.run_ridge_layer(\n",
    "                embed_layer=layer,\n",
    "            )\n",
    "\n",
    "        self._all_ridge_results = all_ridge_results\n",
    "\n",
    "    def sk_test(\n",
    "        self, model: sklearn.linear_model, loader: DataLoader, embed_layer: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for testing sklearn models for a specific layer of embeddings\n",
    "\n",
    "        Args:\n",
    "        - model: sklearn.linear_model, trained model\n",
    "        - loader: DataLoader, train, val, or test data loader\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - np.concatenate(pred): np.ndarray, 1D predicted fitness values\n",
    "        - np.concatenate(true): np.ndarry, 1D true fitness values\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        true = []\n",
    "        for (y, sequence, mut_name, mut_numb, *layer_emb) in loader:\n",
    "            pred.append(model.predict(layer_emb[embed_layer]).squeeze())\n",
    "            true.append(y.cpu().squeeze().numpy())\n",
    "        return np.concatenate(pred), np.concatenate(true)\n",
    "\n",
    "    def pick_model(\n",
    "        self,\n",
    "        embed_layer: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for picking the best model for given alaphs, meaning\n",
    "        lower train_mse and higher test_ndcg\n",
    "        NOTE: alphas tuning is NOT currently optimal\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - sklearn.linear_model, the model with the best alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # init values for comparison\n",
    "        best_mse = np.Inf\n",
    "        best_ndcg = -1\n",
    "        best_rho = -1\n",
    "        best_model = None\n",
    "\n",
    "        # loop through all alphas\n",
    "        for alpha in self.alphas:\n",
    "\n",
    "            # init model for each alpha\n",
    "            if self.ridge_params is None:\n",
    "                self.ridge_params = {}\n",
    "            model = Ridge(\n",
    "                alpha=alpha, random_state=self.ridge_state, **self.ridge_params\n",
    "            )\n",
    "\n",
    "            # fit the model for a given layer of embedding\n",
    "            for (y, sequence, mut_name, mut_numb, *layer_emb) in self.train_loader:\n",
    "                model.fit(layer_emb[embed_layer], y)\n",
    "\n",
    "            # eval the model with train and test\n",
    "            train_pred, train_true = self.sk_test(\n",
    "                model, self.train_loader, embed_layer=embed_layer\n",
    "            )\n",
    "            val_pred, val_true = self.sk_test(\n",
    "                model, self.val_loader, embed_layer=embed_layer\n",
    "            )\n",
    "\n",
    "            # calc the metrics\n",
    "            train_mse = mean_squared_error(train_true, train_pred)\n",
    "            val_ndcg = ndcg_score(val_true[None, :], val_pred[None, :])\n",
    "            val_rho = spearmanr(val_true, val_pred)[0]\n",
    "\n",
    "            # update the model if it has lower train_mse and higher val_ndcg\n",
    "            if train_mse < best_mse and val_ndcg > best_ndcg:\n",
    "                best_model = model\n",
    "                best_mse = train_mse\n",
    "                best_ndcg = val_ndcg\n",
    "                best_rho = val_rho\n",
    "\n",
    "        return best_model\n",
    "\n",
    "    def run_ridge_layer(\n",
    "        self,\n",
    "        embed_layer: int,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running ridge regression for a given layer of embedding\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"train\": {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"\"\"\n",
    "\n",
    "        # train and get the best alpha\n",
    "        best_model = self.pick_model(\n",
    "            embed_layer=embed_layer,\n",
    "        )\n",
    "\n",
    "        # init dict for resulted outputs\n",
    "        result_dict = {}\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, loader in zip(\n",
    "            [\"train\", \"val\", \"test\"],\n",
    "            [self.train_loader, self.val_loader, self.test_loader],\n",
    "        ):\n",
    "            pred, true = self.sk_test(\n",
    "                best_model, self.val_loader, embed_layer=embed_layer\n",
    "            )\n",
    "\n",
    "            result_dict[subset] = {\n",
    "                \"mse\": mean_squared_error(true, pred),\n",
    "                \"pred\": pred,\n",
    "                \"true\": true,\n",
    "                \"ndcg\": ndcg_score(true[None, :], pred[None, :]),\n",
    "                \"rho\": spearmanr(true, pred),\n",
    "            }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=self.all_result_folder,\n",
    "            dataset_path=self.dataset_path,\n",
    "            encoder_name=self.encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self.flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    @property\n",
    "    def all_ridge_results(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"layer#\": {\n",
    "                        \"train\": {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"val\":   {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"test\":  {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        }\n",
    "        \"\"\"\n",
    "        return self._all_ridge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.52s/it]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [01:44<00:00,  1.60s/it]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_0 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_1 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_2 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_3 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_4 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_5 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_6 to: results/sklearn/proeng/gb1/two_vs_rest/esm1_t6_43M_UR50S/mean...\n"
     ]
    }
   ],
   "source": [
    "all_ridge_results = RunRidge(\n",
    "    dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\",\n",
    "    encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "    embed_batch_size=128,\n",
    "    flatten_emb=\"mean\",\n",
    "    embed_path=None,\n",
    "    seq_start_idx=0,\n",
    "    seq_end_idx=56,\n",
    "    loader_batch_size=64,\n",
    "    worker_seed=RAND_SEED,\n",
    "    alphas=SKLEARN_ALPHAS,\n",
    "    ridge_state=RAND_SEED,\n",
    "    ridge_params=None,\n",
    "    all_result_folder=\"results/sklearn\",\n",
    "     #**encoder_params,\n",
    ").all_ridge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
