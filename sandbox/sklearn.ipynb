{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.preprocess.data_process import split_protrain_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-processing the dataset\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Sequence, defaultdict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scr.utils import pickle_save, pickle_load, replace_ext\n",
    "from scr.params.sys import RAND_SEED\n",
    "from scr.params.emb import TRANSFORMER_INFO\n",
    "from scr.preprocess.seq_loader import SeqLoader\n",
    "from scr.encoding.encoding_classes import AbstractEncoder, ESMEncoder, CARPEncoder\n",
    "\n",
    "\n",
    "def get_mut_name(mut_seq: str, parent_seq: str) -> str:\n",
    "    \"\"\"\n",
    "    A function for returning the mutant name\n",
    "\n",
    "    Args:\n",
    "    - mut_seq: str, the full mutant sequence\n",
    "    - parent_seq: str, the full parent sequence\n",
    "\n",
    "    Returns:\n",
    "    - str, parent, indel, or mutant name in the format of\n",
    "        ParentAAMutLocMutAA:ParentAAMutLocMutAA:..., ie. W39W:D40G:G41C:V54Q\n",
    "    \"\"\"\n",
    "\n",
    "    mut_list = []\n",
    "    if parent_seq == mut_seq:\n",
    "        return \"parent\"\n",
    "    elif len(parent_seq) == len(mut_seq):\n",
    "        for i, (p, m) in enumerate(zip(list(parent_seq), list(mut_seq))):\n",
    "            if p != m:\n",
    "                mut_list.append(f\"{p}{i+1}{m}\")\n",
    "        return \":\".join(mut_list)\n",
    "    else:\n",
    "        return \"indel\"\n",
    "\n",
    "\n",
    "class AddMutInfo:\n",
    "    \"\"\"A class for appending mutation info for mainly protein engineering tasks\"\"\"\n",
    "\n",
    "    def __init__(self, parent_seq_path: str, csv_path: str):\n",
    "\n",
    "        # Load the parent sequence from the fasta file\n",
    "        self._parent_seq = SeqLoader(parent_seq_path=parent_seq_path)\n",
    "\n",
    "        # load the dataframe\n",
    "        self._init_df = pd.read_csv(csv_path)\n",
    "\n",
    "        self._df = self._init_df.copy()\n",
    "        # add a column with the mutant names\n",
    "        self._df[\"mut_name\"] = self._init_df[\"sequence\"].apply(\n",
    "            get_mut_name, parent_seq=self._parent_seq\n",
    "        )\n",
    "        # add a column with the number of mutations\n",
    "        self._df[\"mut_numb\"] = (\n",
    "            self._df[\"mut_name\"].str.split(\":\").map(len, na_action=\"ignore\")\n",
    "        )\n",
    "\n",
    "        # get the pickle file path\n",
    "        self._pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "\n",
    "        pickle_save(what2save=self._df, where2save=self._pkl_path)\n",
    "\n",
    "    @property\n",
    "    def parent_seq(self) -> str:\n",
    "        \"\"\"Return the parent sequence\"\"\"\n",
    "        return self._parent_seq\n",
    "\n",
    "    @property\n",
    "    def pkl_path(self) -> str:\n",
    "        \"\"\"Return the pkl file path for the processed dataframe\"\"\"\n",
    "        return self._pkl_path\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the processed dataframe\"\"\"\n",
    "        return self._df\n",
    "\n",
    "\n",
    "class TaskProcess:\n",
    "    \"\"\"A class for handling different downstream tasks\"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_folder: str, a folder path with all the tasks as subfolders where\n",
    "            all the subfolders have datasets as the subsubfolders, ie\n",
    "\n",
    "            {data_folder}/\n",
    "                proeng/\n",
    "                    aav/\n",
    "                        one_vs_many.csv\n",
    "                        two_vs_many.csv\n",
    "                        P03135.fasta\n",
    "                    thermo/\n",
    "                        mixed.csv\n",
    "        \"\"\"\n",
    "\n",
    "        if data_folder[-1] == \"/\":\n",
    "            self._data_folder = data_folder\n",
    "        else:\n",
    "            self._data_folder = data_folder + \"/\"\n",
    "\n",
    "        # sumamarize all files i nthe data folder\n",
    "        self._sum_file_df = self.sum_files()\n",
    "\n",
    "    def sum_files(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Summarize all files in the data folder\n",
    "\n",
    "        Returns:\n",
    "        - A dataframe with \"task\", \"dataset\", \"split\", \"csv_path\", \"fasta_path\", \"pkl_path\" as columns, ie.\n",
    "            (proeng, gb1, low_vs_high, data/proeng/gb1/low_vs_high.csv, data/proeng/gb1/5LDE_1.fasta)\n",
    "            note that csv_path is the list of lmdb files for the structure task\n",
    "        \"\"\"\n",
    "        dataset_folders = glob(f\"{self._data_folder}*/*\")\n",
    "        # need a list of tuples in the order of:\n",
    "        # (task, dataset, split, csv_path, fasta_path)\n",
    "        list_for_df = []\n",
    "        for dataset_folder in dataset_folders:\n",
    "            _, task, dataset = dataset_folder.split(\"/\")\n",
    "            if task == \"structure\":\n",
    "                structure_file_list = [\n",
    "                    file_path\n",
    "                    for file_path in glob(f\"{dataset_folder}/*.*\")\n",
    "                    if os.path.basename(os.path.splitext(file_path)[0]).split(\"_\")[-1]\n",
    "                    in [\"train\", \"valid\", \"cb513\"]\n",
    "                ]\n",
    "                list_for_df.append(\n",
    "                    tuple([task, dataset, \"cb513\", structure_file_list, \"\", \"\"])\n",
    "                )\n",
    "            else:\n",
    "                csv_paths = glob(f\"{dataset_folder}/*.csv\")\n",
    "                fasta_paths = glob(f\"{dataset_folder}/*.fasta\")\n",
    "                pkl_paths = glob(f\"{dataset_folder}/*.pkl\")\n",
    "\n",
    "                assert len(csv_paths) >= 1, \"Less than one csv\"\n",
    "                assert len(fasta_paths) <= 1, \"More than one fasta\"\n",
    "\n",
    "                for csv_path in csv_paths:\n",
    "                    # if parent seq fasta exists\n",
    "                    if len(fasta_paths) == 1:\n",
    "                        fasta_path = fasta_paths[0]\n",
    "\n",
    "                        # if no existing pkl file, generate and save\n",
    "                        if len(pkl_paths) == 0:\n",
    "                            print(f\"Adding mutation info to {csv_path}...\")\n",
    "                            pkl_path = AddMutInfo(\n",
    "                                parent_seq_path=fasta_path, csv_path=csv_path\n",
    "                            ).pkl_path\n",
    "                        # pkl file exits\n",
    "                        else:\n",
    "                            pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "                    # no parent fasta no pkl file\n",
    "                    else:\n",
    "                        fasta_path = \"\"\n",
    "                        pkl_path = \"\"\n",
    "\n",
    "                    list_for_df.append(\n",
    "                        tuple(\n",
    "                            [\n",
    "                                task,\n",
    "                                dataset,\n",
    "                                os.path.basename(os.path.splitext(csv_path)[0]),\n",
    "                                csv_path,\n",
    "                                fasta_path,\n",
    "                                pkl_path,\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            list_for_df,\n",
    "            columns=[\"task\", \"dataset\", \"split\", \"csv_path\", \"fasta_path\", \"pkl_path\"],\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def sum_file_df(self) -> pd.DataFrame:\n",
    "        \"\"\"A summary table for all files in the data folder\"\"\"\n",
    "        return self._sum_file_df\n",
    "\n",
    "\n",
    "class ProtranDataset(Dataset):\n",
    "\n",
    "    \"\"\"A dataset class for processing protein transfer data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        subset: str,\n",
    "        encoder_class: AbstractEncoder,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "        embed_batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_path: str = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        **encoder_params,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "        - subset: str, train, val, test\n",
    "        - encoder_class: AbstractEncoder, the encoder class\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_path: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        # with additional info mut_name, mut_numb\n",
    "        if os.path.splitext(dataset_path)[-1] in [\".pkl\", \".PKL\", \"\"]:\n",
    "            self._df = pickle_load(dataset_path)\n",
    "            self._add_mut_info = True\n",
    "        # without such info\n",
    "        else:\n",
    "            self._df = pd.read_csv(dataset_path)\n",
    "            self._add_mut_info = False\n",
    "\n",
    "        assert \"set\" in self._df.columns, f\"set is not a column in {dataset_path}\"\n",
    "        assert (\n",
    "            \"validation\" in self._df.columns\n",
    "        ), f\"validation is not a column in {dataset_path}\"\n",
    "\n",
    "        self._df_train = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] != True)\n",
    "        ]\n",
    "        self._df_val = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] == True)\n",
    "        ]\n",
    "        self._df_test = self._df.loc[(self._df[\"set\"] == \"test\")]\n",
    "\n",
    "        self._df_dict = {\n",
    "            \"train\": self._df_train,\n",
    "            \"val\": self._df_val,\n",
    "            \"test\": self._df_test,\n",
    "        }\n",
    "\n",
    "        assert subset in list(\n",
    "            self._df_dict.keys()\n",
    "        ), \"split can only be 'train', 'val', or 'test'\"\n",
    "        self._subset = subset\n",
    "\n",
    "        self._subdf_len = len(self._df_dict[self._subset])\n",
    "\n",
    "        # not specified seq start will be from 0\n",
    "        if seq_start_idx == False:\n",
    "            self._seq_start_idx = 0\n",
    "        else:\n",
    "            self._seq_start_idx = int(seq_start_idx)\n",
    "        # not specified seq end will be the full sequence length\n",
    "        if seq_end_idx == False:\n",
    "            self._seq_end_idx = -1\n",
    "        else:\n",
    "            self._seq_end_idx = int(seq_end_idx)\n",
    "\n",
    "        # get unencoded string of input sequence\n",
    "        # will need to convert data type\n",
    "        self.sequence = self._get_column_value(\"sequence\")\n",
    "\n",
    "        # get the encoder\n",
    "        self._encoder = encoder_class(encoder_name=encoder_name)\n",
    "\n",
    "        self._max_emb_layer = self._encoder.max_emb_layer\n",
    "        self._include_input_layer = self._encoder.include_input_layer\n",
    "\n",
    "        # check if pregenerated embedding\n",
    "        if embed_path is not None:\n",
    "            print(f\"Loading pregenerated embeddings from {embed_path}\")\n",
    "            encoded_dict = pickle_load(embed_path)\n",
    "\n",
    "        # encode the sequences without the mut_name\n",
    "        else:\n",
    "            \"\"\"encoded_dict = {\n",
    "                layer: [] for layer in range(self._max_emb_layer + self._include_input_layer)\n",
    "            }\"\"\"\n",
    "            \n",
    "            encoded_dict = defaultdict(list)\n",
    "\n",
    "            for encoded_batch_dict in self._encoder.encode(\n",
    "                mut_seqs=self.sequence,\n",
    "                batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                **encoder_params,\n",
    "            ):\n",
    "\n",
    "                for layer, emb in encoded_batch_dict.items():\n",
    "                    encoded_dict[layer].append(emb)\n",
    "\n",
    "        # assign each layer as its own variable\n",
    "        for layer, emb in encoded_dict.items():\n",
    "            setattr(self, \"layer\" + str(layer), torch.tensor(np.vstack(emb), dtype=torch.float32))\n",
    "            \n",
    "        # get and format the fitness or secondary structure values\n",
    "        # can be numbers or string\n",
    "        # will need to convert data type\n",
    "        # make 1D tensor 2D\n",
    "        self.y = np.expand_dims(self._get_column_value(\"target\"), 1)\n",
    "\n",
    "        # add mut_name and mut_numb for relevant proeng datasets\n",
    "        if self._add_mut_info:\n",
    "            self.mut_name = self._get_column_value(\"mut_name\")\n",
    "            self.mut_numb = self._get_column_value(\"mut_numb\")\n",
    "        else:\n",
    "            self.mut_name = [\"\"] * self._subdf_len\n",
    "            self.mut_numb = [np.nan] * self._subdf_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the selected subset of the dataframe\"\"\"\n",
    "        return self._subdf_len\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Return the item in the order of\n",
    "        encoded sequence (x), target (y), sequence, mut_name (optional), mut_numb (optional)\n",
    "\n",
    "        Args:\n",
    "        - idx: int\n",
    "        \"\"\"\n",
    "        \n",
    "        return (\n",
    "            # self.x[layer_numb][idx],\n",
    "            self.y[idx],\n",
    "            self.sequence[idx],\n",
    "            self.mut_name[idx],\n",
    "            self.mut_numb[idx],\n",
    "            *(\n",
    "                getattr(self,  \"layer\" + str(layer))[idx]\n",
    "                for layer in range(self._max_emb_layer + self._include_input_layer)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _get_column_value(self, column_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Check and return the column values of the selected dataframe subset\n",
    "        \"\"\"\n",
    "        if column_name in self._df.columns:\n",
    "            if column_name == \"sequence\":\n",
    "                return (\n",
    "                    self._df_dict[self._subset][\"sequence\"]\n",
    "                    .astype(str)\n",
    "                    .str[self._seq_start_idx : self._seq_end_idx]\n",
    "                    .values\n",
    "                )\n",
    "            else:\n",
    "                return self._df_dict[self._subset][column_name].values\n",
    "\n",
    "    @property\n",
    "    def df_full(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the full loaded dataset\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def df_train(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_train\n",
    "\n",
    "    @property\n",
    "    def df_val(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for validation only\"\"\"\n",
    "        return self._df_val\n",
    "\n",
    "    @property\n",
    "    def df_test(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_test\n",
    "\n",
    "\n",
    "def split_protrain_loader(\n",
    "    dataset_path: str,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    embed_batch_size: int = 128,\n",
    "    flatten_emb: bool | str = False,\n",
    "    embed_path: str | None = None,\n",
    "    seq_start_idx: bool | int = False,\n",
    "    seq_end_idx: bool | int = False,\n",
    "    subset_list: list[str] = [\"train\", \"val\", \"test\"],\n",
    "    loader_batch_size: int = 64,\n",
    "    worker_seed: int = RAND_SEED,\n",
    "    **encoder_params,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    A function encode and load the data from a path\n",
    "\n",
    "    Args:\n",
    "    - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "        columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "    - encoder_name: str, the name of the encoder\n",
    "    - embed_layer: int, the layer number of the embedding\n",
    "    - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "    - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "    - embed_path: str = None, path to presaved embedding\n",
    "    - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "    - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "    - subset_list: list of str, train, val, test\n",
    "    - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "    - worker_seed: int, the seed for dataloader\n",
    "    - encoder_params: kwarg, additional parameters for encoding\n",
    "    \"\"\"\n",
    "\n",
    "    assert set(subset_list) <= set(\n",
    "        [\"train\", \"val\", \"test\"]\n",
    "    ), \"subset_list can only contrain terms with in be 'train', 'val', or 'test'\"\n",
    "\n",
    "    # specify no shuffling for validation and test\n",
    "    if_shuffle_list = [True if subset == \"train\" else False for subset in subset_list]\n",
    "\n",
    "    if encoder_name in TRANSFORMER_INFO.keys():\n",
    "        encoder_class = ESMEncoder\n",
    "\n",
    "    return (\n",
    "        DataLoader(\n",
    "            dataset=ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_class=encoder_class,\n",
    "                encoder_name=encoder_name,\n",
    "                embed_layer=embed_layer,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_path=embed_path,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                **encoder_params,\n",
    "            ),\n",
    "            batch_size=loader_batch_size,\n",
    "            shuffle=if_shuffle,\n",
    "            worker_init_fn=worker_seed,\n",
    "        )\n",
    "        for subset, if_shuffle in zip(subset_list, if_shuffle_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset=ProtranDataset(\n",
    "                dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\",\n",
    "                subset=\"val\",\n",
    "                encoder_class=ESMEncoder,\n",
    "                encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "                embed_layer=6,\n",
    "                embed_batch_size=128,\n",
    "                flatten_emb=\"mean\",\n",
    "                embed_path=None,\n",
    "                seq_start_idx=0,\n",
    "                seq_end_idx=56,\n",
    "                # **encoder_params,\n",
    "            ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<__main__.ProtranDataset at 0x7f0559a2f190>,),\n",
       " <__main__.ProtranDataset at 0x7f0559a2f190>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset, val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43, 768])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0].layer0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.62s/it]\n",
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1_t6_43M_UR50S upto 6 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_loader, val_loader, test_loader = split_protrain_loader(\n",
    "train_loader, val_loader = split_protrain_loader(\n",
    "    dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\",\n",
    "    encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "    embed_layer=6,\n",
    "    embed_batch_size=128,\n",
    "    flatten_emb=\"mean\",\n",
    "    embed_path=None,\n",
    "    seq_start_idx=0,\n",
    "    seq_end_idx=56,\n",
    "    # subset_list=[\"train\", \"val\", \"test\"],\n",
    "    subset_list=[\"train\", \"val\"],\n",
    "    loader_batch_size=64,\n",
    "    worker_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name=\"esm1_t6_43M_UR50S\"\n",
    "embed_batch_size=128\n",
    "flatten_emb=\"mean\"\n",
    "embed_path=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8029e+00],\n",
      "        [7.1123e-01],\n",
      "        [8.6005e-01],\n",
      "        [6.1898e-01],\n",
      "        [2.2602e+00],\n",
      "        [1.7291e+00],\n",
      "        [1.7197e+00],\n",
      "        [2.2677e+00],\n",
      "        [1.7626e+00],\n",
      "        [2.3160e-01],\n",
      "        [6.1434e-01],\n",
      "        [6.1031e-01],\n",
      "        [1.0339e+00],\n",
      "        [5.1069e-01],\n",
      "        [6.9899e-01],\n",
      "        [1.8930e+00],\n",
      "        [0.0000e+00],\n",
      "        [6.9803e-01],\n",
      "        [5.9696e-01],\n",
      "        [1.1877e+00],\n",
      "        [9.7038e-01],\n",
      "        [5.5813e-01],\n",
      "        [4.1580e+00],\n",
      "        [8.5351e-01],\n",
      "        [6.0882e-01],\n",
      "        [1.0669e+00],\n",
      "        [1.7081e+00],\n",
      "        [3.0943e+00],\n",
      "        [2.9738e-03],\n",
      "        [7.3959e-02],\n",
      "        [1.9298e+00],\n",
      "        [1.9539e+00],\n",
      "        [6.2064e-01],\n",
      "        [7.0479e-01],\n",
      "        [1.1327e+00],\n",
      "        [1.5569e-01],\n",
      "        [1.9346e+00],\n",
      "        [1.2409e+00],\n",
      "        [9.9316e-01],\n",
      "        [1.4394e+00],\n",
      "        [2.0135e+00],\n",
      "        [4.0264e+00],\n",
      "        [1.2880e+00]], dtype=torch.float64) <class 'torch.Tensor'> tensor([[0.0040, 0.1047, 0.0535,  ..., 1.5408, 0.6691, 0.8643],\n",
      "        [0.0061, 0.1083, 0.0585,  ..., 1.5472, 0.6675, 0.8631],\n",
      "        [0.0049, 0.1034, 0.0558,  ..., 1.5488, 0.6625, 0.8598],\n",
      "        ...,\n",
      "        [0.0055, 0.1097, 0.0575,  ..., 1.5440, 0.6829, 0.8628],\n",
      "        [0.0079, 0.1091, 0.0568,  ..., 1.5320, 0.6692, 0.8652],\n",
      "        [0.0060, 0.1049, 0.0549,  ..., 1.5478, 0.6645, 0.8671]]) <class 'torch.Tensor'> 43\n",
      "[[ 1.39672405]\n",
      " [ 1.42511868]\n",
      " [ 1.2593073 ]\n",
      " [ 0.85492048]\n",
      " [ 2.39003233]\n",
      " [ 1.37863426]\n",
      " [ 1.2918054 ]\n",
      " [ 1.33670077]\n",
      " [ 1.19005925]\n",
      " [ 0.88029534]\n",
      " [ 0.46654776]\n",
      " [ 0.30497967]\n",
      " [ 1.03238849]\n",
      " [ 0.67347684]\n",
      " [ 0.69430161]\n",
      " [ 1.82580226]\n",
      " [-0.06580689]\n",
      " [ 0.5408041 ]\n",
      " [ 0.23972444]\n",
      " [ 2.59669843]\n",
      " [ 1.56524145]\n",
      " [ 1.29760386]\n",
      " [ 2.28414421]\n",
      " [ 1.78342131]\n",
      " [ 0.85271665]\n",
      " [ 0.13158453]\n",
      " [ 2.35162837]\n",
      " [ 2.05757077]\n",
      " [ 0.66898073]\n",
      " [ 0.59255373]\n",
      " [ 1.87935702]\n",
      " [ 1.71433629]\n",
      " [ 1.03651562]\n",
      " [ 1.12977369]\n",
      " [ 1.4654745 ]\n",
      " [ 1.31527042]\n",
      " [ 1.29829809]\n",
      " [ 1.33670348]\n",
      " [ 0.78409298]\n",
      " [ 1.15165368]\n",
      " [ 1.29760305]\n",
      " [ 2.7590103 ]\n",
      " [ 1.87069804]]\n"
     ]
    }
   ],
   "source": [
    "for (y, sequence, mut_name, mut_numb, *layer_emb) in val_loader:\n",
    "    print(y, type(y), layer_emb[0], type(layer_emb[0]), len(layer_emb[0]))\n",
    "\n",
    "    model = Ridge(alpha = 0.5, normalize = True, tol = 0.001, \\\n",
    "              solver ='auto', random_state = 42)\n",
    "    \n",
    "    model.fit(layer_emb[0], y)\n",
    "\n",
    "    print(model.predict(layer_emb[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_model(params_list: list[dict]):\n",
    "    for params in params_list:\n",
    "        # init model\n",
    "        model = Ridge(**params)\n",
    "\n",
    "        train_preds = []\n",
    "        val_preds = []\n",
    "        test_preds = []\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "42",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m test_preds \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m layer_numb \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000005vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m (x, y, _, _, _) \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000005vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000005vscode-remote?line=12'>13</a>\u001b[0m     model\u001b[39m.\u001b[39mfit(x[layer_numb], y)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb Cell 6'\u001b[0m in \u001b[0;36mProtranDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=327'>328</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=329'>330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=330'>331</a>\u001b[0m \u001b[39m    Return the item in the order of\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=331'>332</a>\u001b[0m \u001b[39m    encoded sequence (x), target (y), sequence, mut_name (optional), mut_numb (optional)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=332'>333</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=334'>335</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=335'>336</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx[idx],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=336'>337</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[idx],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=337'>338</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msequence[idx],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=338'>339</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmut_name[idx],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=339'>340</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmut_numb[idx],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/sklearn.ipynb#ch0000004vscode-remote?line=340'>341</a>\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 42"
     ]
    }
   ],
   "source": [
    "# training model with 0.5 alpha value\n",
    "model = Ridge(alpha = 0.5, normalize = True, tol = 0.001, \\\n",
    "              solver ='auto', random_state = 42)\n",
    "\n",
    "train_preds = []\n",
    "val_preds = []\n",
    "test_preds = []\n",
    "\n",
    "layer_numb = 0\n",
    "\n",
    "for (x, y, _, _, _) in train_loader:\n",
    "    print(x)\n",
    "    model.fit(x[layer_numb], y)\n",
    "    \n",
    "for (x, y, _, _, _) in train_loader:\n",
    "    train_preds.append(model.predict(x[layer_numb]))\n",
    "\n",
    "for (x, y, _, _, _) in val_loader:\n",
    "    val_preds.append(model.predict(x[layer_numb]))\n",
    "\n",
    "# need to add to loop over alpha etc\n",
    "\n",
    "for (x, y, _, _, _) in test_loader:\n",
    "    test_preds.append(model.predict(x[layer_numb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(train_preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes for models\"\"\"\n",
    "\n",
    "# Import third party modules\n",
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as sm\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import torch\n",
    "\n",
    "# Import custom objects\n",
    "from code.params.globals import BATCH_SIZE\n",
    "from code.params.model_info import *\n",
    "# from code.models.utils import pick_best_params\n",
    "\n",
    "\n",
    "# Define an abstract class for holding all models\n",
    "class AbstractModelWrapper(ABC):\n",
    "    \"\"\"\n",
    "    This is an abstract class for wrapping all models used for making\n",
    "    mutant fitness predictions. It will be inherited by a model class\n",
    "    for making both supervised and unsupervised predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization loads the parent sequence\n",
    "    def __init__(self, *constructor_args, **constructor_kwargs):\n",
    "        \"\"\"\n",
    "        Loads the reference sequence that will be used to make predictions and\n",
    "        stores parameters for the model that will be trained.\n",
    "        \n",
    "        Args:\n",
    "        - constructor_args: Positional arguments that will be passed to the model\n",
    "            constructor for this wrapper.\n",
    "        - constructor_kwargs: Keyword arguments that will be passed to the model\n",
    "            constructor for this wrapper.\n",
    "        \"\"\"\n",
    "        # Initialize the sequence loader\n",
    "        super().__init__(fasta_loc)\n",
    "\n",
    "        # print(\"AbstractModelWrapper constructor_kwargs inputs before assigning\")\n",
    "        # print(constructor_kwargs)\n",
    "\n",
    "        # Store the constructor args and kwargs as instance variables\n",
    "        self._constructor_args = constructor_args\n",
    "        self._constructor_kwargs = deepcopy(constructor_kwargs)\n",
    "\n",
    "        # print(\"AbstractModelWrapper constructor_kwargs inputs\")\n",
    "        # print(constructor_kwargs)\n",
    "        # print(\"AbstractModelWrapper self._constructor_kwargs\")\n",
    "        # print(self._constructor_kwargs)\n",
    "\n",
    "        # The default model is \"None\". We need to initialize a model later.\n",
    "        self.model = None\n",
    "\n",
    "    # All classes will be able to make a prediction\n",
    "    def predict(self, mutants, muts_preencoded=False, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Makes predictions given an input of mutants or encoded mutants. If\n",
    "        unencoded mutants are passed in, then they will first be encoded.\n",
    "        Otherwise, encodings passed in will be assumed correct and fed into the\n",
    "        appropriate model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mutants (tuple of tuple of tuples OR ndarray/torch tensor):\n",
    "            If a tuple of tuple of tuples: All mutations to make a prediction\n",
    "            for. Will use the reference sequence loaded during initialization in\n",
    "            combination with the mutants passed in here. Each entry in the\n",
    "            outer tuple is a tuple containing mutation information for one\n",
    "            variant. Each mutation is specified by its own tuple with the format\n",
    "            (Reference AA, Mutation Position, Mutant AA). Mutation positions are\n",
    "            assumed to be 1-indexed relative to the sequence. See an example\n",
    "            below.\n",
    "            If ndarray or torch tensor: This is mutants that have already been\n",
    "            encoded.\n",
    "        Returns\n",
    "        -------\n",
    "        predictions (ndarray): Numpy array containing the predictions for the\n",
    "            input set of mutants.\n",
    "        Examples\n",
    "        --------\n",
    "        An example input for `mutants` when tuple of tuple of tuples:\n",
    "            mutants = (\n",
    "                ((V, 39, D), (D, 40, G)),\n",
    "                ((V, 39, D), (D, 40, G), (G, 41, F))\n",
    "                )\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"\"\"This is the predict inside the AbstractModelWrapper \\\n",
    "        # calling _complete_prediction\"\"\")\n",
    "\n",
    "        # Confirm that we have a model and scaler for making predictions\n",
    "        assert self.model is not None, \"Must train a model before making predictions\"\n",
    "\n",
    "        # Check mutations for accuracy based on whether they are pre-encoded\n",
    "        # or not\n",
    "        self._check_mutants(mutants, muts_preencoded)\n",
    "\n",
    "        # Every child class will need to finish this function\n",
    "        unscaled_preds = self._complete_prediction(\n",
    "            mutants, muts_preencoded=muts_preencoded, **pred_kwargs\n",
    "        )\n",
    "\n",
    "        # print(\"mutants and unscaled_preds dims:\")\n",
    "        # print(mutants.shape, unscaled_preds.shape)\n",
    "\n",
    "        # Confirm that predictions are a numpy array and that they are 2d\n",
    "        assert isinstance(\n",
    "            unscaled_preds, np.ndarray\n",
    "        ), \"Predictions should be numpy array\"\n",
    "        assert len(unscaled_preds.shape) == 2, \"Expect 2D prediction array\"\n",
    "\n",
    "        # Return the unscaled preds. Supervised models will scale them based on\n",
    "        # the standard scaler used at input\n",
    "        return unscaled_preds\n",
    "\n",
    "    def evaluate(self, mutants, true_fitness, muts_preencoded=False, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Wraps making predictions and evaluation of predictions using a number\n",
    "        of evaluation metrics.\n",
    "        mutants (tuple of tuple of tuples): All mutations to make a prediction\n",
    "            for. Will use the reference sequence loaded during initialization in\n",
    "            combination with the mutants passed in here. Each entry in the\n",
    "            outer tuple is a tuple containing mutation information for one\n",
    "            variant. Each mutation is specified by its own tuple with the format\n",
    "            (Reference AA, Mutation Position, Mutant AA). Mutation positions are\n",
    "            assumed to be 1-indexed relative to the sequence. See an example\n",
    "            below.\n",
    "        true_fitness (ndarray): The true fitness values of the mutants.\n",
    "        muts_preencoded (bool): Whether or not the input mutants are pre-encoded.\n",
    "            If not pre-encoded, this function expects a tuple of tuples of tuples.\n",
    "            If it is pre-encoded, then the mutants should be formatted already\n",
    "            to go into whatever model will be making predictions.\n",
    "        Examples\n",
    "        --------\n",
    "        An example input for `mutants`:\n",
    "            mutants = (\n",
    "                ((V, 39, D), (D, 40, G)),\n",
    "                ((V, 39, D), (D, 40, G), (G, 41, F))\n",
    "                )\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(\"This is the evulate inside the AbstractModelWrapper\")\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.predict(\n",
    "            mutants, muts_preencoded=muts_preencoded, **pred_kwargs\n",
    "        )\n",
    "\n",
    "        # Confirm that the predictions and true values are the same shape and\n",
    "        # that they are two dimensional\n",
    "        \n",
    "        # print(\"The kwargs for the evaluate function in AbstractModelWrapper\")\n",
    "        # print(pred_kwargs)\n",
    "        # print(\"After the predict in the evaluate function in AbstractModelWrapper\")\n",
    "        # print(mutants.shape, predictions.shape, true_fitness.shape)\n",
    "        \n",
    "        \"\"\"assert isinstance(true_fitness, np.ndarray), \"True values should be numpy array\"\n",
    "        assert (\n",
    "            predictions.shape == true_fitness.shape\n",
    "        ), \"Mismatch in truth and prediction shapes\"\n",
    "        n_fitness_vals = predictions.shape[1]\n",
    "        # Evaluate the predictions using spearman rho\n",
    "        rho_vals = np.array(\n",
    "            [\n",
    "                ss.spearmanr(true_fitness[:, i], predictions[:, i])[0]\n",
    "                for i in range(n_fitness_vals)\n",
    "            ]\n",
    "        )\n",
    "        # Evaluate predictions using mse. Keep errors separate by label.\n",
    "        mses = sm.mean_squared_error(\n",
    "            true_fitness, predictions, multioutput=\"raw_values\"\n",
    "        )\n",
    "        return [predictions, [rho_vals, mses]]\"\"\"\n",
    "        \n",
    "        return self.evaluate_metrics(predictions, true_fitness, muts_preencoded, **pred_kwargs)\n",
    "    \n",
    "    def evaluate_metrics(self, predictions, true_fitness, muts_preencoded=False, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Wraps making evaluation metrics calculation given predictions.\n",
    "        predictions (ndarray): \n",
    "        true_fitness (ndarray): The true fitness values of the mutants.\n",
    "        muts_preencoded (bool): Whether or not the input mutants are pre-encoded.\n",
    "            If not pre-encoded, this function expects a tuple of tuples of tuples.\n",
    "            If it is pre-encoded, then the mutants should be formatted already\n",
    "            to go into whatever model will be making predictions.\n",
    "        \"\"\"\n",
    "        # print(\"predictions.shape, true_fitness.shape\")\n",
    "        # print(predictions.shape, true_fitness.shape)\n",
    "        assert isinstance(true_fitness, np.ndarray), \"True values should be numpy array\"\n",
    "        assert (\n",
    "            predictions.shape == true_fitness.shape\n",
    "        ), \"Mismatch in truth and prediction shapes\"\n",
    "        n_fitness_vals = predictions.shape[1]\n",
    "\n",
    "        # Evaluate the predictions using spearman rho\n",
    "        rho_vals = np.array(\n",
    "            [\n",
    "                ss.spearmanr(true_fitness[:, i], predictions[:, i])[0]\n",
    "                for i in range(n_fitness_vals)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Evaluate predictions using mse. Keep errors separate by label.\n",
    "        mses = sm.mean_squared_error(\n",
    "            true_fitness, predictions, multioutput=\"raw_values\"\n",
    "        )\n",
    "\n",
    "        return [predictions, [rho_vals, mses]]\n",
    "\n",
    "    @abstractmethod\n",
    "    def _check_mutants(self, mutants, muts_preencoded):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _complete_prediction(self, mutants, muts_preencoded, **pred_kwargs):\n",
    "        \"\"\"\n",
    "        Every child class will need to finish up the prediction function.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def constructor_args(self):\n",
    "        return self._constructor_args\n",
    "\n",
    "    @property\n",
    "    def constructor_kwargs(self):\n",
    "        return self._constructor_kwargs\n",
    "\n",
    "\n",
    "# Define an abstract class for unsupervised models\n",
    "class AbstractUnsupervisedModelWrapper(AbstractModelWrapper):\n",
    "    def format_mutants(self, unformatted_mutlists, *reformat_args):\n",
    "\n",
    "        # First confirm that the unformatted mutants are correct\n",
    "        self._check_unformatted_mutants(unformatted_mutlists)\n",
    "\n",
    "        # Complete formatting\n",
    "        reformatted_mutlists = self._complete_reformat(\n",
    "            unformatted_mutlists, *reformat_args\n",
    "        )\n",
    "\n",
    "        # Check the mutants at the end for accuracy\n",
    "        self._check_formatted_mutants(reformatted_mutlists)\n",
    "\n",
    "        return reformatted_mutlists\n",
    "\n",
    "    def _check_mutants(self, mutants, muts_preencoded):\n",
    "\n",
    "        # The function we use for checking depends on whether or not the mutants\n",
    "        # are pre-encoded\n",
    "        if muts_preencoded:\n",
    "            self._check_formatted_mutants(mutants)\n",
    "        else:\n",
    "            self._check_unformatted_mutants(mutants)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _check_formatted_mutants(self, mutants):\n",
    "        \"\"\"\n",
    "        Confirms that the mutants are input to the class in a format expected by\n",
    "        the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # All classes will need a function for formatting mutants. This can mean\n",
    "    # tokenizing them for the NLP models or just rearranging the input mutants\n",
    "    # for EVcouplings or triad.\n",
    "    @abstractmethod\n",
    "    def _complete_reformat(self, mutants):\n",
    "        \"\"\"\n",
    "        When mutations are not pre-encoded, this function handles their processing\n",
    "        so that they can be converted from the standard input format to a format\n",
    "        useful for the model. This must be overwritten by inheriting classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# Define an abstract class for supervised models\n",
    "class AbstractSupervisedModelWrapper(AbstractModelWrapper):\n",
    "    \"\"\"\n",
    "    This is an abstract class for wrapping all supervised models used for making\n",
    "    single-to-multi mutant predictions. This class will be inherited by classes\n",
    "    specific to the different model architectures/types we will be training.\n",
    "    \"\"\"\n",
    "\n",
    "    # We add a fitness scaler to the initialized attributes\n",
    "    def __init__(\n",
    "        self,\n",
    "        fasta_loc,\n",
    "        model_class,\n",
    "        encoder=None,\n",
    "        *constructor_args,\n",
    "        **constructor_kwargs\n",
    "    ):\n",
    "\n",
    "        # Initialize using parent method\n",
    "        super().__init__(fasta_loc, *constructor_args, **constructor_kwargs)\n",
    "\n",
    "        # Record the model class\n",
    "        self.model_class = model_class\n",
    "\n",
    "        # print(\n",
    "        #     \"\"\"initilizing models in AbstractSupervisedModelWrapper \\\n",
    "        #     with constructor_kwargs and self.constructor_kwargs\"\"\"\n",
    "        # )\n",
    "        # print(constructor_kwargs)\n",
    "        # print(self.constructor_kwargs)\n",
    "\n",
    "        # Initialize the model\n",
    "        self._initialize_model()\n",
    "\n",
    "        # Specify the encoder encoding type\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # The default fitness scaler is \"None\". We initialize one during training.\n",
    "        self.fitness_scaler = None\n",
    "\n",
    "    # All classes will have a train method. This method might not do anything\n",
    "    # for some of them though (like triad)\n",
    "    def train(\n",
    "        self,\n",
    "        train_mutants,\n",
    "        train_fitnesses,\n",
    "        test_mutants=None,\n",
    "        test_fitnesses=None,\n",
    "        muts_preencoded=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        _skip_check=False,\n",
    "        **pred_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains a model given input mutants and fitnesses.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mutants (tuple of tuple of tuples OR ndarray/torch tensor):\n",
    "            If a tuple of tuple of tuples: All mutations to make a prediction\n",
    "            for. Will use the reference sequence loaded during initialization in\n",
    "            combination with the mutants passed in here. Each entry in the\n",
    "            outer tuple is a tuple containing mutation information for one\n",
    "            variant. Each mutation is specified by its own tuple with the format\n",
    "            (Reference AA, Mutation Position, Mutant AA). Mutation positions are\n",
    "            assumed to be 1-indexed relative to the sequence. See an example\n",
    "            below.\n",
    "            If ndarray or torch tensor: This is mutants that have already been\n",
    "            encoded.\n",
    "        fitnesses (2D array with same type as `mutants` after they have\n",
    "        been encoded):\n",
    "            Provides the labels against which training will be performed\n",
    "        muts_preencoded (bool): Whether or not the input mutants are pre-encoded.\n",
    "            If not pre-encoded, this function expects a tuple of tuples of tuples.\n",
    "            If it is pre-encoded, then the mutants should be formatted already\n",
    "            to go into whatever model will be making predictions.\n",
    "        _skip_check (bool): Private method. Will skip checking the input data.\n",
    "            Used in conjunction with self.train_cv (self.train_cv handles\n",
    "            checking, so there's no need to do it on every iteration)\n",
    "        \"\"\"\n",
    "        # Confirm training inputs are acceptable\n",
    "        if not _skip_check:\n",
    "            self._check_training_data(\n",
    "                train_mutants, train_fitnesses, None, muts_preencoded\n",
    "            )\n",
    "\n",
    "        # print(\"\"\"initilizing models in the train \\ \n",
    "        #     inside the AbstractSupervisedModelWrapper \\\n",
    "        #     with self.constructor_args and self.constructor_kwargs\"\"\")\n",
    "        \n",
    "        # print(self.constructor_args)\n",
    "        # print(self.constructor_kwargs)\n",
    "        \n",
    "        # Initialize a new model.\n",
    "        self._initialize_model()\n",
    "\n",
    "        # Build a scaler and scale fitnesses\n",
    "        self.fitness_scaler = StandardScaler()\n",
    "        scaled_train_fitnesses = self.fitness_scaler.fit_transform(train_fitnesses)\n",
    "\n",
    "        # Check testing data if provided. Scale it too.\n",
    "        if test_mutants is not None:\n",
    "            assert test_fitnesses is not None, \"Did not provide test_fitnesses\"\n",
    "            self._check_training_data(\n",
    "                test_mutants, test_fitnesses, None, muts_preencoded\n",
    "            )\n",
    "            scaled_test_fitnesses = self.fitness_scaler.transform(\n",
    "                test_fitnesses, copy=True\n",
    "            )\n",
    "        else:\n",
    "            scaled_test_fitnesses = None\n",
    "\n",
    "        # print(\"Now complete training with pred_kwargs\")\n",
    "        # print(pred_kwargs)\n",
    "\n",
    "        return self._complete_training(\n",
    "            train_mutants,\n",
    "            scaled_train_fitnesses,\n",
    "            test_mutants,\n",
    "            scaled_test_fitnesses,\n",
    "            muts_preencoded,\n",
    "            batch_size,\n",
    "            **pred_kwargs\n",
    "        )\n",
    "\n",
    "    # All classes will have a cv-train method.\n",
    "    def train_cv(\n",
    "        self,\n",
    "        mutants,\n",
    "        fitnesses,\n",
    "        muts_preencoded=False,\n",
    "        positions=None,\n",
    "        n_cv=N_CV,\n",
    "        shuffle=True,\n",
    "        random_state=2,\n",
    "        split_type=\"random\",\n",
    "        **pred_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs kfold cross validation to train a model.\n",
    "        \"\"\"\n",
    "        # Check the training data\n",
    "        self._check_training_data(mutants, fitnesses, positions, muts_preencoded)\n",
    "\n",
    "        # Generate splits from the cross-validator\n",
    "        splits = self._build_splits(\n",
    "            fitnesses, positions, n_cv, shuffle, random_state, split_type\n",
    "        )\n",
    "\n",
    "        # Loop over all splits\n",
    "        all_test_errs = np.empty(\n",
    "            [n_cv, N_EVAL_METRICS, fitnesses.shape[1]]\n",
    "        )  # (n_cv, N metrics, n fitness values)\n",
    "\n",
    "        stop_epoch_sum = 0\n",
    "\n",
    "        for i, (train_inds, test_inds) in enumerate(splits):\n",
    "            print(\"Running fold {0}\".format(i+1))\n",
    "            # Make the split. How the split is performed depends on the dtype\n",
    "            # of the mutants\n",
    "            train_muts, test_muts = self._make_split(train_inds, test_inds, mutants)\n",
    "            train_fitness, test_fitness = self._make_split(\n",
    "                train_inds, test_inds, fitnesses\n",
    "            )\n",
    "\n",
    "            # Train\n",
    "            # print(\"\"\"This is the train inside the AbstractSupervisedModelWrapper \\\n",
    "            #     train_cv with pred_kwargs\"\"\")\n",
    "            # print(pred_kwargs)\n",
    "\n",
    "            train_op = self.train(\n",
    "                train_muts,\n",
    "                train_fitness,\n",
    "                test_mutants=test_muts,\n",
    "                test_fitnesses=test_fitness,\n",
    "                muts_preencoded=muts_preencoded,\n",
    "                _skip_check=True,\n",
    "                **pred_kwargs\n",
    "            )\n",
    "\n",
    "            # get the number of early stopping epoch\n",
    "            if train_op is not None:\n",
    "                stop_epoch_sum += train_op\n",
    "\n",
    "            # Evaluate\n",
    "            # print(\"\"\"This is the evaluate inside the AbstractSupervisedModelWrapper \\\n",
    "            # train_cv with pred_kwargs\"\"\")\n",
    "            # print(pred_kwargs)\n",
    "\n",
    "            all_test_errs[i] = self.evaluate(\n",
    "                test_muts, test_fitness, muts_preencoded=muts_preencoded, **pred_kwargs\n",
    "            )[1]\n",
    "\n",
    "        # Return the average test error over the different splits\n",
    "        # print(\"avg epoch after train_cv {0}\".format(np.int(stop_epoch_sum/n_cv)+1))\n",
    "        \n",
    "        \"\"\"print(\"all_test_errs\")\n",
    "        print(all_test_errs)\n",
    "        \n",
    "        print(\"all_test_errs taking the mean\")\n",
    "        print(all_test_errs.mean(axis=0))\"\"\"\n",
    "        \n",
    "        return all_test_errs.mean(axis=0), np.int(stop_epoch_sum / n_cv) + 1\n",
    "\n",
    "    # We extend the prediction method to rescale any predictions made\n",
    "    def predict(\n",
    "        self, mutants, muts_preencoded=False, batch_size=BATCH_SIZE, **pred_kwargs\n",
    "    ):\n",
    "\n",
    "        # print(\n",
    "        #     \"This is the predict inside the AbstractSupervisedModelWrapper with pred_kwargs\"\n",
    "        # )\n",
    "        # print(pred_kwargs)\n",
    "\n",
    "        # Confirm that we have a scaler\n",
    "        assert (\n",
    "            self.fitness_scaler is not None\n",
    "        ), \"Must train model before making predictions\"\n",
    "\n",
    "        # See if we need an encoder\n",
    "        self._check_for_encoder(muts_preencoded)\n",
    "\n",
    "        # Get the unscaled predictions\n",
    "        unscaled_predictions = super().predict(\n",
    "            mutants,\n",
    "            muts_preencoded=muts_preencoded,\n",
    "            batch_size=batch_size,\n",
    "            **pred_kwargs\n",
    "        )\n",
    "\n",
    "        # print(\"mutants and unscaled_predictions dim:\")\n",
    "        # print(mutants.shape, unscaled_predictions.shape)\n",
    "\n",
    "        # Return the rescaled predictions\n",
    "        return self.fitness_scaler.inverse_transform(unscaled_predictions)\n",
    "\n",
    "    def hyperopt_gridsearch(\n",
    "        self,\n",
    "        mutants,\n",
    "        fitnesses,\n",
    "        constructor_args_list,\n",
    "        constructor_kwargs_list,\n",
    "        **cv_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs a grid search to identify the optimal hyperparameters for the\n",
    "        model.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\"\"Inside hyperopt_gridsearch in AbstractSupervisedModelWrapper \\\n",
    "        with constructor_kwargs_list and cv_kwargs\"\"\")\n",
    "        print(constructor_kwargs_list, cv_kwargs)\n",
    "\n",
    "        # Confirm that there are as many constructor args as there are kwargs\n",
    "        n_tests = len(constructor_args_list)\n",
    "        assert n_tests == len(constructor_kwargs_list)\n",
    "\n",
    "        best_cv_rho = 0\n",
    "        best_cv_mse = np.inf\n",
    "        opt_params = np.nan\n",
    "        train_params = {}\n",
    "\n",
    "        # Loop over all combinations of constructor args and kwargs and test\n",
    "        n_fitness_vals = fitnesses.shape[1]\n",
    "        hyperopt_test_errs = np.empty([n_tests, N_EVAL_METRICS, n_fitness_vals])\n",
    "        for i, (constructor_args, constructor_kwargs) in enumerate(\n",
    "            zip(constructor_args_list, constructor_kwargs_list)\n",
    "        ):\n",
    "\n",
    "            # Reassign constructor args and kwargs\n",
    "            self._constructor_args = constructor_args\n",
    "            self._constructor_kwargs = deepcopy(constructor_kwargs)\n",
    "            \n",
    "            # print(\"\"\"Inside hyperopt_gridsearch in AbstractSupervisedModelWrapper \\\n",
    "            # before train_cv constructor_kwargs and self._constructor_kwargs\"\"\")\n",
    "            # print(constructor_kwargs, self._constructor_kwargs)\n",
    "\n",
    "            # Run cross-validation and record error\n",
    "            hyperopt_test_errs[i], stop_epoch_avg = self.train_cv(\n",
    "                mutants, fitnesses, **cv_kwargs\n",
    "            )\n",
    "            # print(\"stop_epoch_avg in hyper tune is {0}\".format(stop_epoch_avg))\n",
    "\n",
    "            current_rho = hyperopt_test_errs[i][0][0]\n",
    "            current_mse = hyperopt_test_errs[i][1][0]\n",
    "            # print(\"current rho: {0}, mse: {1}\".format(current_rho, current_mse))\n",
    "\n",
    "            \"\"\"best_cv_mse, best_cv_rho, opt_params, _ = pick_best_params(current_mse, best_cv_mse, \n",
    "                                                                    current_rho, best_cv_rho, \n",
    "                                                                    self._constructor_kwargs)\"\"\"\n",
    "            \"\"\"# Update the best errors and associated kwargs\n",
    "            if current_mse != np.nan and current_mse < best_cv_mse:\n",
    "                best_cv_mse = current_mse\n",
    "                best_cv_rho = current_rho\n",
    "                opt_params = self._constructor_kwargs\n",
    "            elif (\n",
    "                current_mse != np.nan\n",
    "                and current_rho != np.nan\n",
    "                and current_mse == best_cv_mse\n",
    "                and current_rho > best_cv_rho\n",
    "            ):\n",
    "                best_cv_rho = current_rho\n",
    "                opt_params = self._constructor_kwargs\"\"\"\n",
    "\n",
    "            if (current_mse != np.nan and current_mse < best_cv_mse) or (\n",
    "                current_mse != np.nan\n",
    "                and current_rho != np.nan\n",
    "                and current_mse == best_cv_mse\n",
    "                and current_rho > best_cv_rho\n",
    "            ):\n",
    "                best_cv_mse = current_mse\n",
    "                best_cv_rho = current_rho\n",
    "                opt_params = self._constructor_kwargs\n",
    "\n",
    "            if stop_epoch_avg != 1:\n",
    "                train_params = {\"epochs\": stop_epoch_avg}\n",
    "\n",
    "        print(\n",
    "            \"Chosen best opt_params, train_params: {0} {1}\".format(\n",
    "                opt_params, train_params\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return hyperopt_test_errs, opt_params, train_params, best_cv_mse, best_cv_rho\n",
    "\n",
    "    # Confirm that we have an encoder if mutations are not preencoded\n",
    "    def _check_for_encoder(self, muts_preencoded):\n",
    "        if not muts_preencoded:\n",
    "            assert (\n",
    "                self.encoder is not None\n",
    "            ), \"If muts are not pre-encoded, an encoder is needed\"\n",
    "\n",
    "    def _check_training_data(self, mutants, fitnesses, positions, muts_preencoded):\n",
    "        \"\"\"\n",
    "        Confirms assumptions that we make about the training data as well as\n",
    "        encodes mutants if a tuple of tuple of tuples was passed in.\n",
    "        \"\"\"\n",
    "        # See if we need an encoder\n",
    "        self._check_for_encoder(muts_preencoded)\n",
    "\n",
    "        # Confirm that the input mutants will work. Determine whether the mutants\n",
    "        # are encoded or not.\n",
    "        self._check_mutants(mutants, muts_preencoded)\n",
    "\n",
    "        # Confirm that (1) `fitnesses` is 2D and (2) that the lengths of the\n",
    "        # mutant and fitness arrays match\n",
    "        assert len(fitnesses.shape) == 2, \"Expect 2D array for fitness\"\n",
    "        assert len(mutants) == len(fitnesses), \"Mismatch in training data lengths\"\n",
    "\n",
    "        # If the mutants are pre-encoded, then we expect the fitnesses and\n",
    "        # mutants to have the same datatype\n",
    "        if muts_preencoded:\n",
    "            assert type(fitnesses) is type(\n",
    "                mutants\n",
    "            ), \"Expect x and y to have the same type\"\n",
    "\n",
    "        # If positions are provided, then they should be 1d and and have the\n",
    "        # same length as mutants and fitnesses\n",
    "        if positions is not None:\n",
    "            assert len(positions.shape) == 1, \"Expect 1D array for positions\"\n",
    "            assert len(positions) == len(\n",
    "                fitnesses\n",
    "            ), \"Mismatch in number of positions provided\"\n",
    "\n",
    "    def _build_splits(\n",
    "        self, fitnesses, positions, n_cv, shuffle, random_state, split_type\n",
    "    ):\n",
    "\n",
    "        # Confirm that fitnesses are a numpy array\n",
    "        assert isinstance(fitnesses, np.ndarray), \"Fitnesses expected to be numpy array\"\n",
    "\n",
    "        # Group split kwargs\n",
    "        splargs = {\"n_splits\": n_cv, \"shuffle\": shuffle, \"random_state\": random_state}\n",
    "\n",
    "        # Initialize the appropriate object. If the split type is \"random\", then\n",
    "        # we slit randomly by position\n",
    "        if split_type == \"random\":\n",
    "\n",
    "            # Build the object\n",
    "            kfold_obj = KFold(**splargs)\n",
    "\n",
    "            # Generate splits\n",
    "            splits = kfold_obj.split(fitnesses)\n",
    "\n",
    "        # If the split type is \"even_pos\", then we split such that each position\n",
    "        # is roughly equivalent represented in each split\n",
    "        elif split_type == \"even_pos\":\n",
    "\n",
    "            # Confirm that classes are provided, the positions are a numpy array,\n",
    "            # the positions have the same length as fitnesses, and that the positions\n",
    "            # are a 1d numpy array\n",
    "            assert positions is not None, \"Must provide position information\"\n",
    "            assert isinstance(positions, np.ndarray), \"Positions should be numpy array\"\n",
    "            assert len(positions.shape) == 1, \"Positions should be 1d array\"\n",
    "            assert len(positions) == len(\n",
    "                fitnesses\n",
    "            ), \"Mismatch between fitnesses and positions\"\n",
    "\n",
    "            # Build the object\n",
    "            kfold_obj = StratifiedKFold(**splargs)\n",
    "\n",
    "            # Generate splits\n",
    "            splits = kfold_obj.split(X=fitnesses, y=positions)\n",
    "\n",
    "        # Anything else and we have an unrecognized splitter\n",
    "        else:\n",
    "            raise AssertionError(\"Unrecognized kfold type requested\")\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def _make_split(self, train_inds, test_inds, to_split):\n",
    "\n",
    "        # Confirm that the inputs inds are numpy arrays\n",
    "        assert isinstance(train_inds, np.ndarray)\n",
    "        assert isinstance(test_inds, np.ndarray)\n",
    "\n",
    "        # If the object to be split is a numpy array, just fancy index\n",
    "        if isinstance(to_split, np.ndarray):\n",
    "            return to_split[train_inds], to_split[test_inds]\n",
    "\n",
    "        # If the object to be split is a torch tensor, convert training and\n",
    "        # testing inds to tensors first, then split\n",
    "        elif isinstance(to_split, torch.Tensor):\n",
    "            return (\n",
    "                to_split[torch.from_numpy(train_inds)],\n",
    "                to_split[torch.from_numpy(test_inds)],\n",
    "            )\n",
    "\n",
    "        # If the object to be split is a tuple, then we use list-comp\n",
    "        elif isinstance(to_split, tuple):\n",
    "            return (\n",
    "                tuple(to_split[ind] for ind in train_inds),\n",
    "                tuple(to_split[ind] for ind in test_inds),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown datatype input to cross-validation\")\n",
    "\n",
    "    def _check_mutants(self, mutants, muts_preencoded):\n",
    "        \"\"\"\n",
    "        We only check if muts and not preencoded. There is no good way to check\n",
    "        encoded mutations.\n",
    "        \"\"\"\n",
    "        # The function we use for checking depends on whether or not the mutants\n",
    "        # are pre-encoded\n",
    "        if not muts_preencoded:\n",
    "            self._check_unformatted_mutants(mutants)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"\n",
    "        Builds a new instance of self.model using self.constructor_args and\n",
    "        self.constructor_kwargs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _complete_training(\n",
    "        self,\n",
    "        train_mutants,\n",
    "        scaled_train_fitnesses,\n",
    "        test_mutants,\n",
    "        scaled_test_fitnesses,\n",
    "        muts_preencoded,\n",
    "        batch_size,\n",
    "        **pred_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Finishes the `train` method for each child class.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_model_class_name(self):\n",
    "        \"\"\"\n",
    "        Obtain the model class name of the class as a string\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
