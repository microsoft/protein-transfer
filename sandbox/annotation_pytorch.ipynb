{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/structure/secondary_structure/casp12.csv\").target.dtype.kind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/structure/secondary_structure/casp12.csv\").target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEVLEEPAPGPGGADAAERRGLRRLLLSGFQEELRALLVLAGPAFL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFED...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAKRTFSNLETFLIFLLVMMSAITVALLSLLFITSGTIENHKDLGG...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MGNCQAGHNLHLCLAHHPPLVCATLILLLLGLSGLGLGSFLLTHRT...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDPSKQGTLNRVENSVYRTAFKLRSVQTLCQLDLMDSFLIQQVLWR...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence         target    set  \\\n",
       "0  MEVLEEPAPGPGGADAAERRGLRRLLLSGFQEELRALLVLAGPAFL...  Cell membrane  train   \n",
       "1  MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFED...  Cell membrane  train   \n",
       "2  MAKRTFSNLETFLIFLLVMMSAITVALLSLLFITSGTIENHKDLGG...  Cell membrane  train   \n",
       "3  MGNCQAGHNLHLCLAHHPPLVCATLILLLLGLSGLGLGSFLLTHRT...  Cell membrane  train   \n",
       "4  MDPSKQGTLNRVENSVYRTAFKLRSVQTLCQLDLMDSFLIQQVLWR...  Cell membrane  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/annotation/scl/balanced.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([nan, True], dtype=object), array(['train', 'test'], dtype=object))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.validation.unique(), df.set.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cell membrane', 'Cytoplasm', 'Endoplasmic reticulum',\n",
       "       'Golgi apparatus', 'Lysosome/Vacuole', 'Mitochondrion', 'Nucleus',\n",
       "       'Peroxisome', 'Plastid', 'Extracellular'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"data/structure/secondary_structure/casp12.csv\")\n",
    "len(np.unique(np.array(df2[\"target\"][0][1:-1].split(\", \"))))\n",
    "# df2[\"target\"] = df2[\"target\"].apply(lambda x: np.array(x[1:-1].split(\", \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A script with model training and testing details assuming\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "\n",
    "# seed everything\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "torch.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed_all(RAND_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_x_y(\n",
    "    model, device, batch, embed_layer: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    A function process x and y from the loader\n",
    "\n",
    "    Args:\n",
    "    -\n",
    "\n",
    "    Returns:\n",
    "    - x\n",
    "    - y\n",
    "    \"\"\"\n",
    "\n",
    "    # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "    x = batch[4][embed_layer]\n",
    "    y = batch[0]\n",
    "\n",
    "    \"\"\"\n",
    "    # process y depends on model type\n",
    "    # annotation classification\n",
    "    if model.model_name == \"LinearClassifier\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y.flatten())\n",
    "    \"\"\"\n",
    "    \n",
    "    # ss3 / ss8 type\n",
    "    if model.model_name == \"MultiLabelMultiClass\":\n",
    "        # convert the y into np.arrays with -1 padding to the same length\n",
    "        y = np.stack(\n",
    "            [\n",
    "                np.pad(\n",
    "                    i,\n",
    "                    pad_width=(0, x.shape[1] - len(i)),\n",
    "                    constant_values=-1,\n",
    "                )\n",
    "                for i in y\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 0,\n",
    "    flatten_emb: bool | str = False,\n",
    "    # if_encode_all: bool = True,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    criterion: nn.Module | None = None,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    **encoder_params,\n",
    ") -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    Runs one epoch.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - device: torch.device or str\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - optimizer: optional torch.optim.Optimizer, must also provide criterion,\n",
    "        only provided for training\n",
    "\n",
    "    Returns: \n",
    "    - float, average loss over batches\n",
    "    \"\"\"\n",
    "    if optimizer is not None:\n",
    "        assert criterion is not None\n",
    "        model.train()\n",
    "        is_train = True\n",
    "    else:\n",
    "        model.eval()\n",
    "        is_train = False\n",
    "\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        # if not if_encode_all:\n",
    "        # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "        for batch in loader:\n",
    "            x, y = get_x_y(model, device, batch, embed_layer)\n",
    "\n",
    "            \"\"\"\n",
    "            x = batch[4][embed_layer]\n",
    "            y = batch[0]\n",
    "\n",
    "            # process y depends on model type\n",
    "            # annotation classification\n",
    "            if model.model_name == \"LinearClassifier\":\n",
    "                le = LabelEncoder()\n",
    "                y = le.fit_transform(y.flatten())\n",
    "            # ss3 / ss8 type\n",
    "            elif model.model_name == \"MultiLabelMultiClass\":\n",
    "                # convert the y into np.arrays with -1 padding to the same length\n",
    "                y = np.stack(\n",
    "                    [\n",
    "                        np.pad(\n",
    "                            np.array(i[1:-1].split(\", \")),\n",
    "                            pad_width=(0, x.shape[1] - len(i)),\n",
    "                            constant_values=-1,\n",
    "                        )\n",
    "                        for i in y\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \"\"\"\n",
    "\n",
    "            outputs = model(x)\n",
    "\n",
    "            if criterion is not None:\n",
    "                if model.model_name == \"LinearRegression\":\n",
    "                    loss = criterion(outputs, y.float())\n",
    "                elif model.model_name == \"LinearClassifier\":\n",
    "                    loss = criterion(outputs, y.squeeze())\n",
    "\n",
    "                if optimizer is not None:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "    return cum_loss / len(loader)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 0,\n",
    "    flatten_emb: bool | str = False,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    learning_rate: float = 1e-4,\n",
    "    lr_decay: float = 0.1,\n",
    "    epochs: int = 100,\n",
    "    early_stop: bool = True,\n",
    "    tolerance: int = 10,\n",
    "    min_epoch: int = 5,\n",
    "    **encoder_params,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - train_loader: torch.utils.data.DataLoader, \n",
    "    - val_loader: torch.utils.data.DataLoader, \n",
    "    - criterion: nn.Module, loss function, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float, factor by which to decay LR on plateau\n",
    "    - epochs: int, number of epochs to train for\n",
    "    - early_stop: bool = True,\n",
    "\n",
    "    Returns: \n",
    "    - tuple of np.ndarray, (train_losses, val_losses)\n",
    "        train/val_losses: np.ndarray, shape [epochs], entries are average loss\n",
    "        over batches for that epoch\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=lr_decay\n",
    "    )\n",
    "\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "\n",
    "    # init for early stopping\n",
    "    counter = 0\n",
    "    min_val_loss = np.Inf\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        train_losses[epoch] = run_epoch(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            encoder_name=encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            embed_batch_size=embed_batch_size,\n",
    "            flatten_emb=flatten_emb,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        val_loss = run_epoch(\n",
    "            model=model,\n",
    "            loader=val_loader,\n",
    "            encoder_name=encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            embed_batch_size=embed_batch_size,\n",
    "            flatten_emb=flatten_emb,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            optimizer=None,\n",
    "            **encoder_params,\n",
    "        )\n",
    "        val_losses[epoch] = val_loss\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if early_stop:\n",
    "            # when val loss decrease, reset min loss and counter\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if epoch > min_epoch and counter == tolerance:\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    embed_layer: int,\n",
    "    criterion: nn.Module | None,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    print_every: int = 1000,\n",
    ") -> tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Runs one epoch of testing, returning predictions and labels.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - print_every: int, how often (number of batches) to print avg loss\n",
    "    \n",
    "    Returns: tuple (avg_loss, preds, labels)\n",
    "    - avg_loss: float, average loss per training example \n",
    "    - preds: np.ndarray, shape [num_examples, ...], predictions over dataset\n",
    "    - labels: np.ndarray, shape [num_examples, ...], dataset labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    msg = \"[{step:5d}] loss: {loss:.3f}\"\n",
    "\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    pred_probs = []\n",
    "    pred_classes = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(tqdm(loader)):\n",
    "            # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "            x, y = get_x_y(model, device, batch, embed_layer)\n",
    "\n",
    "            \"\"\"\n",
    "            x = batch[4][embed_layer]\n",
    "            y = batch[0]\n",
    "\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \"\"\"\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x)\n",
    "\n",
    "            # append results\n",
    "            labels.extend(y.detach().cpu().squeeze().numpy())\n",
    "\n",
    "            # append class\n",
    "            if model.model_name == \"LinearClassifier\":\n",
    "                pred_classes.extend(\n",
    "                    outputs.detach()\n",
    "                    .cpu()\n",
    "                    .data.max(1, keepdim=True)[1]\n",
    "                    .squeeze()\n",
    "                    .numpy()\n",
    "                )\n",
    "                pred_probs.append(outputs.detach().cpu().squeeze().numpy())\n",
    "            else:\n",
    "                pred_probs.extend(outputs.detach().cpu().squeeze().numpy())\n",
    "\n",
    "            if criterion is not None:\n",
    "                if model.model_name == \"LinearRegression\":\n",
    "                    loss = criterion(outputs, y)\n",
    "                elif model.model_name == \"LinearClassifier\":\n",
    "                    loss = criterion(outputs, y.squeeze())\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "                if ((i + 1) % print_every == 0) or (i + 1 == len(loader)):\n",
    "                    tqdm.write(msg.format(step=i + 1, loss=cum_loss / len(loader)))\n",
    "\n",
    "    avg_loss = cum_loss / len(loader)\n",
    "    return avg_loss, pred_probs, pred_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for running pytorch models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent import futures\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "\n",
    "from scr.preprocess.data_process import split_protrain_loader, DatasetInfo\n",
    "from scr.encoding.encoding_classes import get_emb_info, ESMEncoder, CARPEncoder\n",
    "from scr.model.pytorch_model import LinearRegression, LinearClassifier\n",
    "# from scr.model.train_test import train, test\n",
    "from scr.vis.learning_vis import plot_lc\n",
    "from scr.utils import get_folder_file_names, pickle_save, get_default_output_path\n",
    "\n",
    "\n",
    "class Run_Pytorch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        loader_batch_size: int = 64,\n",
    "        worker_seed: int = RAND_SEED,\n",
    "        if_encode_all: bool = True,\n",
    "        if_multiprocess: bool = False,\n",
    "        learning_rate: float = 1e-4,\n",
    "        lr_decay: float = 0.1,\n",
    "        epochs: int = 100,\n",
    "        early_stop: bool = True,\n",
    "        tolerance: int = 10,\n",
    "        min_epoch: int = 5,\n",
    "        device: torch.device | str = DEVICE,\n",
    "        all_plot_folder: str = \"results/learning_curves\",\n",
    "        all_result_folder: str = \"results/train_val_test\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running pytorch model\n",
    "\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        \n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "        - worker_seed: int, the seed for dataloader\n",
    "        - learning_rate: float\n",
    "        - lr_decay: float, factor by which to decay LR on plateau\n",
    "        - epochs: int, number of epochs to train for\n",
    "        - device: torch.device or str\n",
    "        - all_plot_folder: str, the parent folder path for saving all the learning curves\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "\n",
    "        Returns:\n",
    "        - result_dict: dict, with the keys and dict values\n",
    "            \"losses\": {\"train_losses\": np.ndarray, \"val_losses\": np.ndarray}\n",
    "            \"train\": {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._dataset_path = dataset_path\n",
    "        self._encoder_name = encoder_name\n",
    "        self._reset_param = reset_param\n",
    "        self._resample_param = resample_param\n",
    "        self._embed_batch_size = embed_batch_size\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._lr_decay = lr_decay\n",
    "        self._epochs = epochs\n",
    "        self._early_stop = early_stop\n",
    "        self._tolerance = tolerance\n",
    "        self._min_epoch = min_epoch\n",
    "        self._device = device\n",
    "        self._all_plot_folder = all_plot_folder\n",
    "        self._all_result_folder = all_result_folder\n",
    "        self._encoder_params = encoder_params\n",
    "\n",
    "        self._ds_info = DatasetInfo(self._dataset_path)\n",
    "        self._model_type = self._ds_info.model_type\n",
    "        self._numb_class = self._ds_info.numb_class\n",
    "\n",
    "        self._train_loader, self._val_loader, self._test_loader = split_protrain_loader(\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            embed_folder=embed_folder,\n",
    "            seq_start_idx=seq_start_idx,\n",
    "            seq_end_idx=seq_end_idx,\n",
    "            subset_list=[\"train\", \"val\", \"test\"],\n",
    "            loader_batch_size=loader_batch_size,\n",
    "            worker_seed=worker_seed,\n",
    "            if_encode_all=if_encode_all,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        encoder_name, encoder_class, total_emb_layer = get_emb_info(encoder_name)\n",
    "\n",
    "        if encoder_class == ESMEncoder:\n",
    "            self._encoder_info_dict = TRANSFORMER_INFO\n",
    "        elif encoder_class == CARPEncoder:\n",
    "            self._encoder_info_dict = CARP_INFO\n",
    "\n",
    "        if if_multiprocess:\n",
    "            print(\"Running different emb layer in parallel...\")\n",
    "            # add the thredpool max_workers=None\n",
    "            with futures.ProcessPoolExecutor(max_workers=os.cpu_count() - 1) as pool:\n",
    "                # for each layer train the model and save the model\n",
    "                for embed_layer in tqdm(range(total_emb_layer)):\n",
    "                    pool.submit(self.run_pytorch_layer, embed_layer)\n",
    "\n",
    "        else:\n",
    "            for embed_layer in range(total_emb_layer):\n",
    "                print(f\"Running pytorch model for layer {embed_layer}\")\n",
    "                self.run_pytorch_layer(embed_layer)\n",
    "\n",
    "    def run_pytorch_layer(self, embed_layer):\n",
    "\n",
    "        # init model based on datasets\n",
    "        if self._model_type == \"LinearRegression\":\n",
    "            model = LinearRegression(\n",
    "                input_dim=self._encoder_info_dict[self._encoder_name][0], output_dim=1\n",
    "            )\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "        elif self._model_type == \"LinearClassifier\":\n",
    "            model = LinearClassifier(\n",
    "                input_dim=self._encoder_info_dict[self._encoder_name][0], \n",
    "                numb_class=self._numb_class\n",
    "            )\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        model.to(self._device, non_blocking=True)\n",
    "\n",
    "        criterion.to(self._device, non_blocking=True)\n",
    "\n",
    "        train_losses, val_losses = train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=self._train_loader,\n",
    "            val_loader=self._val_loader,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            device=self._device,\n",
    "            learning_rate=self._learning_rate,\n",
    "            lr_decay=self._lr_decay,\n",
    "            epochs=self._epochs,\n",
    "            early_stop=self._early_stop,\n",
    "            tolerance=self._tolerance,\n",
    "            min_epoch=self._min_epoch,\n",
    "            **self._encoder_params,\n",
    "        )\n",
    "\n",
    "        # record the losses\n",
    "        result_dict = {\n",
    "            \"losses\": {\"train_losses\": train_losses, \"val_losses\": val_losses}\n",
    "        }\n",
    "\n",
    "        plot_lc(\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            all_plot_folder=self._all_plot_folder,\n",
    "        )\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, loader in zip(\n",
    "            [\"train\", \"val\", \"test\"],\n",
    "            [self._train_loader, self._val_loader, self._test_loader],\n",
    "        ):\n",
    "            loss, pred, cls, true = test(\n",
    "                model=model, loader=loader, embed_layer=embed_layer, device=self._device, criterion=criterion\n",
    "            )\n",
    "\n",
    "            if model.model_name == \"LinearRegression\":\n",
    "                result_dict[subset] = {\n",
    "                    \"mse\": loss,\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"ndcg\": ndcg_score(true[None, :], pred[None, :]),\n",
    "                    \"rho\": spearmanr(true, pred),\n",
    "                }\n",
    "\n",
    "            elif model.model_name == \"LinearClassifier\":\n",
    "                result_dict[subset] = {\n",
    "                    \"cross-entropy\": loss,\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"acc\": accuracy_score(true, cls),\n",
    "                    \"rocauc\": roc_auc_score(true, pred, multi_class=\"ovo\"),\n",
    "                }            \n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=get_default_output_path(self._all_result_folder),\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting classes into int...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting classes into int...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting classes into int...\n",
      "Running pytorch model for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:15<00:00, 37.59s/it]\n",
      "  0%|          | 0/2376 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making results/learning_curves/annotation ...\n",
      "Making results/learning_curves/annotation/scl ...\n",
      "Making results/learning_curves/annotation/scl/balanced ...\n",
      "Making results/learning_curves/annotation/scl/balanced/esm1_t6_43M_UR50S ...\n",
      "Making results/learning_curves/annotation/scl/balanced/esm1_t6_43M_UR50S/mean ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=0'>1</a>\u001b[0m Run_Pytorch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=1'>2</a>\u001b[0m         dataset_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/annotation/scl/balanced.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=2'>3</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mesm1_t6_43M_UR50S\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=3'>4</a>\u001b[0m         reset_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=4'>5</a>\u001b[0m         resample_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=5'>6</a>\u001b[0m         embed_batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=6'>7</a>\u001b[0m         flatten_emb \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=7'>8</a>\u001b[0m         embed_folder\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39membeddings/annotation/scl/balanced\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=8'>9</a>\u001b[0m         seq_start_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=9'>10</a>\u001b[0m         seq_end_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=10'>11</a>\u001b[0m         loader_batch_size \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=11'>12</a>\u001b[0m         worker_seed\u001b[39m=\u001b[39;49m RAND_SEED,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=12'>13</a>\u001b[0m         if_encode_all \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=13'>14</a>\u001b[0m         learning_rate \u001b[39m=\u001b[39;49m \u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=14'>15</a>\u001b[0m         lr_decay \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=15'>16</a>\u001b[0m         epochs \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=16'>17</a>\u001b[0m         early_stop \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=17'>18</a>\u001b[0m         tolerance \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=18'>19</a>\u001b[0m         min_epoch \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=19'>20</a>\u001b[0m         device \u001b[39m=\u001b[39;49m DEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=20'>21</a>\u001b[0m         all_plot_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mresults/learning_curves\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=21'>22</a>\u001b[0m         all_result_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mresults/train_val_test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=22'>23</a>\u001b[0m         \u001b[39m# **encoder_params\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=23'>24</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 13\u001b[0m in \u001b[0;36mRun_Pytorch.__init__\u001b[0;34m(self, dataset_path, encoder_name, reset_param, resample_param, embed_batch_size, flatten_emb, embed_folder, seq_start_idx, seq_end_idx, loader_batch_size, worker_seed, if_encode_all, if_multiprocess, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, device, all_plot_folder, all_result_folder, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=152'>153</a>\u001b[0m \u001b[39mfor\u001b[39;00m embed_layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_emb_layer):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=153'>154</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning pytorch model for layer \u001b[39m\u001b[39m{\u001b[39;00membed_layer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=154'>155</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_pytorch_layer(embed_layer)\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 13\u001b[0m in \u001b[0;36mRun_Pytorch.run_pytorch_layer\u001b[0;34m(self, embed_layer)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=212'>213</a>\u001b[0m \u001b[39m# now test the model with the test data\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=213'>214</a>\u001b[0m \u001b[39mfor\u001b[39;00m subset, loader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=214'>215</a>\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=215'>216</a>\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_loader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_val_loader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_loader],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=216'>217</a>\u001b[0m ):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=217'>218</a>\u001b[0m     loss, pred, \u001b[39mcls\u001b[39m, true \u001b[39m=\u001b[39m test(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=218'>219</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, loader\u001b[39m=\u001b[39;49mloader, embed_layer\u001b[39m=\u001b[39;49membed_layer, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device, criterion\u001b[39m=\u001b[39;49mcriterion\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=219'>220</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=221'>222</a>\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mmodel_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLinearRegression\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=222'>223</a>\u001b[0m         result_dict[subset] \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=223'>224</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m: loss,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=224'>225</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m\"\u001b[39m: pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=227'>228</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrho\u001b[39m\u001b[39m\"\u001b[39m: spearmanr(true, pred),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=228'>229</a>\u001b[0m         }\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 13\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, loader, embed_layer, criterion, device, print_every)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=320'>321</a>\u001b[0m     pred_probs\u001b[39m.\u001b[39mextend(outputs\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=322'>323</a>\u001b[0m \u001b[39mif\u001b[39;00m criterion \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=323'>324</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, y)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=324'>325</a>\u001b[0m     cum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=326'>327</a>\u001b[0m     \u001b[39mif\u001b[39;00m ((i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(loader)):\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/modules/loss.py:1047\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   1046\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, Tensor)\n\u001b[0;32m-> 1047\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1048\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/functional.py:2690\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2688\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2689\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2690\u001b[0m \u001b[39mreturn\u001b[39;00m nll_loss(log_softmax(\u001b[39minput\u001b[39;49m, \u001b[39m1\u001b[39;49m), target, weight, \u001b[39mNone\u001b[39;49;00m, ignore_index, \u001b[39mNone\u001b[39;49;00m, reduction)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/functional.py:2385\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2381\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2382\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected input batch_size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) to match target batch_size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), target\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m   2383\u001b[0m     )\n\u001b[1;32m   2384\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 2385\u001b[0m     ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n\u001b[1;32m   2386\u001b[0m \u001b[39melif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m   2387\u001b[0m     ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mnll_loss2d(\u001b[39minput\u001b[39m, target, weight, _Reduction\u001b[39m.\u001b[39mget_enum(reduction), ignore_index)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "Run_Pytorch(\n",
    "        dataset_path=\"data/annotation/scl/balanced.csv\",\n",
    "        encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "        reset_param = False,\n",
    "        resample_param = False,\n",
    "        embed_batch_size = 128,\n",
    "        flatten_emb = \"mean\",\n",
    "        embed_folder= \"embeddings/annotation/scl/balanced\",\n",
    "        seq_start_idx = False,\n",
    "        seq_end_idx = False,\n",
    "        loader_batch_size = 4,\n",
    "        worker_seed= RAND_SEED,\n",
    "        if_encode_all = False,\n",
    "        learning_rate = 1e-4,\n",
    "        lr_decay = 0.1,\n",
    "        epochs = 2,\n",
    "        early_stop = True,\n",
    "        tolerance = 10,\n",
    "        min_epoch = 5,\n",
    "        device = DEVICE,\n",
    "        all_plot_folder = \"results/learning_curves\",\n",
    "        all_result_folder = \"results/train_val_test\",\n",
    "        # **encoder_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch model for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=0'>1</a>\u001b[0m Run_Pytorch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=1'>2</a>\u001b[0m         dataset_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/proeng/gb1/low_vs_high.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=2'>3</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mesm1_t6_43M_UR50S\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=3'>4</a>\u001b[0m         reset_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=4'>5</a>\u001b[0m         resample_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=5'>6</a>\u001b[0m         embed_batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=6'>7</a>\u001b[0m         flatten_emb \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=7'>8</a>\u001b[0m         embed_folder\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39membeddings/proeng/gb1/low_vs_high\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=8'>9</a>\u001b[0m         seq_start_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=9'>10</a>\u001b[0m         seq_end_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=10'>11</a>\u001b[0m         loader_batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=11'>12</a>\u001b[0m         worker_seed\u001b[39m=\u001b[39;49m RAND_SEED,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=12'>13</a>\u001b[0m         if_encode_all \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=13'>14</a>\u001b[0m         learning_rate \u001b[39m=\u001b[39;49m \u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=14'>15</a>\u001b[0m         lr_decay \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=15'>16</a>\u001b[0m         epochs \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=16'>17</a>\u001b[0m         early_stop \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=17'>18</a>\u001b[0m         tolerance \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=18'>19</a>\u001b[0m         min_epoch \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=19'>20</a>\u001b[0m         device \u001b[39m=\u001b[39;49m DEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=20'>21</a>\u001b[0m         all_plot_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest/learning_curves\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=21'>22</a>\u001b[0m         all_result_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest/train_val_test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=22'>23</a>\u001b[0m         \u001b[39m# **encoder_params\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=23'>24</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 15\u001b[0m in \u001b[0;36mRun_Pytorch.__init__\u001b[0;34m(self, dataset_path, encoder_name, reset_param, resample_param, embed_batch_size, flatten_emb, embed_folder, seq_start_idx, seq_end_idx, loader_batch_size, worker_seed, if_encode_all, if_multiprocess, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, device, all_plot_folder, all_result_folder, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=152'>153</a>\u001b[0m \u001b[39mfor\u001b[39;00m embed_layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_emb_layer):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=153'>154</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning pytorch model for layer \u001b[39m\u001b[39m{\u001b[39;00membed_layer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=154'>155</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_pytorch_layer(embed_layer)\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 15\u001b[0m in \u001b[0;36mRun_Pytorch.run_pytorch_layer\u001b[0;34m(self, embed_layer)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=172'>173</a>\u001b[0m model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=174'>175</a>\u001b[0m criterion\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=176'>177</a>\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m train(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=177'>178</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=178'>179</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=179'>180</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=180'>181</a>\u001b[0m     val_loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_val_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=181'>182</a>\u001b[0m     encoder_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=182'>183</a>\u001b[0m     embed_layer\u001b[39m=\u001b[39;49membed_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=183'>184</a>\u001b[0m     reset_param\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=184'>185</a>\u001b[0m     resample_param\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resample_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=185'>186</a>\u001b[0m     embed_batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embed_batch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=186'>187</a>\u001b[0m     flatten_emb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flatten_emb,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=187'>188</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=188'>189</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learning_rate,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=189'>190</a>\u001b[0m     lr_decay\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lr_decay,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=190'>191</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_epochs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=191'>192</a>\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_early_stop,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=192'>193</a>\u001b[0m     tolerance\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tolerance,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=193'>194</a>\u001b[0m     min_epoch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_min_epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=194'>195</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=195'>196</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=197'>198</a>\u001b[0m \u001b[39m# record the losses\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=198'>199</a>\u001b[0m result_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=199'>200</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlosses\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mtrain_losses\u001b[39m\u001b[39m\"\u001b[39m: train_losses, \u001b[39m\"\u001b[39m\u001b[39mval_losses\u001b[39m\u001b[39m\"\u001b[39m: val_losses}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=200'>201</a>\u001b[0m }\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, train_loader, val_loader, encoder_name, embed_layer, reset_param, resample_param, embed_batch_size, flatten_emb, device, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=207'>208</a>\u001b[0m min_val_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mInf\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=209'>210</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=211'>212</a>\u001b[0m     train_losses[epoch] \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=212'>213</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=213'>214</a>\u001b[0m         loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=214'>215</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49mencoder_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=215'>216</a>\u001b[0m         embed_layer\u001b[39m=\u001b[39;49membed_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=216'>217</a>\u001b[0m         reset_param\u001b[39m=\u001b[39;49mreset_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=217'>218</a>\u001b[0m         resample_param\u001b[39m=\u001b[39;49mresample_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=218'>219</a>\u001b[0m         embed_batch_size\u001b[39m=\u001b[39;49membed_batch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=219'>220</a>\u001b[0m         flatten_emb\u001b[39m=\u001b[39;49mflatten_emb,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=220'>221</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=221'>222</a>\u001b[0m         criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=222'>223</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=223'>224</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=224'>225</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=226'>227</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=227'>228</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=228'>229</a>\u001b[0m         loader\u001b[39m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=238'>239</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=239'>240</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=240'>241</a>\u001b[0m     val_losses[epoch] \u001b[39m=\u001b[39m val_loss\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 15\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, encoder_name, embed_layer, reset_param, resample_param, embed_batch_size, flatten_emb, device, criterion, optimizer, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=145'>146</a>\u001b[0m \u001b[39melif\u001b[39;00m criterion \u001b[39m==\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss():\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=146'>147</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, y\u001b[39m.\u001b[39msqueeze())\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=147'>148</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=148'>149</a>\u001b[0m \u001b[39mif\u001b[39;00m optimizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000041vscode-remote?line=149'>150</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "Run_Pytorch(\n",
    "        dataset_path=\"data/proeng/gb1/low_vs_high.csv\",\n",
    "        encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "        reset_param = False,\n",
    "        resample_param = False,\n",
    "        embed_batch_size = 128,\n",
    "        flatten_emb = \"mean\",\n",
    "        embed_folder= \"embeddings/proeng/gb1/low_vs_high\",\n",
    "        seq_start_idx = False,\n",
    "        seq_end_idx = False,\n",
    "        loader_batch_size = 64,\n",
    "        worker_seed= RAND_SEED,\n",
    "        if_encode_all = False,\n",
    "        learning_rate = 1e-4,\n",
    "        lr_decay = 0.1,\n",
    "        epochs = 2,\n",
    "        early_stop = True,\n",
    "        tolerance = 10,\n",
    "        min_epoch = 5,\n",
    "        device = DEVICE,\n",
    "        all_plot_folder = \"test/learning_curves\",\n",
    "        all_result_folder = \"test/train_val_test\",\n",
    "        # **encoder_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ss3/ss8 into np.array...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ss3/ss8 into np.array...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ss3/ss8 into np.array...\n",
      "Running pytorch model for layer 0\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=0'>1</a>\u001b[0m Run_Pytorch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=1'>2</a>\u001b[0m         dataset_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/structure/secondary_structure/casp12.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=2'>3</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mesm1_t6_43M_UR50S\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=3'>4</a>\u001b[0m         reset_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=4'>5</a>\u001b[0m         resample_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=5'>6</a>\u001b[0m         embed_batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=6'>7</a>\u001b[0m         flatten_emb \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=7'>8</a>\u001b[0m         \u001b[39m# embed_folder= \"embeddings/annotation/scl/balanced\",\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=8'>9</a>\u001b[0m         seq_start_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=9'>10</a>\u001b[0m         seq_end_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=10'>11</a>\u001b[0m         loader_batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=11'>12</a>\u001b[0m         worker_seed\u001b[39m=\u001b[39;49m RAND_SEED,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=12'>13</a>\u001b[0m         if_encode_all \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=13'>14</a>\u001b[0m         learning_rate \u001b[39m=\u001b[39;49m \u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=14'>15</a>\u001b[0m         lr_decay \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=15'>16</a>\u001b[0m         epochs \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=16'>17</a>\u001b[0m         early_stop \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=17'>18</a>\u001b[0m         tolerance \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=18'>19</a>\u001b[0m         min_epoch \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=19'>20</a>\u001b[0m         device \u001b[39m=\u001b[39;49m DEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=20'>21</a>\u001b[0m         all_plot_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mresults/learning_curves\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=21'>22</a>\u001b[0m         all_result_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mresults/train_val_test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=22'>23</a>\u001b[0m         \u001b[39m# **encoder_params\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=23'>24</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 19\u001b[0m in \u001b[0;36mRun_Pytorch.__init__\u001b[0;34m(self, dataset_path, encoder_name, reset_param, resample_param, embed_batch_size, flatten_emb, embed_folder, seq_start_idx, seq_end_idx, loader_batch_size, worker_seed, if_encode_all, if_multiprocess, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, device, all_plot_folder, all_result_folder, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=153'>154</a>\u001b[0m \u001b[39mfor\u001b[39;00m embed_layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_emb_layer):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=154'>155</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning pytorch model for layer \u001b[39m\u001b[39m{\u001b[39;00membed_layer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=155'>156</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_pytorch_layer(embed_layer)\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 19\u001b[0m in \u001b[0;36mRun_Pytorch.run_pytorch_layer\u001b[0;34m(self, embed_layer)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=167'>168</a>\u001b[0m     model \u001b[39m=\u001b[39m LinearClassifier(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=168'>169</a>\u001b[0m         input_dim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoder_info_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoder_name][\u001b[39m0\u001b[39m], \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=169'>170</a>\u001b[0m         numb_class\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numb_class\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=170'>171</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=171'>172</a>\u001b[0m     criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=173'>174</a>\u001b[0m model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=175'>176</a>\u001b[0m criterion\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=177'>178</a>\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m train(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=178'>179</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=179'>180</a>\u001b[0m     criterion\u001b[39m=\u001b[39mcriterion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=195'>196</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000040vscode-remote?line=196'>197</a>\u001b[0m )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "Run_Pytorch(\n",
    "        dataset_path=\"data/structure/secondary_structure/casp12.csv\",\n",
    "        encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "        reset_param = False,\n",
    "        resample_param = False,\n",
    "        embed_batch_size = 128,\n",
    "        flatten_emb = \"mean\",\n",
    "        # embed_folder= \"embeddings/annotation/scl/balanced\",\n",
    "        seq_start_idx = False,\n",
    "        seq_end_idx = False,\n",
    "        loader_batch_size = 64,\n",
    "        worker_seed= RAND_SEED,\n",
    "        if_encode_all = False,\n",
    "        learning_rate = 1e-4,\n",
    "        lr_decay = 0.1,\n",
    "        epochs = 2,\n",
    "        early_stop = True,\n",
    "        tolerance = 10,\n",
    "        min_epoch = 5,\n",
    "        device = DEVICE,\n",
    "        all_plot_folder = \"results/learning_curves\",\n",
    "        all_result_folder = \"results/train_val_test\",\n",
    "        # **encoder_params\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
