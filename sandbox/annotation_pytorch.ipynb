{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/structure/secondary_structure/casp12.csv\").target.dtype.kind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/structure/secondary_structure/casp12.csv\").target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEVLEEPAPGPGGADAAERRGLRRLLLSGFQEELRALLVLAGPAFL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFED...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAKRTFSNLETFLIFLLVMMSAITVALLSLLFITSGTIENHKDLGG...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MGNCQAGHNLHLCLAHHPPLVCATLILLLLGLSGLGLGSFLLTHRT...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDPSKQGTLNRVENSVYRTAFKLRSVQTLCQLDLMDSFLIQQVLWR...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence         target    set  \\\n",
       "0  MEVLEEPAPGPGGADAAERRGLRRLLLSGFQEELRALLVLAGPAFL...  Cell membrane  train   \n",
       "1  MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFED...  Cell membrane  train   \n",
       "2  MAKRTFSNLETFLIFLLVMMSAITVALLSLLFITSGTIENHKDLGG...  Cell membrane  train   \n",
       "3  MGNCQAGHNLHLCLAHHPPLVCATLILLLLGLSGLGLGSFLLTHRT...  Cell membrane  train   \n",
       "4  MDPSKQGTLNRVENSVYRTAFKLRSVQTLCQLDLMDSFLIQQVLWR...  Cell membrane  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/annotation/scl/balanced.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([nan, True], dtype=object), array(['train', 'test'], dtype=object))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.validation.unique(), df.set.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cell membrane', 'Cytoplasm', 'Endoplasmic reticulum',\n",
       "       'Golgi apparatus', 'Lysosome/Vacuole', 'Mitochondrion', 'Nucleus',\n",
       "       'Peroxisome', 'Plastid', 'Extracellular'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"data/structure/secondary_structure/casp12.csv\")\n",
    "len(np.unique(np.array(df2[\"target\"][0][1:-1].split(\", \"))))\n",
    "# df2[\"target\"] = df2[\"target\"].apply(lambda x: np.array(x[1:-1].split(\", \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetInfo:\n",
    "    \"\"\"\n",
    "    A class returns the information of a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, the path for the csv\n",
    "        \"\"\"\n",
    "        self._df = pd.read_csv(dataset_path)\n",
    "\n",
    "    def get_model_type(self) -> str:\n",
    "        # pick linear regression if y numerical\n",
    "        if self._df.target.dtype.kind in \"iufc\":\n",
    "            return \"LinearRegression\"\n",
    "        else:\n",
    "            # ss3\n",
    "            if \"[\" in self._df.target[0]:\n",
    "                return \"MultiLabelMultiClass\"\n",
    "            # annotation\n",
    "            else:\n",
    "                return \"LinearClassifier\"\n",
    "\n",
    "    def get_numb_class(self) -> int:\n",
    "        \"\"\"\n",
    "        A function to get number of class\n",
    "        \"\"\"\n",
    "        if self.model_type == \"LinearClassifier\":\n",
    "            return self._df.target.nunique()\n",
    "        elif self.model_type == \"MultiLabelMultiClass\":\n",
    "            return len(np.unique(np.array(self._df[\"target\"][0][1:-1].split(\", \")))) + 1\n",
    "\n",
    "    @property\n",
    "    def model_type(self):\n",
    "        \"\"\"Return the pytorch model type\"\"\"\n",
    "        return self.get_model_type()\n",
    "\n",
    "    @property\n",
    "    def numb_class(self):\n",
    "        \"\"\"Return number of classes for classification\"\"\"\n",
    "        return self.get_numb_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"[\" in df2[\"target\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LinearClassifier', 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetInfo(\"data/annotation/scl/balanced.csv\").model_type, DatasetInfo(\"data/annotation/scl/balanced.csv\").numb_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MultiLabelMultiClass', 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetInfo(\"data/structure/secondary_structure/casp12.csv\").model_type, DatasetInfo(\"data/structure/secondary_structure/casp12.csv\").numb_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-processing the dataset\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Sequence, defaultdict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scr.utils import pickle_save, pickle_load, replace_ext\n",
    "from scr.params.sys import RAND_SEED\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO, MAX_SEQ_LEN\n",
    "from scr.preprocess.seq_loader import SeqLoader\n",
    "from scr.encoding.encoding_classes import (\n",
    "    AbstractEncoder,\n",
    "    ESMEncoder,\n",
    "    CARPEncoder,\n",
    "    OnehotEncoder,\n",
    ")\n",
    "\n",
    "\n",
    "def get_mut_name(mut_seq: str, parent_seq: str) -> str:\n",
    "    \"\"\"\n",
    "    A function for returning the mutant name\n",
    "\n",
    "    Args:\n",
    "    - mut_seq: str, the full mutant sequence\n",
    "    - parent_seq: str, the full parent sequence\n",
    "\n",
    "    Returns:\n",
    "    - str, parent, indel, or mutant name in the format of\n",
    "        ParentAAMutLocMutAA:ParentAAMutLocMutAA:..., ie. W39W:D40G:G41C:V54Q\n",
    "    \"\"\"\n",
    "\n",
    "    mut_list = []\n",
    "    if parent_seq == mut_seq:\n",
    "        return \"parent\"\n",
    "    elif len(parent_seq) == len(mut_seq):\n",
    "        for i, (p, m) in enumerate(zip(list(parent_seq), list(mut_seq))):\n",
    "            if p != m:\n",
    "                mut_list.append(f\"{p}{i+1}{m}\")\n",
    "        return \":\".join(mut_list)\n",
    "    else:\n",
    "        return \"indel\"\n",
    "\n",
    "\n",
    "class AddMutInfo:\n",
    "    \"\"\"A class for appending mutation info for mainly protein engineering tasks\"\"\"\n",
    "\n",
    "    def __init__(self, parent_seq_path: str, csv_path: str):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - parent_seq_path: str, path for the parent sequence\n",
    "        - csv_path: str, path for the fitness csv file\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the parent sequence from the fasta file\n",
    "        self._parent_seq = SeqLoader(parent_seq_path=parent_seq_path)\n",
    "\n",
    "        # load the dataframe\n",
    "        self._init_df = pd.read_csv(csv_path)\n",
    "\n",
    "        self._df = self._init_df.copy()\n",
    "        # add a column with the mutant names\n",
    "        self._df[\"mut_name\"] = self._init_df[\"sequence\"].apply(\n",
    "            get_mut_name, parent_seq=self._parent_seq\n",
    "        )\n",
    "        # add a column with the number of mutations\n",
    "        self._df[\"mut_numb\"] = (\n",
    "            self._df[\"mut_name\"].str.split(\":\").map(len, na_action=\"ignore\")\n",
    "        )\n",
    "\n",
    "        # get the pickle file path\n",
    "        self._pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "\n",
    "        pickle_save(what2save=self._df, where2save=self._pkl_path)\n",
    "\n",
    "    @property\n",
    "    def parent_seq(self) -> str:\n",
    "        \"\"\"Return the parent sequence\"\"\"\n",
    "        return self._parent_seq\n",
    "\n",
    "    @property\n",
    "    def pkl_path(self) -> str:\n",
    "        \"\"\"Return the pkl file path for the processed dataframe\"\"\"\n",
    "        return self._pkl_path\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the processed dataframe\"\"\"\n",
    "        return self._df\n",
    "\n",
    "\n",
    "def std_ssdf(\n",
    "    ssdf_path: str = \"data/structure/secondary_structure/tape_ss3.csv\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A function that standardize secondary structure dataset\n",
    "    to set up as columns as sequence, target, set, validation\n",
    "    where set is train or test and add validation as true\n",
    "    \"\"\"\n",
    "    folder_path = os.path.dirname(ssdf_path)\n",
    "\n",
    "    df = pd.read_csv(ssdf_path)\n",
    "\n",
    "    # convert the string into numpy array\n",
    "    # df[\"ss3\"] = df[\"ss3\"].apply(lambda x: np.array(x[1:-1].split(\", \")))\n",
    "\n",
    "    # add validation column\n",
    "    df[\"validation\"] = df[\"split\"].apply(lambda x: True if x == \"valid\" else \"\")\n",
    "    # now replace valid to train\n",
    "    df = df.replace(\"valid\", \"train\")\n",
    "    # rename all columns\n",
    "    df.columns = [\"sequence\", \"target\", \"set\", \"validation\"]\n",
    "    \n",
    "    # get all kinds of test sets\n",
    "    ss_tests = set(df[\"set\"].unique()) - set([\"train\"])\n",
    "\n",
    "    for ss_test in ss_tests:\n",
    "        df.loc[~df[\"set\"].isin(set(ss_tests) - set([ss_test]))].replace(\n",
    "            ss_test, \"test\"\n",
    "        ).to_csv(os.path.join(folder_path, ss_test + \".csv\"), index=False)\n",
    "\n",
    "\n",
    "class DatasetInfo:\n",
    "    \"\"\"\n",
    "    A class returns the information of a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, the path for the csv\n",
    "        \"\"\"\n",
    "        self._df = pd.read_csv(dataset_path)\n",
    "\n",
    "    def get_model_type(self) -> str:\n",
    "        # pick linear regression if y numerical\n",
    "        if self._df.target.dtype.kind in \"iufc\":\n",
    "            return \"LinearRegression\"\n",
    "        else:\n",
    "            # ss3\n",
    "            if \"[\" in self._df.target[0]:\n",
    "                return \"MultiLabelMultiClass\"\n",
    "            # annotation\n",
    "            else:\n",
    "                return \"LinearClassifier\"\n",
    "\n",
    "    def get_numb_class(self) -> int:\n",
    "        \"\"\"\n",
    "        A function to get number of class\n",
    "        \"\"\"\n",
    "        # annotation class number\n",
    "        if self.model_type == \"LinearClassifier\":\n",
    "            return self._df.target.nunique()\n",
    "        # ss3 or ss8 secondary structure states plus padding\n",
    "        elif self.model_type == \"MultiLabelMultiClass\":\n",
    "            return len(np.unique(np.array(self._df[\"target\"][0][1:-1].split(\", \")))) + 1\n",
    "\n",
    "    @property\n",
    "    def model_type(self) -> str:\n",
    "        \"\"\"Return the pytorch model type\"\"\"\n",
    "        return self.get_model_type()\n",
    "\n",
    "    @property\n",
    "    def numb_class(self) -> int:\n",
    "        \"\"\"Return number of classes for classification\"\"\"\n",
    "        return self.get_numb_class()\n",
    "\n",
    "\n",
    "class TaskProcess:\n",
    "    \"\"\"A class for handling different downstream tasks\"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_folder: str, a folder path with all the tasks as subfolders where\n",
    "            all the subfolders have datasets as the subsubfolders, ie\n",
    "\n",
    "            {data_folder}/\n",
    "                proeng/\n",
    "                    aav/\n",
    "                        one_vs_many.csv\n",
    "                        two_vs_many.csv\n",
    "                        P03135.fasta\n",
    "                    thermo/\n",
    "                        mixed.csv\n",
    "        \"\"\"\n",
    "\n",
    "        if data_folder[-1] == \"/\":\n",
    "            self._data_folder = data_folder\n",
    "        else:\n",
    "            self._data_folder = data_folder + \"/\"\n",
    "\n",
    "        # sumamarize all files i nthe data folder\n",
    "        self._sum_file_df = self.sum_files()\n",
    "\n",
    "    def sum_files(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Summarize all files in the data folder\n",
    "\n",
    "        Returns:\n",
    "        - A dataframe with \"task\", \"dataset\", \"split\",\n",
    "            \"csv_path\", \"fasta_path\", \"pkl_path\" as columns, ie.\n",
    "            (proeng, gb1, low_vs_high, data/proeng/gb1/low_vs_high.csv,\n",
    "            data/proeng/gb1/5LDE_1.fasta)\n",
    "            note that csv_path is the list of lmdb files for the structure task\n",
    "        \"\"\"\n",
    "        dataset_folders = glob(f\"{self._data_folder}*/*\")\n",
    "        # need a list of tuples in the order of:\n",
    "        # (task, dataset, split, csv_path, fasta_path)\n",
    "        list_for_df = []\n",
    "        for dataset_folder in dataset_folders:\n",
    "            _, task, dataset = dataset_folder.split(\"/\")\n",
    "            if task == \"structure\":\n",
    "                structure_file_list = [\n",
    "                    file_path\n",
    "                    for file_path in glob(f\"{dataset_folder}/*.*\")\n",
    "                    if os.path.basename(os.path.splitext(file_path)[0]).split(\"_\")[-1]\n",
    "                    in [\"train\", \"valid\", \"cb513\"]\n",
    "                ]\n",
    "                list_for_df.append(\n",
    "                    tuple([task, dataset, \"cb513\", structure_file_list, \"\", \"\"])\n",
    "                )\n",
    "            else:\n",
    "                csv_paths = glob(f\"{dataset_folder}/*.csv\")\n",
    "                fasta_paths = glob(f\"{dataset_folder}/*.fasta\")\n",
    "                pkl_paths = glob(f\"{dataset_folder}/*.pkl\")\n",
    "\n",
    "                assert len(csv_paths) >= 1, \"Less than one csv\"\n",
    "                assert len(fasta_paths) <= 1, \"More than one fasta\"\n",
    "\n",
    "                for csv_path in csv_paths:\n",
    "                    # if parent seq fasta exists\n",
    "                    if len(fasta_paths) == 1:\n",
    "                        fasta_path = fasta_paths[0]\n",
    "\n",
    "                        # if no existing pkl file, generate and save\n",
    "                        if len(pkl_paths) == 0:\n",
    "                            print(f\"Adding mutation info to {csv_path}...\")\n",
    "                            pkl_path = AddMutInfo(\n",
    "                                parent_seq_path=fasta_path, csv_path=csv_path\n",
    "                            ).pkl_path\n",
    "                        # pkl file exits\n",
    "                        else:\n",
    "                            pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "                    # no parent fasta no pkl file\n",
    "                    else:\n",
    "                        fasta_path = \"\"\n",
    "                        pkl_path = \"\"\n",
    "\n",
    "                    list_for_df.append(\n",
    "                        tuple(\n",
    "                            [\n",
    "                                task,\n",
    "                                dataset,\n",
    "                                os.path.basename(os.path.splitext(csv_path)[0]),\n",
    "                                csv_path,\n",
    "                                fasta_path,\n",
    "                                pkl_path,\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            list_for_df,\n",
    "            columns=[\"task\", \"dataset\", \"split\", \"csv_path\", \"fasta_path\", \"pkl_path\"],\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def sum_file_df(self) -> pd.DataFrame:\n",
    "        \"\"\"A summary table for all files in the data folder\"\"\"\n",
    "        return self._sum_file_df\n",
    "\n",
    "\n",
    "class ProtranDataset(Dataset):\n",
    "\n",
    "    \"\"\"A dataset class for processing protein transfer data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        subset: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        if_encode_all: bool = True,\n",
    "        **encoder_params,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format, ie\n",
    "            \"data/proeng/gb1/low_vs_high.csv\"\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - subset: str, train, val, test\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding folder, ie\n",
    "            \"embeddings/proeng/gb1/low_vs_high\"\n",
    "            for which then can add the subset to be, ie\n",
    "            \"embeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/test/embedding.h5\"\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - if_encode_all: bool = True, if encode full dataset all layers on the fly\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        # with additional info mut_name, mut_numb\n",
    "        if os.path.splitext(dataset_path)[-1] in [\".pkl\", \".PKL\", \"\"]:\n",
    "            self._df = pickle_load(dataset_path)\n",
    "            self._add_mut_info = True\n",
    "        # without such info\n",
    "        else:\n",
    "            self._df = pd.read_csv(dataset_path)\n",
    "            self._ds_info = DatasetInfo(dataset_path)\n",
    "            self._model_type = self._ds_info.model_type\n",
    "            self._numb_class = self._ds_info.numb_class\n",
    "\n",
    "            self._add_mut_info = False\n",
    "\n",
    "        assert \"set\" in self._df.columns, f\"set is not a column in {dataset_path}\"\n",
    "        assert (\n",
    "            \"validation\" in self._df.columns\n",
    "        ), f\"validation is not a column in {dataset_path}\"\n",
    "\n",
    "        self._df_train = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] != True)\n",
    "        ]\n",
    "        self._df_val = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] == True)\n",
    "        ]\n",
    "        self._df_test = self._df.loc[(self._df[\"set\"] == \"test\")]\n",
    "\n",
    "        self._df_dict = {\n",
    "            \"train\": self._df_train,\n",
    "            \"val\": self._df_val,\n",
    "            \"test\": self._df_test,\n",
    "        }\n",
    "\n",
    "        assert subset in list(\n",
    "            self._df_dict.keys()\n",
    "        ), \"split can only be 'train', 'val', or 'test'\"\n",
    "        self._subset = subset\n",
    "\n",
    "        self._subdf_len = len(self._df_dict[self._subset])\n",
    "\n",
    "        # not specified seq start will be from 0\n",
    "        if seq_start_idx == False:\n",
    "            self._seq_start_idx = 0\n",
    "        else:\n",
    "            self._seq_start_idx = int(seq_start_idx)\n",
    "        # not specified seq end will be the full sequence length\n",
    "        if seq_end_idx == False:\n",
    "            self._seq_end_idx = -1\n",
    "            self._max_seq_len = self._df.sequence.str.len().max()\n",
    "        else:\n",
    "            self._seq_end_idx = int(seq_end_idx)\n",
    "            self._max_seq_len = self._seq_end_idx - self._seq_start_idx\n",
    "\n",
    "        # get unencoded string of input sequence\n",
    "        # will need to convert data type\n",
    "        self.sequence = self._get_column_value(\"sequence\")\n",
    "\n",
    "        self.if_encode_all = if_encode_all\n",
    "        self._embed_folder = embed_folder\n",
    "\n",
    "        self._encoder_name = encoder_name\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        # get the encoder class\n",
    "        if self._encoder_name in TRANSFORMER_INFO.keys():\n",
    "            encoder_class = ESMEncoder\n",
    "        elif self._encoder_name in CARP_INFO.keys():\n",
    "            encoder_class = CARPEncoder\n",
    "        else:\n",
    "            encoder_class = OnehotEncoder\n",
    "            encoder_params[\"max_seq_len\"] = self._max_seq_len\n",
    "\n",
    "        # get the encoder\n",
    "        self._encoder = encoder_class(\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            **encoder_params,\n",
    "        )\n",
    "        self._total_emb_layer = self._encoder.total_emb_layer\n",
    "\n",
    "        # encode all and load in memory\n",
    "        if self.if_encode_all and self._embed_folder is None:\n",
    "            # encode the sequences without the mut_name\n",
    "            # init an empty dict with empty list to append emb\n",
    "            encoded_dict = defaultdict(list)\n",
    "\n",
    "            # use the encoder generator for batch emb\n",
    "            # assume no labels included\n",
    "            for encoded_batch_dict in self._encoder.encode(\n",
    "                mut_seqs=self.sequence,\n",
    "                batch_size=embed_batch_size,\n",
    "                flatten_emb=self._flatten_emb,\n",
    "            ):\n",
    "\n",
    "                for layer, emb in encoded_batch_dict.items():\n",
    "                    encoded_dict[layer].append(emb)\n",
    "\n",
    "            # assign each layer as its own variable\n",
    "            for layer, emb in encoded_dict.items():\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"layer\" + str(layer),\n",
    "                    np.vstack(emb)\n",
    "                    # torch.tensor(np.vstack(emb), dtype=torch.float32),\n",
    "                )\n",
    "\n",
    "        # get and format the fitness or secondary structure values\n",
    "        # can be numbers or string\n",
    "        # will need to convert data type\n",
    "        # make 1D tensor 2D\n",
    "        self.y = np.expand_dims(self._get_column_value(\"target\"), 1)\n",
    "\n",
    "        # add mut_name and mut_numb for relevant proeng datasets\n",
    "        if self._add_mut_info:\n",
    "            self.mut_name = self._get_column_value(\"mut_name\")\n",
    "            self.mut_numb = self._get_column_value(\"mut_numb\")\n",
    "        else:\n",
    "            self.mut_name = [\"\"] * self._subdf_len\n",
    "            self.mut_numb = [np.nan] * self._subdf_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the selected subset of the dataframe\"\"\"\n",
    "        return self._subdf_len\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Return the item in the order of\n",
    "        target (y), sequence, mut_name (optional), mut_numb (optional),\n",
    "        embedding per layer upto the max number of layer for the encoder\n",
    "\n",
    "        Args:\n",
    "        - idx: int\n",
    "        \"\"\"\n",
    "        if self.if_encode_all and self._embed_folder is None:\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "                *(\n",
    "                    getattr(self, \"layer\" + str(layer))[idx]\n",
    "                    for layer in range(self._total_emb_layer)\n",
    "                ),\n",
    "            )\n",
    "        elif self._embed_folder is not None:\n",
    "            # load the .h5 file with the embeddings\n",
    "            \"\"\"\n",
    "            gb1_emb = tables.open_file(\"embeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/test/embedding.h5\")\n",
    "            gb1_emb.flush()\n",
    "            gb1_emb.root.layer0[0:5]\n",
    "            \"\"\"\n",
    "            emb_table = tables.open_file(\n",
    "                os.path.join(\n",
    "                    self._embed_folder,\n",
    "                    self._encoder_name,\n",
    "                    self._flatten_emb,\n",
    "                    self._subset,\n",
    "                    \"embedding.h5\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            emb_table.flush()\n",
    "\n",
    "            layer_embs = [\n",
    "                getattr(emb_table.root, \"layer\" + str(layer))[idx]\n",
    "                for layer in range(self._total_emb_layer)\n",
    "            ]\n",
    "\n",
    "            emb_table.close()\n",
    "\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "                layer_embs,\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "            )\n",
    "\n",
    "    def _get_column_value(self, column_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Check and return the column values of the selected dataframe subset\n",
    "\n",
    "        Args:\n",
    "        - column_name: str, the name of the dataframe column\n",
    "        \"\"\"\n",
    "        if column_name in self._df.columns:\n",
    "            y = self._df_dict[self._subset][column_name]\n",
    "\n",
    "            if column_name == \"sequence\":\n",
    "                return (\n",
    "                    # self._df_dict[self._subset][\"sequence\"]\n",
    "                    y\n",
    "                    .astype(str)\n",
    "                    .str[self._seq_start_idx : self._seq_end_idx]\n",
    "                    .apply(\n",
    "                        lambda x: x[: int(MAX_SEQ_LEN // 2)]\n",
    "                        + x[-int(MAX_SEQ_LEN // 2) :]\n",
    "                        if len(x) > MAX_SEQ_LEN\n",
    "                        else x\n",
    "                    )\n",
    "                    .values\n",
    "                )\n",
    "            elif column_name == \"target\":\n",
    "                if self._model_type == \"LinearClassifier\":\n",
    "                    print(\"Converting classes into int...\")\n",
    "                    le = LabelEncoder()\n",
    "                    return le.fit_transform(y.values.flatten())\n",
    "                elif self._model_type == \"MultiLabelMultiClass\":\n",
    "                    print(\"Converting ss3/ss8 into np.array...\")\n",
    "                    return y.apply(lambda x: np.array(x[1:-1].split(\", \"))).values\n",
    "            else:\n",
    "                return y.values\n",
    "\n",
    "    @property\n",
    "    def df_full(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the full loaded dataset\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def df_train(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_train\n",
    "\n",
    "    @property\n",
    "    def df_val(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for validation only\"\"\"\n",
    "        return self._df_val\n",
    "\n",
    "    @property\n",
    "    def df_test(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_test\n",
    "\n",
    "    @property\n",
    "    def max_seq_len(self) -> int:\n",
    "        \"\"\"Longest sequence length\"\"\"\n",
    "        return self._max_seq_len\n",
    "\n",
    "\n",
    "def split_protrain_loader(\n",
    "    dataset_path: str,\n",
    "    encoder_name: str,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 128,\n",
    "    flatten_emb: bool | str = False,\n",
    "    embed_folder: str | None = None,\n",
    "    seq_start_idx: bool | int = False,\n",
    "    seq_end_idx: bool | int = False,\n",
    "    subset_list: list[str] = [\"train\", \"val\", \"test\"],\n",
    "    loader_batch_size: int = 64,\n",
    "    worker_seed: int = RAND_SEED,\n",
    "    if_encode_all: bool = True,\n",
    "    **encoder_params,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    A function encode and load the data from a path\n",
    "\n",
    "    Args:\n",
    "    - dataset_path: str, full path to the dataset, in pkl or panda readable format, ie\n",
    "        \"data/proeng/gb1/low_vs_high.csv\"\n",
    "        columns include: sequence, target, set, validation,\n",
    "        mut_name (optional), mut_numb (optional)\n",
    "    - encoder_name: str, the name of the encoder\n",
    "    - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "    - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "    - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "    - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "    - embed_folder: str = None, path to presaved embedding\n",
    "    - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "    - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "    - subset_list: list of str, train, val, test\n",
    "    - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "    - worker_seed: int, the seed for dataloader\n",
    "    - if_encode_all: bool = True, if encode full dataset all layers on the fly\n",
    "    - encoder_params: kwarg, additional parameters for encoding\n",
    "    \"\"\"\n",
    "\n",
    "    assert set(subset_list) <= set(\n",
    "        [\"train\", \"val\", \"test\"]\n",
    "    ), \"subset_list can only contain terms with in be 'train', 'val', or 'test'\"\n",
    "\n",
    "    # specify no shuffling for validation and test\n",
    "    if_shuffle_list = [True if subset == \"train\" else False for subset in subset_list]\n",
    "\n",
    "    return (\n",
    "        DataLoader(\n",
    "            dataset=ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_folder=embed_folder,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                if_encode_all=if_encode_all,\n",
    "                **encoder_params,\n",
    "            ),\n",
    "            batch_size=loader_batch_size,\n",
    "            shuffle=if_shuffle,\n",
    "            worker_init_fn=worker_seed,\n",
    "        )\n",
    "        for subset, if_shuffle in zip(subset_list, if_shuffle_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A script with model training and testing details assuming\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "\n",
    "# seed everything\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "torch.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed_all(RAND_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_x_y(\n",
    "    model, device, batch, embed_layer: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    A function process x and y from the loader\n",
    "\n",
    "    Args:\n",
    "    -\n",
    "\n",
    "    Returns:\n",
    "    - x\n",
    "    - y\n",
    "    \"\"\"\n",
    "    print(batch)\n",
    "    # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "    x = batch[4][embed_layer]\n",
    "    y = batch[0]\n",
    "\n",
    "    \"\"\"\n",
    "    # process y depends on model type\n",
    "    # annotation classification\n",
    "    if model.model_name == \"LinearClassifier\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y.flatten())\n",
    "    \"\"\"\n",
    "    \n",
    "    # ss3 / ss8 type\n",
    "    if model.model_name == \"MultiLabelMultiClass\":\n",
    "        # convert the y into np.arrays with -1 padding to the same length\n",
    "        y = np.stack(\n",
    "            [\n",
    "                np.pad(\n",
    "                    i,\n",
    "                    pad_width=(0, x.shape[1] - len(i)),\n",
    "                    constant_values=-1,\n",
    "                )\n",
    "                for i in y\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 0,\n",
    "    flatten_emb: bool | str = False,\n",
    "    # if_encode_all: bool = True,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    criterion: nn.Module | None = None,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    **encoder_params,\n",
    ") -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    Runs one epoch.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - device: torch.device or str\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - optimizer: optional torch.optim.Optimizer, must also provide criterion,\n",
    "        only provided for training\n",
    "\n",
    "    Returns: \n",
    "    - float, average loss over batches\n",
    "    \"\"\"\n",
    "    if optimizer is not None:\n",
    "        assert criterion is not None\n",
    "        model.train()\n",
    "        is_train = True\n",
    "    else:\n",
    "        model.eval()\n",
    "        is_train = False\n",
    "\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        # if not if_encode_all:\n",
    "        # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "        for batch in loader:\n",
    "            x, y = get_x_y(model, device, batch, embed_layer)\n",
    "            print(x, y)\n",
    "\n",
    "            \"\"\"\n",
    "            x = batch[4][embed_layer]\n",
    "            y = batch[0]\n",
    "\n",
    "            # process y depends on model type\n",
    "            # annotation classification\n",
    "            if model.model_name == \"LinearClassifier\":\n",
    "                le = LabelEncoder()\n",
    "                y = le.fit_transform(y.flatten())\n",
    "            # ss3 / ss8 type\n",
    "            elif model.model_name == \"MultiLabelMultiClass\":\n",
    "                # convert the y into np.arrays with -1 padding to the same length\n",
    "                y = np.stack(\n",
    "                    [\n",
    "                        np.pad(\n",
    "                            np.array(i[1:-1].split(\", \")),\n",
    "                            pad_width=(0, x.shape[1] - len(i)),\n",
    "                            constant_values=-1,\n",
    "                        )\n",
    "                        for i in y\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \"\"\"\n",
    "\n",
    "            outputs = model(x)\n",
    "\n",
    "            if criterion is not None:\n",
    "                loss = criterion(outputs, y.float())\n",
    "\n",
    "                if optimizer is not None:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "    return cum_loss / len(loader)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 0,\n",
    "    flatten_emb: bool | str = False,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    learning_rate: float = 1e-4,\n",
    "    lr_decay: float = 0.1,\n",
    "    epochs: int = 100,\n",
    "    early_stop: bool = True,\n",
    "    tolerance: int = 10,\n",
    "    min_epoch: int = 5,\n",
    "    **encoder_params,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - train_loader: torch.utils.data.DataLoader, \n",
    "    - val_loader: torch.utils.data.DataLoader, \n",
    "    - criterion: nn.Module, loss function, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float, factor by which to decay LR on plateau\n",
    "    - epochs: int, number of epochs to train for\n",
    "    - early_stop: bool = True,\n",
    "\n",
    "    Returns: \n",
    "    - tuple of np.ndarray, (train_losses, val_losses)\n",
    "        train/val_losses: np.ndarray, shape [epochs], entries are average loss\n",
    "        over batches for that epoch\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=lr_decay\n",
    "    )\n",
    "\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "\n",
    "    # init for early stopping\n",
    "    counter = 0\n",
    "    min_val_loss = np.Inf\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        train_losses[epoch] = run_epoch(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            encoder_name=encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            embed_batch_size=embed_batch_size,\n",
    "            flatten_emb=flatten_emb,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        val_loss = run_epoch(\n",
    "            model=model,\n",
    "            loader=val_loader,\n",
    "            encoder_name=encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            embed_batch_size=embed_batch_size,\n",
    "            flatten_emb=flatten_emb,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            optimizer=None,\n",
    "            **encoder_params,\n",
    "        )\n",
    "        val_losses[epoch] = val_loss\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if early_stop:\n",
    "            # when val loss decrease, reset min loss and counter\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if epoch > min_epoch and counter == tolerance:\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    embed_layer: int,\n",
    "    criterion: nn.Module | None,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    print_every: int = 1000,\n",
    ") -> tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Runs one epoch of testing, returning predictions and labels.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - print_every: int, how often (number of batches) to print avg loss\n",
    "    \n",
    "    Returns: tuple (avg_loss, preds, labels)\n",
    "    - avg_loss: float, average loss per training example \n",
    "    - preds: np.ndarray, shape [num_examples, ...], predictions over dataset\n",
    "    - labels: np.ndarray, shape [num_examples, ...], dataset labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    msg = \"[{step:5d}] loss: {loss:.3f}\"\n",
    "\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    pred_probs = []\n",
    "    pred_classes = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(tqdm(loader)):\n",
    "            # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "            x, y = get_x_y(model, device, batch, embed_layer)\n",
    "\n",
    "            \"\"\"\n",
    "            x = batch[4][embed_layer]\n",
    "            y = batch[0]\n",
    "\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \"\"\"\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x)\n",
    "            print(outputs)\n",
    "\n",
    "            # append results\n",
    "            labels.extend(y.detach().cpu().squeeze().numpy())\n",
    "\n",
    "            # append class\n",
    "            if model.model_name == \"LinearClassifier\":\n",
    "                pred_classes.extend(\n",
    "                    outputs.detach()\n",
    "                    .cpu()\n",
    "                    .data.max(1, keepdim=True)[1]\n",
    "                    .squeeze()\n",
    "                    .numpy()\n",
    "                )\n",
    "                pred_probs.append(outputs.detach().cpu().squeeze().numpy())\n",
    "            else:\n",
    "                pred_probs.extend(outputs.detach().cpu().squeeze().numpy())\n",
    "\n",
    "            if criterion is not None:\n",
    "                loss = criterion(outputs, y)\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "                if ((i + 1) % print_every == 0) or (i + 1 == len(loader)):\n",
    "                    tqdm.write(msg.format(step=i + 1, loss=cum_loss / len(loader)))\n",
    "\n",
    "    avg_loss = cum_loss / len(loader)\n",
    "    return avg_loss, pred_probs, pred_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for running pytorch models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent import futures\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "\n",
    "# from scr.preprocess.data_process import split_protrain_loader, DatasetInfo\n",
    "from scr.preprocess.data_process import DatasetInfo\n",
    "from scr.encoding.encoding_classes import get_emb_info, ESMEncoder, CARPEncoder\n",
    "from scr.model.pytorch_model import LinearRegression, LinearClassifier\n",
    "# from scr.model.train_test import train, test\n",
    "from scr.vis.learning_vis import plot_lc\n",
    "from scr.utils import get_folder_file_names, pickle_save, get_default_output_path\n",
    "\n",
    "\n",
    "class Run_Pytorch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        loader_batch_size: int = 64,\n",
    "        worker_seed: int = RAND_SEED,\n",
    "        if_encode_all: bool = True,\n",
    "        if_multiprocess: bool = False,\n",
    "        learning_rate: float = 1e-4,\n",
    "        lr_decay: float = 0.1,\n",
    "        epochs: int = 100,\n",
    "        early_stop: bool = True,\n",
    "        tolerance: int = 10,\n",
    "        min_epoch: int = 5,\n",
    "        device: torch.device | str = DEVICE,\n",
    "        all_plot_folder: str = \"results/learning_curves\",\n",
    "        all_result_folder: str = \"results/train_val_test\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running pytorch model\n",
    "\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        \n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "        - worker_seed: int, the seed for dataloader\n",
    "        - learning_rate: float\n",
    "        - lr_decay: float, factor by which to decay LR on plateau\n",
    "        - epochs: int, number of epochs to train for\n",
    "        - device: torch.device or str\n",
    "        - all_plot_folder: str, the parent folder path for saving all the learning curves\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "\n",
    "        Returns:\n",
    "        - result_dict: dict, with the keys and dict values\n",
    "            \"losses\": {\"train_losses\": np.ndarray, \"val_losses\": np.ndarray}\n",
    "            \"train\": {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._dataset_path = dataset_path\n",
    "        self._encoder_name = encoder_name\n",
    "        self._reset_param = reset_param\n",
    "        self._resample_param = resample_param\n",
    "        self._embed_batch_size = embed_batch_size\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._lr_decay = lr_decay\n",
    "        self._epochs = epochs\n",
    "        self._early_stop = early_stop\n",
    "        self._tolerance = tolerance\n",
    "        self._min_epoch = min_epoch\n",
    "        self._device = device\n",
    "        self._all_plot_folder = all_plot_folder\n",
    "        self._all_result_folder = all_result_folder\n",
    "        self._encoder_params = encoder_params\n",
    "\n",
    "        self._ds_info = DatasetInfo(self._dataset_path)\n",
    "        self._model_type = self._ds_info.model_type\n",
    "        self._numb_class = self._ds_info.numb_class\n",
    "\n",
    "        self._train_loader, self._val_loader, self._test_loader = split_protrain_loader(\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            embed_folder=embed_folder,\n",
    "            seq_start_idx=seq_start_idx,\n",
    "            seq_end_idx=seq_end_idx,\n",
    "            subset_list=[\"train\", \"val\", \"test\"],\n",
    "            loader_batch_size=loader_batch_size,\n",
    "            worker_seed=worker_seed,\n",
    "            if_encode_all=if_encode_all,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        encoder_name, encoder_class, total_emb_layer = get_emb_info(encoder_name)\n",
    "\n",
    "        if encoder_class == ESMEncoder:\n",
    "            self._encoder_info_dict = TRANSFORMER_INFO\n",
    "        elif encoder_class == CARPEncoder:\n",
    "            self._encoder_info_dict = CARP_INFO\n",
    "\n",
    "        if if_multiprocess:\n",
    "            print(\"Running different emb layer in parallel...\")\n",
    "            # add the thredpool max_workers=None\n",
    "            with futures.ProcessPoolExecutor(max_workers=os.cpu_count() - 1) as pool:\n",
    "                # for each layer train the model and save the model\n",
    "                for embed_layer in tqdm(range(total_emb_layer)):\n",
    "                    pool.submit(self.run_pytorch_layer, embed_layer)\n",
    "\n",
    "        else:\n",
    "            for embed_layer in range(total_emb_layer):\n",
    "                print(f\"Running pytorch model for layer {embed_layer}\")\n",
    "                self.run_pytorch_layer(embed_layer)\n",
    "\n",
    "    def run_pytorch_layer(self, embed_layer):\n",
    "\n",
    "        # init model based on datasets\n",
    "        if self._model_type == \"LinearRegression\":\n",
    "            model = LinearRegression(\n",
    "                input_dim=self._encoder_info_dict[self._encoder_name][0], output_dim=1\n",
    "            )\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "        elif self._model_type == \"LinearClassifier\":\n",
    "            model = LinearClassifier(\n",
    "                input_dim=self._encoder_info_dict[self._encoder_name][0], \n",
    "                numb_class=self._numb_class\n",
    "            )\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        model.to(self._device, non_blocking=True)\n",
    "\n",
    "        criterion.to(self._device, non_blocking=True)\n",
    "\n",
    "        train_losses, val_losses = train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=self._train_loader,\n",
    "            val_loader=self._val_loader,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            device=self._device,\n",
    "            learning_rate=self._learning_rate,\n",
    "            lr_decay=self._lr_decay,\n",
    "            epochs=self._epochs,\n",
    "            early_stop=self._early_stop,\n",
    "            tolerance=self._tolerance,\n",
    "            min_epoch=self._min_epoch,\n",
    "            **self._encoder_params,\n",
    "        )\n",
    "\n",
    "        # record the losses\n",
    "        result_dict = {\n",
    "            \"losses\": {\"train_losses\": train_losses, \"val_losses\": val_losses}\n",
    "        }\n",
    "\n",
    "        plot_lc(\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            all_plot_folder=self._all_plot_folder,\n",
    "        )\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, loader in zip(\n",
    "            [\"train\", \"val\", \"test\"],\n",
    "            [self._train_loader, self._val_loader, self._test_loader],\n",
    "        ):\n",
    "            loss, pred, cls, true = test(\n",
    "                model=model, loader=loader, device=self._device, criterion=criterion\n",
    "            )\n",
    "\n",
    "            if model.model_name == \"LinearRegression\":\n",
    "                result_dict[subset] = {\n",
    "                    \"mse\": loss,\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"ndcg\": ndcg_score(true[None, :], pred[None, :]),\n",
    "                    \"rho\": spearmanr(true, pred),\n",
    "                }\n",
    "\n",
    "            elif model.model_name == \"LinearClassifier\":\n",
    "                result_dict[subset] = {\n",
    "                    \"cross-entropy\": loss,\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"acc\": accuracy_score(true, cls),\n",
    "                    \"rocauc\": roc_auc_score(true, pred, multi_class=\"ovo\"),\n",
    "                }            \n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=get_default_output_path(self._all_result_folder),\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting classes into int...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting classes into int...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting classes into int...\n",
      "Running pytorch model for layer 0\n",
      "[tensor([[4],\n",
      "        [9],\n",
      "        [1],\n",
      "        [9],\n",
      "        [1],\n",
      "        [7],\n",
      "        [5],\n",
      "        [7],\n",
      "        [8],\n",
      "        [1],\n",
      "        [1],\n",
      "        [7],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7],\n",
      "        [6],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7],\n",
      "        [9],\n",
      "        [6],\n",
      "        [1],\n",
      "        [0],\n",
      "        [9],\n",
      "        [2],\n",
      "        [3],\n",
      "        [9],\n",
      "        [7],\n",
      "        [7],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [9],\n",
      "        [7],\n",
      "        [6],\n",
      "        [1],\n",
      "        [7],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [7],\n",
      "        [3],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [9],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [7],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [9]]), ('MSAIFNFQSLLTVILLLICTCAYIRSLAPSLLDRNKTGLLGIFWKCARIGERKSPYVAVCCIVMAFSILFI', 'ASYKVKLITPEGAVEFDCPDDVYILDQAEEMGHDLPYSCRAGSCSSCAGKVTAGNVDQSDGNFLDDDQMADGFVLTCVAYPQSDVTIETHKEEELT', 'MEPCEDKCKRGLSLNFFEEDGPSLRLNSDSLDFLARDFKVEGMSQDNFDQKTKLYITEESLQKEVNIFLTKIYIRESEREPPQSHNSVFQLLSKIRNSVPNVSAFRNNLSILSKELSFFSFARHIHNRRLCWAEFIYCIRRGIKAIFKTTVQFLPTRLAKIFEKKASEVLKDNLLQTCNSKREGEVCDVKEPAVASSESSDCFNDMQELNNIVDLRDYSNSRFQQNRLLDRNLKGWIQGESEKALKGRRTTKRNDKENYNYPDFSNDNELLFSLATLIVENNPKKENIIPKYYLRYLQRLSRTEINKEIIEIEKLELEVVQFQMSIANLINTQVEVTNTIEELGLRCRPLPNEN', 'MESTLSAFSTVKATAMARSSGGPSLPLLTISKALNRHFTGARHLHPLLLARCSPSVRRLGGFHGSRFTSSNSALRSLGAAVLPVIRHRLECLSSSSPSFRSISSGGGSGFGGYNGGSGGGGGGGSESGDSKSKLGANASDGVSVPSSDIIILDVGGMTCGGCSASVKKILESQPQVASASVNLTTETAIVWPVPEAKSVPDWQKSLGETLANHLTNCGFQSTPRDLVTENFFKVFETKTKDKQARLKESGRELAVSWALCAVCLVGHLTHFLGVNAPWIHAIHSTGFHVSLCLITLLGPGRKLVLDGIKSLLKGSPNMNTLVGLGALSSFSVSSLAAMIPKLGWKTFFEEPVMLIAFVLLGRNLEQRAKIKATSDMTGLLSVLPSKARLLLDGDLQNSTVEVPCNSLSVGDLVVILPGDRVPADGVVKSGRSTIDESSFTGEPLPVTKESGSQVAAGSINLNGTLTVEVHRSGGETAVGDIIRLVEEAQSREAPVQQLVDKVAGRFTYGVMALSAATFTFWNLFGAHVLPSALHNGSPMSLALQLSCSVLVVACPCALGLATPTAMLVGTSLGARRGLLLRGGDILEKFSLVDTVVFDKTGTLTKGHPVVTEVIIPENPRHNLNDTWSEVEVLMLAAAVESNTTHPVGKAIVKAARARNCQTMKAEDGTFTEEPGSGAVAIVNNKRVTVGTLEWVKRHGATGNSLLALEEHEINNQSVVYIGVDNTLAAVIRFEDKVREDAAQVVENLTRQGIDVYMLSGDKRNAANYVASVVGINHERVIAGVKPAEKKNFINELQKNKKIVAMVGDGINDAAALASSNVGVAMGGGAGAASEVSPVVLMGNRLTQLLDAMELSRQTMKTVKQNLWWAFGYNIVGIPIAAGVLLPLTGTMLTPSMAGALMGVSSLGVMTNSLLLRYRFFSNRNDKNVKPEPKEGTKQPHENTRWKQS', 'MESAPIVLDNGTGFVKVGYAKDNFPRFQFPSIVGRPILRAEEKTGNVQIKDVMVGDEAEAVRSLLQVKYPMENGIIRDFEEMNQLWDYTFFEKLKIDPRGRKILLTEPPMNPVANREKMCETMFERYGFGGVYVAIQAVLSLYAQGLSSGVVVDSGDGVTHIVPVYESVVLNHLVGRLDVAGRDATRYLISLLLRKGYAFNRTADFETVREMKEKLCYVSYDLELDHKLSEETTVLMRNYTLPDGRVIKVGSERYECPECLFQPHLVGSEQPGLSEFIFDTIQAADVDIRKYLYRAIVLSGGSSMYAGLPSRLEKEIKQLWFERVLHGDPARLPNFKVKIEDAPRRRHAVFIGGAVLADIMAQNDHMWVSKAEWEEYGVRALDKLGPRT', 'MMVQRLGPISPPASQVSTACKQISPSLPRAVNAANLNRPPSDTRSVILQESLVSTTLSLTESQSALSVKQEWSQSYRAFPSLSSSHSSQNGTDLGDLLSLPPGTPVSGNSVSNSLPPYLFGMENSHSPYPSPRHSATRAHSTRSKKRALSLSPLSDGIGIDFNTIIRTSPTSLVAYINGPRASPANLSPQSEVYGHFLGVRGSCIPQSCAVASGQKGILVASGGHTLPGYGEDGTLEYERMQQLEHGGLQPGPVNNMVLQPGLPGQDGQTANMLKTERLEEFPASALDLPSALPLPLPPPQGPPPPYHAHPHLHHPELLPHTQSLSLAQTGLEEDGEMEDSGGKHCCRWIDCSALYDQQEELVRHIEKVHIDQRKGEDFTCFWTGCPRRYKPFNARYKLLIHMRVHSGEKPNKCTFEGCKKAFSRLENLKIHLRSHTGEKPYLCQHPGCQKAFSNSSDRAKHQRTHLDTKPYACQIPGCTKRYTDPSSLRKHVKAHSSREQQARKKLRSSTELHPDLLTDCLAVQPLQPATSPGDAADHTVGHSPGPGPGPGPGAELYSAPIFASNHSTRSGTAAGAGPPPHPVSHPSPGHNVQGSPHNPSSQLPPLTAVDAGAERFAPPTPSPHHISPGRVPAPPSLLQRAQAPHSQQPPGSLLKPYQPETNSSFQPNGIHVHGFYGQLQTFCPPHYPDSQRTVPPSGSCSMVPSFEDCLVPTSMGQAGFDVFHRAFSTHSGITVYDLPSASSSLFGESLRSGPEDPTFLQLSAVDRCPSQLSSVYTE', 'MTTSTIENGASSPIIVSSSTPKLYQEGAGVWIPDQELGWIGADVIEHSETSADQVLVRTEDDREVKIPLSKVFQKNPDILEGVDDLSFLSHLHEPAILHNLHHRYNLNQIYTYIGKILIAINPYTSLPLYGKEMISAYYGKQLGTLAPHVYAVAEDAFKDMRYDGTSQSILVSGESGAGKTETTKFLLQYFAAMGNMIKESTSSSSINGINTSSDGIPVTPPPSPMKKSPVDKSVEERVLESTPLLEAFGNAKTLRNDNSSRFGKFIEIHFNEMGSIIGAKILTYLLEKSRIVRQVYNERNYHIFYQLLSGASEELKEKLNLKTIEEYSYLNKSGCFEIEGVSDEEHFNKTCHAMQVAGITLVEQENVFRILSAILLIGNFEFENIAGSNDDSCQLIDRDPLEKVSVLLGCAQPDELLNSMLTRKVVTGKESYISHNTKERAENARDSLSMFLYGMMFDWLVVKINSSMSISTQQKSKSFIGVLDIYGFESFEVNGFEQFCINYANEKLQQEQEIKSKRMSVELQQHIDEGKQQEIQQLQSTIAQLKQQQQSETDRLEKEIQQMKRERETQMKLVESTKLNYHMLEDRMELYRNVMEIIDYKETEWEKLARLAGCKELDTKLLSDFLLSCKLEHTSLGSQMWFHQIDYWCPYERDSSKGIFYGIIRSIVDFTIKNFDDVDLLSYLLACCSLTLFLYKKNLVKHLNGANSIMPIIPTLGDLEELNERLSHQSLTTSGKFSGGGGGGGIDFIDQLQQSTGITFGLIFKATTLKLSPLVDGAILNENYNKKLTSISASSFGSGSFGLGSNGVGSVLSIELITTYLSSIITIFQHRMVHFTLSQRFFNQVFCWIGALIMKGFMLRQTFCTETFATFVKTKIDFLTRWADDIGNVWVGDVANAFQQVREVINVLNIKDKEKIIDDKIRKQYCPTLNSNQLKQVLSLFSPGEFGGKRVSAKVIASICPPNKSSAGQSFVQDENKLNTIPIDSLHYLEIQDIKTLSLPLSIRQTIETEIINLKQQIACK', 'MGRSRRTGAHRAHSLARQMKAKRRRPDLDEIHRELRPQGSARPQPDPNAEFDPDLPGGGLHRCLACARYFIDSTNLKTHFRSKDHKKRLKQLSVEPYSQEEAERAAGMGSYVPPRRLAVPTEVSTEVPEMDTS', 'MSQEIRQNEKISYRIEGPFFIIHLMNPDNLNALEGEDYIYLGELLELADRNRDVYFTIIQSSGRFFSSGADFKGIAKAQGDDTNKYPSETSKWVSNFVARNVYVTDAFIKHSKVLICCLNGPAIGLSAALVALCDIVYSINDKVYLLYPFANLGLITEGGTTVSLPLKFGTNTTYECLMFNKPFKYDIMCENGFISKNFNMPSSNAEAFNAKVLEELREKVKGLYLPSCLGMKKLLKSNHIDAFNKANSVEVNESLKYWVDGEPLKRFRQLGSKQRKHR', 'MKSEPTSLDFTSSNLRRMNRDHSSNNTNRTVLNLPKEILIIIFSFLDPRSLLSAQCTCKYWKKLLSDDLSWRTAFFHHFAGDQSQIFSPLGNGTWRQEYLLRSTITRAYEKGKGQTVQYDCRVGQLTNLYYDFSSGRLYSGNWLTGTISVSDPTTGKVERSLLHASTDGSFTHGLSTMTLGKQIFGFGFMDGRVGVILMSRQAETPRKFRYCLDSHADSVTCIDALTGDLPPTGEIGMVTGSDDGSVHCWDVKTGVSLQSFQFRSSQILSLCFRPKYKMLLVDTFNYELNSYQLYLIPGYARSRKNEQPILLSSRKCVLTDEEEPPCLMTADCCAGVAFLSRGAPKNCICRVSFKEFLEKNDNVGVQTSSIPLNGKPTSISLDTNDRVLSKSTPGRGARLLAVGDENGLVYVVNTRTEDPNKAILRTITAYSNFPITDIYLNEVAMVVGSASGYCGVYDTVTGNFLKKIASARNAARREPINCILLDSNPLSLKGVITMSKHVKSWSYTIPKPFVNKRSKVLPLRPSVTHDNLSKSSDYSKNEVEREIMLGLDQIAQERREKMEARQKFEQHFGEGLVGLSEEEIIAYVTMLSQEEEAKRMVQLSMDVDKIEEDFKENDEQATSSLNALSSNHEPPQEQANVAELNEQEQIELAMRLSLME', 'MASDGASALPGPDMSMKPSAAPSPSPALPFLPPTSDPPDHPPREPPPQPIMPSVFSPDNPLMLSAFPSSLLVTGDGGPCLSGAGAGKVIVKVKTEGGSAEPSQTQNFILTQTALNSTAPGTPCGGLEGPAPPFVTASNVKTILPSKAVGVSQEGPPGLPPQPPPPVAQLVPIVPLEKAWPGPHGTTGEGGPVATLSKPSLGDRSKISKDVYENFRQWQRYKALARRHLSQSPDTEALSCFLIPVLRSLARLKPTMTLEEGLPLAVQEWEHTSNFDRMIFYEMAERFMEFEAEEMQIQNTQLMNGSQGLSPATPLKLDPLGPLASEVCQQPVYIPKKAASKTRAPRRRQRKAQRPPAPEAPKEIPPEAVKEYVDIMEWLVGTHLATGESDGKQEEEGQQQEEEGMYPDPGLLSYINELCSQKVFVSKVEAVIHPQFLADLLSPEKQRDPLALIEELEQEEGLTLAQLVQKRLMALEEEEDAEAPPSFSGAQLDSSPSGSVEDEDGDGRLRPSPLCWQGGFQPESTPSLDAGLAELAPLQGQGLEKQVLGLQKGQQTGGRGVLPQGKEPLAVPWEGSSGAMWGDDRGTPMAQSYDQNPSPRAAGERDDVCLSPGVWLSSEMDAVGLELPVQIEEVIESFQVEKCVTEYQEGCQGLGSRGNISLGPGETLVPGDTESSVIPCGGTVAAAALEKRNYCSLPGPLRANSPPLRSKENQEQSCETVGHPSDLWAEGCFPLLESGDSTLGSSKETLPPTCQGNLLIMGTEDASSLPEASQEAGSRGNSFSPLLETIEPVNILDVKDDCGLQLRVSEDTCPLNVHSYDPQGEGRVDPDLSKPKNLAPLQESQESYTTGTPKATSSHQGLGSTLPRRGTRNAIVPRETSVSKTHRSADRAKGKEKKKKEAEEEDEELSNFAYLLASKLSLSPREHPLSPHHASGGQGSQRASHLLPAGAKGPSKLPYPVAKSGKRALAGGPAPTEKTPHSGAQLGVPREKPLALGVVRPSQPRKRRCDSFVTGRRKKRRRS', 'MILRTPQPKRLRSDAGESPFPTGATGSGNQLIIYEDSPLPAPAPLQTSHDHSADQHLCTYQCRQMVKADVLDALSTAEKQVEESKTKLQTLNANFTEADAERKHFRDKFLYSEQELAAAKAREKMLQEQLLMEINNSQERYTKELQSCHELEVKLQNEMNLRKKAESSAATAEEKAKLLEDKLTQLSGSVDREKKRLNNDIAQLGKEAKLSVARIGADLERMQCRAQNAETESNLLRSQLEHLKLIFDECLQEKTEVDKKLSSFTSEAASSSDNSVLVKHLQEELKRYEAEVREARKLKSRHLDAELLNVNLLEEQSRRERAESELSKFHDLQLSMEKLENELSSWKSLLNDIPGVSCPDDIVMRFSVLQNEVVQSTMKIGEASTRIKQLEETLEAIQLGRQNAVSEAALAKEKSEALKTDVKRIEVMLTLVTEEKEQLKAVVNELRKSNSEGSVSGAADGALIQGFESSLAKKENYIKDLEQDLNQLKDVNNRQRTEIELLNEKLVDEARRNKSLERDSDRLRSEISLLESKLGHGDYSAANTRVLRMVNTLGVENEAKQTIEALQAELQKTKERLQAVEELKSQSGDAGKLVDSHITGKIAQLKEQNATLEKREERYKTVFADRISVFRRACCELFGYKIVMDEHQRPNGIPVTRFTLQSIYAQSDDEKLEFEYESGNTSILNNEYASQGDIAKQIEIFIRKFNSIPAFTANLTMESFNRRTL', 'MSSPQRRKAMPWALSLLLMGFQLLVTYAWCSEEEMGGNNKIVQDPMFLATVEFALNTFNVQSKEEHAYRLLRVLSSWREDSMDRKWRGKMVFSMNLQLRQTVCRKFEDDIDNCPFQESLELNNVRQGISFPQVHSCGCCMGCGVGTGAADKAIPRDKG', 'MNGDELPFESGFEVYAKGTHKSEFDSNLLDPRSDPIWDAIREEAKLEAEKEPILSSFLYAGILAHDCLEQALGFVLANRLQNPTLLATQLLDIFYGVMMHDKGIQSSIRHDLQAFKDRDPACLSYSSAILHLKGYHALQAYRVAHKLWNEGRKLLALALQSRISEVFGIDIHPAARIGEGILLDHGTGVVIGETAVIGNGVSILHGVTLGGTGKETGDRHPKIGEGALLGACVTILGNISIGAGAMVAAGSLVLKDVPSHSVVAGNPAKLIRVMEEQDPSLAMKHDATKEFFRHVADGYKGAQSNGPSLSAGDTEKGHTNST', 'MAFPAGFGWAAATAAYQVEGGWDADGKGPCVWDTFTHQGGERVFKNQTGDVACGSYTLWEEDLKCIKQLGLTHYRFSLSWSRLLPDGTTGFINQKGIDYYNKIIDDLLKNGVTPIVTLYHFDLPQTLEDQGGWLSEAIIESFDKYAQFCFSTFGDRVKQWITINEANVLSVMSYDLGMFPPGIPHFGTGGYQAAHNLIKAHARSWHSYDSLFRKKQKGMVSLSLFAVWLEPADPNSVSDQEAAKRAITFHLDLFAKPIFIDGDYPEVVKSQIASMSQKQGYPSSRLPEFTEEEKKMIKGTADFFAVQYYTTRLIKYQENKKGELGILQDAEIEFFPDPSWKNVDWIYVVPWGVCKLLKYIKDTYNNPVIYITENGFPQSDPAPLDDTQRWEYFRQTFQELFKAIQLDKVNLQVYCAWSLLDNFEWNQGYSSRFGLFHVDFEDPARPRVPYTSAKEYAKIIRNNGLEAH', 'MMDSPFLELWQSKAVSIREQLGLGDRPNDSYCYNSAKNSTVLQGVTFGGIPTVLLIDVSCFLFLILVFSIIRRRFWDYGRIALVSEADSESRFQRLSSTSSSGQQDFENELGCCPWLTAIFRLHDDQILEWCGEDAIHYLSFQRHIIFLLVVVSFLSLCVILPVNLSGDLLDKDPYSFGRTTIANLQTDNDLLWLHTIFAVIYLFLTVGFMRHHTQSIKYKEENLVRRTLFITGLPRDARKETVESHFRDAYPTCEVVDVQLCYNVAKLIYLCKEKKKTEKSLTYYTNLQVKTGQRTLINPKPCGQFCCCEVLGCEWEDAISYYTRMKDRLLERITEEERHVQDQPLGMAFVTFQEKSMATYILKDFNACKCQSLQCKGEPQPSSHSRELYTSKWTVTFAADPEDICWKNLSIQGLRWWLQWLGINFTLFLGLFFLTTPSIILSTMDKFNVTKPIHALNNPIISQFFPTLLLWSFSALLPSIVYYSTLLESHWTKSGENQIMMTKVYIFLIFMVLILPSLGLTSLDFFFRWLFDKTSSEASIRLECVFLPDQGAFFVNYVIASAFIGNGMELLRLPGLILYTFRMIMAKTAADRRNVKQNQAFQYEFGAMYAWMLCVFTVIVAYSITCPIIAPFGLIYILLKHMVDRHNLYFVYLPAKLEKGIHFAAVNQALAAPILCLFWLYFFSFLRLGMKAPATLFTFLVLLLTILVCLAHTCFGCFKHLSPLNYKTEEPASDKGSEAEAHMPPPFTPYVPRILNGLASERTALSPQQQQQQTYGAIHNISGTIPGQCLAQSATGSVAAAPQE', 'RICYLAPRDTQICAPGQEICYLKSWDDGTGSIRGNRLEFGCAATCPTVKRGIHIKCCSTDKCNPHPKL', 'MSGYSSDRDRGRDRGFGAPRFGGSRAGPLSGKKFGNPGEKLVKKKWNLDELPKFEKNFYQEHPDLARRTAQEVETYRRSKEITVRGHNCPKPVLNFYEANFPANVMDVIARQNFTEPTAIQAQGWPVALSGLDMVGVAQTGSGKTLSYLLPAIVHINHQPFLERGDGPICLVLAPTRELAQQVQQVAAEYCRACRLKSTCIYGGAPKGPQIRDLERGVEICIATPGRLIDFLECGKTNLRRTTYLVLDEADRMLDMGFEPQIRKIVDQIRPDRQTLMWSATWPKEVRQLAEDFLKDYIHINIGALELSANHNILQIVDVCHDVEKDEKLIRLMEEIMSEKENKTIVFVETKRRCDELTRKMRRDGWPAMGIHGDKSQQERDWVLNEFKHGKAPILIATDVASRGLDVEDVKFVINYDYPNSSEDYIHRIGRTARSTKTGTAYTFFTPNNIKQVSDLISVLREANQAINPKLLQLVEDRGSGRSRGRGGMKDDRRDRYSAGKRGGFNTFRDRENYDRGYSSLLKRDFGAKTQNGVYSAANYTNGSFGSNFVSAGIQTSFRTGNPTGTYQNGYDSTQQYGSNVPNMHNGMNQQAYAYPATAAAPMIGYPMPTGYS', 'MSHLVSSSLGTTTTATPTSRSPHTNHSTPYNQNSITSNRSSPVPKNSVNSRIIPQTMNPPIDMKSNNILNPEKDTDTSRGDHSESKASSISSASGTTTTNNNNVSNNNSTGKTQIVFIHKLYDMLHDESISHLIWWSPSLDSFYVTPGEEFSRVLSQYFKHTNIASFIRQLNMYGFHKVNEPFLNQDDQQQQQQLQSNRWEFRHSTNQFRKGDTESLKNIKRRSSKTLNAQKEVVNIKSLPPTSHPMEYNTGYSYQNEDSAHYFVHHHSITTMQSPADMRPRSPSTPIPMQPLAQQQQQQQQQQQQQQQQLPSQPVPNGPPVFSGPIPPGAVNQSPQEYLTRPSILNNVQGSFENATNFKFVELTNQINLLRNDFFTMNNRYEILQNELKYQTADSMAVLEILEKLSNDNRIATDIRDLKNVVSQRMQRLNNQFIPQQSNFAPHIPGQQQQQQHGNSVSSNYHLESTNVSRNPSTTNLNVAPQPYPLNPHYTIYANNRASGSSEINNGVFRAREDSNNSKRNLSVYDPLQPVPSRNSSRILIEESTPTHPPTNFNPQQSQSQSQVQLGPAMPPQGFRNRAESTYSPLSHSSNKSQILNKAPTPVNHSPLVQQQQKEAKQELNDSSVAPPSQSSLPVTRPLSRQQQQQQQTLHHPSTTSSRTNSLPNPVAEHPAPQSSYFMQRNSFNTIYEHQKSLRVPSPKRVRYATPPRSIPEQPISSTAPTTMITSTSKTTSTSGAAISRSENHSVVSLTGGALPSVSELDKSIRTGSSVSLPPIKSIKDNDNKNDNGNSDDNGNDHKKRKL', 'MAAFRDIEEVSQGLLSLLGANRAEAQQRRLLGRHEQVVERLLETQDGAEKQLREILTMEKEVAQSLLNAKEQVHQGGVELQQLEAGLQEAGEEDTRLKASLLYLTRELEELKEIEADLERQEKEVDEDTTVTIPSAVYVAQLYHQVSKIEWDYECEPGMVKGIHHGPSVAQPIHLDSTQLSRKFISDYLWSLVDTE', 'MPFMNFRNFNFMPLLRPLNPVYKGAEHLCRKNIHNQVYNVETGKTIQILQDLVPLVLYRALPDAILDHHVELVIFPNSLNFPRIEGLTIYKFIFKTARLLLSSTYGSSAKRPIDIIHQLHSSMENKNVRYSMKCNWIASPNDEYTWVFHFDIDKKGIIYRHIIDNLERNRSRQCEKISALKPDPI', 'MDRCKHVEQLQLAQGHSILDPQKWYCMVCNTTESIWACLSCSHVACGKYIQEHALKHFQESSHPVAFEVNDMYAFCYLCNDYVLNDNAAGDLKSLRSTLSTIKSKKYPCVVPSDSVLHPVDAQDRVYSLLDGTQSLPGNEDPTCAALWHRRRVLMGKAFRTWFEQSAIGRKGQEPTQERMVAKREAKRRQQQELEQQMKAELESTPPRKSLRLQGSSEEAATIEIVPVRAPPPPPASPAKDKAALPTSEDRTFKKVSDSLIKRRPMVTPGVTGLRNLGNTCYMNSVLQVLSHLLIFRQCFLKLDLNQWLAVAASDKARSYKHSAVTEAAAQQMNEGQEKEKGFVCSRHSGLSSGLSGGASKGRNMELIQPREPSSPYSSLCHELHILFQVMWSGEWALVSPFAMLHSVWRLIPAFRGYAQQDAQEFLCELLDKIQRELETTGTKLPALIPTSQRRLIEQVLNVVNNIFHGQFLSQVTCLACDNKSDTIESFWDLSLEFPERYQCSGKDAASQPCLVTDMLDKFTETEALEGKIYMCDHCNSKRRKFSSKSVVFTEAQKQLMICHLPQVLRLHLKRFRWSGRNNREKIGVHVVFEETLNMEPYCCRETLNALRPECFLYNLSAVVIHHGKGFGSGHYTAYCYNSEGGFWVHCNDSKLSMCTMEEVRKAQAYILFYTQRVTENGHSKLLPPELLSNSQHPSKETDASSNEVL', 'MSRRALRRLRGEQRGQEPLGPDALKFVLLDDDDAEEEGPKPGLGGRRPGGAGKEGVRVNNRFELINTEDLEDDLVVNGERSDCTLPDSVSSGNKGRAKHGNAETKQDGGATKAGSSEQSNASGKLRKKKKKQKNKKSCTGESSENGLEDIDRILERIEDSSGFSHPGPPPLSSRKHVLYVEHRHLNPDTELKRYFGARAVLGEQRPRQRQRVYPKCTWLTTPKSTWPRYSKPGLSMRLLESKKGLSFFAFDHNEEYQQAQHKFLVAVESMEPNNIVVLLQTSPYHVDSLLQLSDACRFQEDQEMARDLIERALYSMECAFHPLFSLTSGTCRLDYRRPENRSFYLTLYKQMSFLEKRGCPRTALEYCKLILSLEPDEDPLCMLLLIDHLALRARNYEYLIRLFQEWEAHRNLSQLPNFAFSVPLAYFLLSQQTDLPEHELSSARQQASLLIQQALTMFPGVLMPLLEYCSVRPDATVSNHRFFGPDAEISQPPALGQLVSLYLGRSHFLWKEPAIMSWLEENVHEVLQAVDAGDPAVEACENRRKVLYQRAPRNIHRHVILSEIKEAVAALPSDVTTQSVMGFDPLPPLDTIYSYVRPERLSPVSHGNTIALFFRSLLPNYTTEGERLEEGVAGGPNRNQGLNRLMLAVRDMMANFHFNDLEVPREDNPEGEGDW', 'MIFYDKLKPADVLVIGSADGRVIEAIEYIADLHKQHGFKFAICLGNLFSHKRTTSADVVKLKNEKVKVPIPVYFGVGTAGLPESIISHMAMYGPEVAPNLFCMGICGFMKTFYKFTIAQLGGSYNEEKYYQPPEKFEQSLNEKCFHRSDVQKLSKRCDILFSSEWPEDVQENSTLPERKLPKGCMPLAALAANCMPQYFFVPGPVYYEREPYKNSAAINVNTGTVTHFVALAPFKNSKNEKFSYAFTLYPLTTEYMQPAPPNCTASPFEHRPIPLKRASEDQIIPQQTNKFHKSKSSTALFKSKKDSSSSLNKMHKSESHSALNNLHKSESGTSLNNRRSKVGPGSCFFCLSNPNVALHLIVAIGNEAYMALPKGPLTTTASNTPALASSGHVLIIPIAHASALSTLSDTSYEKTLNEMNRFRKAVTDMYNACDSDALVYEISRANGVHLHWQMIPIPKISSHRIESVFLEMAKEAGYDFEERDVEPHELNYFRVFLPSGKILIHRLQLRERFDLQFGRRAAAKILGLEDRVDWRKCVQTEDEEKAESEAFKMCFKPYDF', 'MSFLLLTPPPCLLIPPPPLSHRRSSSLFLKHPFQPSPRPLSFCKPSALRLRANTTVNSLKALETIKPYLQSESKTVLLGWLCSCVSVVSLSQIVPRLGSFTSNLNANAASLTKLKGECLVLAGLVLAKVVAYYLQQAFLWEAALNTVYKIRVFAYRRVLERELEFFEGGNGISSGDIAYRITAEASEVADTIYALLNTVVPSAIQISVMTAHMIVASPALTLVSAMVIPSVALLIAYLGDRLRKISRKAQIASAQLSTYLNEVLPAILFVKANNAEISESVRFQRFARADLDERFKKKKMKSLIPQIVQVMYLGSLSIFCVGAVILAGSSLSSSAIVSFVASLAFLIDPVQDLGKAYNELKQGEPAIERLFDLTSLESKVIERPEAIQLEKVAGEVELCDISFKYDENMLPVLDGLNLHIKAGETVALVGPSGGGKTTLIKLLLRLYEPSSGSIIIDKIDIKDIKLESLRKHVGLVSQDTTLFSGTIADNIGYRDLTTGIDMKRVELAAKTANADEFIRNLPEGYNTGVGPRGSSLSGGQKQRLAIARALYQKSSILILDEATSALDSLSELLVREALERVMQDHTVIVIAHRLETVMMAQRVFLVERGKLKELNRSSLLSTHKDSLTSAGLV', 'MAAASAVSVLLVAAERNRWHRLPSLLLPPRTWVWRQRTMKYTTATGRNITKVLIANRGEIACRVMRTAKKLGVQTVAVYSEADRNSMHVDMADEAYSIGPAPSQQSYLSMEKIIQVAKTSAAQAIHPGCGFLSENMEFAELCKQEGIIFIGPPPSAIRDMGIKSTSKSIMAAAGVPVVEGYHGEDQSDQCLKEHARRIGYPVMIKAVRGGGGKGMRIVRSEQEFQEQLESARREAKKSFNDDAMLIEKFVDTPRHVEVQVFGDHHGNAVYLFERDCSVQRRHQKIIEEAPAPGIKSEVRKKLGEAAVRAAKAVNYVGAGTVEFIMDSKHNFCFMEMNTRLQVEHPVTEMITGTDLVEWQLRIAAGEKIPLSQEEITLQGHAFEARIYAEDPSNNFMPVAGPLVHLSTPRADPSTRIETGVRQGDEVSVHYDPMIAKLVVWAADRQAALTKLRYSLRQYNIVGLHTNIDFLLNLSGHPEFEAGNVHTDFIPQHHKQLLLSRKAAAKESLCQAALGLILKEKAMTDTFTLQAHDQFSPFSSSSGRRLNISYTRNMTLKDGKNNVAIAVTYNHDGSYSMQIEDKTFQVLGNLYSEGDCTYLKCSVNGVASKAKLIILENTIYLFSKEGSIEIDIPVPKYLSSVSSQETQGGPLAPMTGTIEKVFVKAGDKVKAGDSLMVMIAMKMEHTIKSPKDGTVKKVFYREGAQANRHTPLVEFEEEESDKRES', 'MFRRGDNSNFNVQNSFFLPLEYEYTVKDNVPSKKKSSIGFFPLDDSLFTSKNNSGHHKSEQLHRGNAETIRSQFGTDAVPIRIDEKEGKWDRIQDDNSSNLNYQINNSNDPASSGKYTQSIDCNHIAESKFSKKNGNIDSLRSNSATFMLNTADEDVIEFSFDDNVPYAELLSGATLEKCSLTLNEINKKLFNTLYDFRVSKDNPEENLVELILPNCVVLLNFFEDIELLADSSDEAFEKSTFINTIEFIVHDIWVETLIKNINLLQMFDADLKCYNDKYIICKLKGQYPSTNIVDIMCRLKHFSNSILETFKFGIELKEQDQCHNRNTIINYVLFSRVFSTIVLEIQKCFILIVKFMYSVNFLEKFSDEVFLSFIEILIKIVFEHQIPQLFLGIDEIIQLWLKNNEGKRQQLLSAWCNGTVQDMKQSQQRESSNAESESITSSTEEDEEGLQFNKWDVIEPFIDNIKALNQSKSH', 'MQKHSPGPPALALLSQSLLTTGNGDTLIINCPGFGQHRVDPAAFQAVFDRKAIGPVTNYSVATHVNISFTLSAIWNCYSRIHTFNCHHARPWHNQFVQWNPDECGGIKKSGMATENLWLSDVFIEESVDQTPAGLMASMSIVKATSNTISQCGWSASANWTPSISPSMDRARAWRRMSRSFQIHHRTSFRTRREWVLLGIQKRTIKVTVATNQYEQAIFHVAIRRRCRPSPYVVNFLVPSGILIAIDALSFYLPLESGNCAPFKMTVLLGYSVFLLMMNDLLPATSTSSHASLVAPLALMQTPLPAGVYFALCLSLMVGSLLETIFITHLLHVATTQPLPLPRWLHSLLLHCTGQGRCCPTAPQKGNKGPGLTPTHLPGVKEPEVSAGQMPGPGEAELTGGSEWTRAQREHEAQKQHSVELWVQFSHAMDALLFRLYLLFMASSIITVICLWN', 'MSVGLKVSAFLHPTLALSSRDVSLSSSSSSLYLDRKILRPGSGRRWCKSRRTEPILAVVESSRVPELDSSSEPVQVFDGSTRLYISYTCPFAQRAWIARNYKGLQNKIELVPIDLKNRPAWYKEKVYSANKVPALEHNNRVLGESLDLIKYIDTNFEGPSLTPDGLEKQVVADELLSYTDSFSKAVRSTLNGTDTNAADVAFDYIEQALSKFNEGPFFLGQFSLVDVAYAPFIERFRLILSDVMNVDITSGRPNLALWIQEMNKIEAYTETRQDPQELVERYKRRVQAEAR', 'MAISKAWISLLLALAVVLSAPAARAEEAAAAEEGGDAAAEAVLTLDADGFDEAVAKHPFMVVEFYAPWCGHCKKLAPEYEKAAQELSKHDPPIVLAKVDANDEKNKPLATKYEIQGFPTLKIFRNQGKNIQEYKGPREAEGIVEYLKKQVGPASKEIKSPEDATNLIDDKKIYIVGIFSELSGTEYTNFIEVAEKLRSDYDFGHTLHANHLPRGDAAVERPLVRLFKPFDELVVDSKDFDVTALEKFIDASSTPKVVTFDKNPDNHPYLLKFFQSSAAKAMLFLNFSTGPFESFKSVYYGAAEEFKDKEIKFLIGDIEASQGAFQYFGLREDQVPLIIIQDGESKKFLKAHVEPDQIVSWLKEYFDGKLSPFRKSEPIPEVNDEPVKVVVADNVHDFVFKSGKNVLVEFYAPWCGHCKKLAPILDEAATTLKSDKDVVIAKMDATANDVPSEFDVQGYPTLYFVTPSGKMVPYESGRTADEIVDFIKKNKETAGQAKEKAESAPAEPLKDE', 'RRCFNQQSSQPQTNKSCPPGENSCYRKQWRDHRGTIIERGCGCPTVKPGVKLRCCQSEDCN', 'MPSLSTPPSQNLAFSPAASATSSRLTPSSKRSFYPHRLPDPTALCRCSSSSGSNSSSSSSSDDNPRWDSAIQDVLKSAIKRFDSVLSWYATLDNDDGEQGSENVEKIDDDWDWDRWKKHFDQVDDQDRLLSVLKSQLNRAIKREDYEDAARLKVAIAATATNDAVGKVMSTFYRALLEERYKDAVYLRDKAGAGLVGWWSGISEDVKDPFGLIVQITAEHGRYVARSYNPRQLSTSAAGAPLFEIFLTLDGKGNYKKQAVYLKWKEIFPDVPTMPSRTLTPGRFLTSPGRKEDTGNLAVESSEDEESDNSDDDSDLLEESSGFQSFLRDMIPGVKVKVMKVTAPGRVDKDFISKVIEQIADEEDEENDLDIEDIDVEDDTKAEIDEKNADIELESVTDEIIDNNGGREIAVKFVIGDIVDRLSGNQPLKESLRSPANLESVENSSFYLRLEKDLNVKESKGVEGTTLVDGKGSRQSRRRIENIMGDLAKSIEKEKKISVKMLKDVGELLSLTLSQAQNRQQLSGLTKFRRIDVTPSLDPLDGLYIGAHGLYTSEVIHLKRKFGQWKGGKESKKPTDIEFYEYVEAVKLTGDPYVPAGKVAFRAKIGRRYELPHKGLIPEEFGVIARYKGQGRLADPGFRNPRWVDGELVILDGKYVKGGPVVGFVYWAPEYHFVMFFNRLRLQ', 'MVRKKNPPLRNVASEGEGQILEPIGTESKVSGKNKEFSADQMSENTDQSDAAELNHKEEHSLHVQDPSSSSKKDLKSAVLSEKAGFNYESPSKGGNFPSFPHDEVTDRNMLAFSSPAAGGVCEPLKSPQRAEADDPQDMACTPSGDSLETKEDQKMSPKATEETGQAQSGQANCQGLSPVSVASKNPQVPSDGGVRLNKSKTDLLVNDNPDPAPLSPELQDFKCNICGYGYYGNDPTDLIKHFRKYHLGLHNRTRQDAELDSKILALHNMVQFSHSKDFQKVNRSVFSGVLQDINSSRPVLLNGTYDVQVTSGGTFIGIGRKTPDCQGNTKYFRCKFCNFTYMGNSSTELEQHFLQTHPNKIKASLPSSEVAKPSEKNSNKSIPALQSSDSGDLGKWQDKITVKAGDDTPVGYSVPIKPLDSSRQNGTEATSYYWCKFCSFSCESSSSLKLLEHYGKQHGAVQSGGLNPELNDKLSRGSVINQNDLAKSSEGETMTKTDKSSSGAKKKDFSLEEKDGLKEKVWTESSSDDLRNVTWRGADILRGSPSYTQASLGLLTPVSGTQEQTKTLRDSPNVEAAHLARPIYGLAVETKGFLQGAPAGGEKSGALPQQYPASGENKSKDESQSLLRRRRGSGVFCANCLTTKTSLWRKNANGGYVCNACGLYQKLHSTPRPLNIIKQNNGEQIIRRRTRKRLNPEALQAEQLNKQQRGSNEEQVNGSPLERRSEDHLTESHQREIPLPSLSKYEAQGSLTKSHSAQQPVLVSQTLDIHKRMQPLHIQIKSPQESTGDPGNSSSVSEGKGSSERGSPIEKYMRPAKHPNYSPPGSPIEKYQYPLFGLPFVHNDFQSEADWLRFWSKYKLSVPGNPHYLSHVPGLPNPCQNYVPYPTFNLPPHFSAVGSDNDIPLDLAIKHSRPGPTANGASKEKTKAPPNVKNEGPLNVVKTEKVDRSTQDELSTKCVHCGIVFLDEVMYALHMSCHGDSGPFQCSICQHLCTDKYDFTTHIQRGLHRNNAQVEKNGKPK', 'MSCQAFTSADTFIPLNSDASATLPLIMHHSAAECLPVSNHATNVMSTATGLHYSVPSCHYGNQPSTYGVMAGSLTPCLYKFPDHTLSHGFPPIHQPLLAEDPTAADFKQELRRKSKLVEEPIDMDSPEIRELEKFANEFKVRRIKLGYTQTNVGEALAAVHGSEFSQTTICRFENLQLSFKNACKLKAILSKWLEEAEQVGALYNEKVGANERKRKRRTTISIAAKDALERHFGEQNKPSSQEIMRMAEELNLEKEVVRVWFCNRRQREKRVKTSLNQSLFSISKEHLEC', 'MVSCLCFRPSRKTKLKDKSHKRSIRNQTSSSSAQPAGTAKEVDSSSSQTVVQDSSRYRCQIFSYRELAIATNSFRNESLIGRGGFGTVYKGRLSTGQNIAVKMLDQSGIQGDKEFLVEVLMLSLLHHRNLVHLFGYCAEGDQRLVVYEYMPLGSVEDHLYDLSEGQEALDWKTRMKIALGAAKGLAFLHNEAQPPVIYRDLKTSNILLDHDYKPKLSDFGLAKFGPSDDMSHVSTRVMGTHGYCAPEYANTGKLTLKSDIYSFGVVLLELISGRKALMPSSECVGNQSRYLVHWARPLFLNGRIRQIVDPRLARKGGFSNILLYRGIEVAFLCLAEEANARPSISQVVECLKYIIDHTIRKERRTRRRLLGGNKDGAGTSRSPDETMMRMLEEEEEYVTSEEAIERRRVIVDDARTWAGMNRRGATPPTPT', 'MLRVLLSAQTSPARLSGLLLIPPVQPCCLGPSKWGDRPVGGGPSAGPVQGLQRLLEQAKSPGELLRWLGQNPSKVRAHHYSVALRRLGQLLGSRPRPPPVEQVTLQDLSQLIIRNCPSFDIHTIHVCLHLAVLLGFPSDGPLVCALEQERRLRLPPKPPPPLQPLLRGGQGLEAALSCPRFLRYPRQHLISSLAEARPEELTPHVMVLLAQHLARHRLREPQLLEAIAHFLVVQETQLSSKVVQKLVLPFGRLNYLPLEQQFMPCLERILAREAGVAPLATVNILMSLCQLRCLPFRALHFVFSPGFINYISGTPHALIVRRYLSLLDTAVELELPGYRGPRLPRRQQVPIFPQPLITDRARCKYSHKDIVAEGLRQLLGEEKYRQDLTVPPGYCTDFLLCASSSGAVLPVRTQDPFLPYPPRSCPQGQAASSATTRDPAQRVVLVLRERWHFCRDGRVLLGSRALRERHLGLMGYQLLPLPFEELESQRGLPQLKSYLRQKLQALGLRWGPEG', 'MGSVNNEEKTLIEPQRLLRKNTWHPEVDDSEVPPSVFPEYPVHKAIQKTSDSFRKRNYSAGDYVIAPLGGEREGSSLTHSWTFQPGKHNQRLYSDNFQEAQRQWKRLQEWGEVKETKKIRKRFDRFSGRKYINHYEIIKELGRGMHGKVKLGRDTVTRELLAIKIIPKTERRPKLGRANASSQKEKVRREIAILKKCVHPNVVRLREVIDDPSSTKVYLVLEYMSGGEVPWTDCDSPVLSISEARQYFRDVVLGLEYLHYQGIIHRDIKPANLLLNSSNCVKISDFGVSYIANAGLNEDNDVELAKTVGTPAFFAPELCWTDLDRPRPKISEAIDVWALGVTLFCLLFGRCPFNASMEYELFDKIVNERLNIPSTPDIGEEGRDLLKRLLCKDPEQRITLVEVKLHPWTLDGLKDPEKWLQNTDPSTVSRVEVSTDEVASAISLVGRLRRKLGKLFRFRRPKARVFDSSSSVPSDSSICRPESSGNSSIGLSASELSDSFNRLAVNESQKDRERKQVHPVEMGRNSSEKKPRCDFGWDYEAFPNDNQDADDACSYNTGDSIPQVSKSINGHFETYSRTSMDTDDVASFESPNAKHEESGMPVVTFRNYENYDANPSNFHPVVPGFVSSPNLHLAGGSDTPIYCIEHSFTPT', 'MNSSPPFLLKISAPSTSPQADCPNNYSFPPESPSSCRKGFTPVLTLEVPVAPGKDFNDHLSCNAGLSPNAGNRFTNPPYSREPFSCLTISSPCLPRRIPTPPPPPPVLSSPPPPERCPFEPFSPLLGRLYRQEPAGSSSPCFDRFSLQGSPSPHQRNLCCNYIDSPESQRSCPPSPRLCYVTSPPLIHQAPRASPVTSPELTHITLETGPVISTPLMPGSQGNYSIISPLLTHRPLRPGLAISPPLAHRSVETRPLTPASISHRGPHCPSRRSYNDPPLSSASSPPSGNPYHDNPMPPNSCEPKPQLDVPLGKNGCGPPLSSQAGMSGSPISPQEGCIHYSHLCPDSQISAPRSPFCVINLPPESAGSPSSSLPQALQKPCVGSFLWEPGGNSYLLLTPGTIISGPSCTTGPPLPQCPNPSPYFPSPLNNQCVAPPQSPRGYNEPRPPTSAPPQMKSPKSPESRRNPYKCRSLDNTPHHTPPSHSKSHKTNTCPQPPSQSFGLFSPCMEPAITTTSNSCPKEPPPETAVLKTVAPTSCPHSSPCNPALPSRYPKSSPHVPPPVSPCNTHMYSVVPPTSHLSPLSSPLNQSIPLPQPAVLPCGTYSAPRGPPSHIKSVAPPCSTHIYSFIPLRTPFDPRCLPVVPRARFCPTTVPCGIHTYAVTSPVPLNNPSQIPYSCSLPPSKTSSTCSTSVSSTIVCSDYQSSDSQINHQNKSQSPNKNSSLHNQSKSPLRRGAFQSRSRSRSSSPLQSSTQDRNESTNMGVKHHKRSRKQSQSPADGKIESQSKSLQHRKSVGQIKSPHSKK', 'MIRLSPGFAINPRFRSDSFVNSQKPPFLSVQIGSQRRNLSRIGSENGDGVAFDAVAYEADRLSLDAAAMEDMAETAKKELESDPDSDPKAWKWVIRKKMWDLMEARNYAMSPRPVHHRIPNFVGASAAARKLAELDAFRMAMVVKVNPDSPQKQIRFLTLSGEKKLLTPQPRLRTGFFSVLESDLLKPETIMEACTSVGVAKYGRAIGLDEKIKVDLIVIGSVAVNPQTGARLGKGEGFAELEYGMLRYMGAIDDSTPVVTTVHDCQLVDDIPLEKLAIHDVPVDIICTPTRVIFTNTPIPKPQGIYWDKLSPEKLRQIRILRELKNRLEKKTGRKLPTGPSEKLPPTAERKR', 'MAFRPFGKDLGPMSSKPAPFTPFGASSTTRLYLSFLFLHTANVGFACSDSSIQPPASQNHSAFAGQSFGPGGIRSGPSIQRAPPLSASQNPQLSIGKPYRPGGVQSVPPINRIPSPSAFQNPSPSSGQPYQPGGIQRIPEPFNGIAWGPEASRTSPSVRPYQFPGVQRPNLNPQYGHDGSRNFLKDHGEHSRATSPPATSHILSRMGTDAVEIGRSQDSKRKSRSDILPDQNMGFSRRNQSPVSGFENGNLVDGFQPLSSRTWMRSPSSAENNPVRSRSNPNQLIHQEQTGNSSFPYAHEVAEIQEATRRKSSAVAPSDKPLGDDPILSQHDSQRFSTSPPTSGTKSYTLSRSSDSQFPGQPSSVNSFNNARKTNSSPATKRTRSPPVYPIEEDIPRNSFPSQDCTEGEEQARAKRLARFKGELEPIADRPVDIQLTKSPVNKTMKPLDNKQTFNSLESSRDALKGDALPDYENSEQPSLIIGVCPDMCPESERGERERKGDLDHYERVDGEHSSLNDDNLLFSAPGVSVWNKWVANGSDIDFTCCLSVARDVEAENDMCETTCGASAVLFLASGGLPLNLQREQLNLILESVPNGSVLPLLVVISSCNGEHMEPDTDIVSGLGLHDIDKSKIASFSIVSIANKSQKGQEVHFFNDSRLRDGFKWLASNSPLQPNLHHVKLRELFLTHFSFSLELLKQMPDQEVGPNICISAFNDALETSRRNITSAAEANPIGWPCPETMLLEDNRKECLMVKRYLPNLDWSSAENVELLSSVLENCKLPDFEDDLTWLTVGCASGAEIENHTQRLEGCLIEYLTQRSNLMGVSLATKETGVMLERNTRLELHNSSRYHITPRWIGIFQRIFNWRIMGLFDASSSSAYVLKSDLNMSTSSYADKFLAEDASYPSCPPNLPLLHEMIEISCSPLKSPPPYDDKAQRVVETGMLIDDHRDIEESMLEKNREACRGIDLMITEDDELGERSWRSKGREAAEKKTIEKRESERLDELLEKCNMVQNSIAEKLCIY', 'MIPRTRTLLQSKIPITRYFARCWAPRVRYNVCRTLPAAALHTNIIAHNEVKKDDKKVHLGSFKVDKPKMMIAFTCKKCNTRSSHTMSKQAYEKGTVLISCPHCKVRHLIADHLKIFHDHHVTVEQLMKANGEQVSQDVGDLEFEDIPDSLKDVLGKYAKNNSENASQLPHPSQ', 'MAAVEPENTNPQSTEEEKETGQEIVSPDQYIKHPLQNRWALWFFKNDKSKTWQANLRLISKFDTVEDFWALYNHIQLSSNLMSGCDYSLFKDGIEPMWEDEKNKRGGRWLITLNKQQRRNDLDRFWLETLMCLIGESFDEHSDDVCGAVVNVRAKGDKIAIWTTEFENKDAVTHIGRVYKERLGLPAKVVIGYQSHADTATKSGSTTKNRFV', 'MFGGELDDAFGVFEGKVPKSLKEESKNSQNSQNSQKIKRTLTDKNASNQEQGTKKLESSVGEQESATKRAKIENLKDNQDLIPNNDVNGIHINNSAVADTKHKPKIGDIAADDISNEVSIKNEGDTIPEATVADSFEQEASLQVAGKVGMTEAKSSTEEVVELRHQVRHQVSIPPNYDYVPISKHKSPIPPARTYPFTLDPFQAVSIACIERQESVLVSAHTSAGKTVVAEYAVAQSLRDKQRVIYTSPIKALSNQKYRELLAEFGDVGLMTGDVTINPDATCLVMTTEILRSMLYRGSEVMREVAWVIFDEIHYMRDKERGVVWEETIILLPDKSHFVFLSATIPNAMQFAEWITKIHRQPCHVVYTDFRPTPLQHYLFPSGSDGIHLVVDEKSNFREENFQRAMSALMEKQGDDPAAMATKGNAKKGKTGKGGVKGPSDIYKIVKMIMVKNYNPVIVFSFSKRECEALALQMSKLDMNDQTERDLVTTIFNNAVNQLSEKDRELPQIEHMIDEKMDPPVAKSMLKGEADRLDSAFHLSYNMILNLLRVEGISPEFMLERCFFQFQNSLEVPKLEAKLEESQQHYDSFTILDERPLEEYHTLKTQLERYRTDVRTVVNHPNFCLSFLQGGRLVRVKVGNEDFDWGVVVNVSKRPLPKGQSNEYLPQESYIVHTLVMVASDTGPLRIRSGHLPEVHPPAAEDKGKFEVVPFLLSSLDGIAHIRVFLPNDLKSQGQKLTVGKALSEVKRRFPEGITLLDPVENMNIKEPTFIKLMKKVNILESRLLSNPLHNFSELEEKYAEYLRKLALLEEVKDLKKKLSKARSIMQLDELNSRKRVLRRLGFTTSDDVIEVKGRVACEISSGDGLLLTELIFNGMFNDLTPEQCAALLSCLVFQEKSEVENQRMKEELAGPLKILQEMARRIAKVSKESKQELNEEEYVNSFKPSLMEVVYAWAHGASFAQICKMTDVYEGSLIRMFRRLEELIRQMVDAAKVIGNTSLQQKMEDTIACIHRDIVFSASLY', 'MVDQRRSALGFVLLLLCLVLFFDCVVVGQTQSRFSEKLILLNLRSSLGLRGTDWPIKGDPCVDWRGIQCENGSIIGINISGFRRTRIGKLNPQFSVDPLRNLTRLSYFNASGLALPGTIPEWFGVSLLALEVLDLSSCSVNGVVPFTLGNLTSLRTLNLSQNSLTSLVPSSLGQLLNLSQLDLSRNSFTGVLPQSFSSLKNLLTLDVSSNYLTGPIPPGLGALSKLIHLNFSSNSFSSPIPSELGDLVNLVDFDLSINSLSGSVPQELRKLSKLQLMAIGDNLLSGTLPVDLFSAESQLQTLVLRENGFSGSLPDVCWSLPKLRILDIAKNNFTGLLPYSSYDSDQIAEMVDISSNTFYGELTPILRRFRIMDLSGNYFEGKLPDYVTGENVSVTSNCLRNERRQKPSAICAAFYKSRGLDFDDFGRPNLTQPTSKNASSGISRRTVIILAAVGGGVAFILLFVILPIILVLCMRHRRRAAQRGNNDRPKPAGEASQQPPKGAQTFDLSRLGNAFSYEQLLQATEEFNDANLIKRGHSGNLFRGFLENGIPVVIKKIDVREGKSEGYISELELFSKAGHQRLVPFLGHCLENESQKFLVYKFMRHGDLASSLFRKSENEGDGLKSLDWITRLKIALGAAEGLSYLHHECSPPLVHRDVQASSILLDDKFEVRLGSLSEAYAQGDAYQSRISRLLRLPQSSEPSSSGVTNAICSYDVYCFGKVLLELVTGKLGISSPDNALAKEYMEEALPYISTNEKELVTKILDPSLMVDEDLLEEVWAMAIIAKSCLNPKPTRRPLMRHIVNALENPLKVVREDTNSGSGSSRLRTNSSRGSWNAAIFGSWRQSASDVTAVQAGATTSGGGGGGGGNGLRNSGSQGSSGRNNNNNGNSSSSRRRQSSEIVPEPAAYGVVEDN', 'MRRFLLLYATQQGQAKAIAEEICEQAVVHGFSADLHCISESDKVSVIQNTPTFAMGG', 'MALWRAYQRALAAHPWKVQVLTAGSLMGLGDIISQQLVERRGLQEHQRGRTLTMVSLGCGFVGPVVGGWYKVLDRFIPGTTKVDALKKMLLDQGGFAPCFLGCFLPLVGALNGLSAQDNWAKLQRDYPDALITNYYLWPAVQLANFYLVPLHYRLAVVQCVAVIWNSYLSWKAHR', 'MSQLSFTGKSSSKGRSRLTQEVRPTASQIIADEEASDLDEYEEDLEGSGNEDDFGPSMSRSSRGRKRRKGDPLELQSQFEERNETDAINFQLLVRNVVRYAICSQTSHNTITRKDIVQKAFPEGTSRNLFQSVFEEADRQLQLSFGFRLVAVTQSNRKKDMAVSQLRRPATSNANSSNLHRYWVLRSTLPMELQKDSRLIVDSVLDTAYYGFLMTVIAFIAVSHCSVGHSELQSFLQELLTEEETTPLHLDITRSLSLLVRQGYLDRVKDDTHNQFVYYIGSRAVTEISIEGLKSFVTEFFPDSDIDMDALLTEYRQEYQNQSSSSA', 'MGLSIGSLLICVFLGIVPFATANTNSSSYEEHRNYLLNIFHNPFVNDSIKEKNIPQLIAFYQRYPTDVPLSDADRQQFERFIHDYREYRAVLVDGAPPQGGSFGNIFGHFLGRVGTRYISSLFNKKREERKSNHAYIIEDY', 'MVCLKTLSVFLAAFAAADARAVFKTQGHKNSEMIPDNYIVVMKDGVSQDDFKAHISSVASIHSTNKAKRGTNTQGMKREFDIMNWRGYHGHFDRDTLEEILNDSKVDYVEQDQVVRISGLVTQRSAPSWGLGRVSHRQAGSRDYVFDDSAGRGVTIYGVDTGIDINHQDFRGRARWGTNTADRDNADRHGHGTHTASTFAGTAYGIAKNANIVAVKVLGSDGSGSTSGIIAGINYCVQDAQQRGILGKAAMNLSLGGGFSQANNDAVTRAQNAGIFVAVAAGNDNRDARNYSPASAPAVCTVASSTINDSKSSFSNWGPVVDIYAPGSDIIAARPGGGSTTMSGTSMASPHVAGMGAYMIGMGADPRQVCDRLKQLATAAIRNPGSSTTNRLLYNGSG', 'MSKLFSTVNSARHSVPLGGMRDYVHIKKLEMNTVLGPDSWNQLMPQKCLLSLDMGTDFSKSAATDDLKYSLNYAVISRDLTNFVSKKKNWGSVSNLAKSVSQFVMDKYSGVECLNLEVQADTTHIRSDHISCIIQQERGNPESQEFDVVRISELKMLTLIGVFTFERLKKQYVTLDIKLPWPKKAELPPPVQSIIDNVVKFVEESNFKTVEALVESVSAVIAHNEYFQKFPDSPLVVKVLKLNAITATEGVGVSCIREPREIAMVNIPYLSSIHESSDIKFQLSSSQNTPIEGKNTWKRAFLAFGSNIGDRFKHIQMALQLLSREKTVKLRNISSIFESEPMYFKDQTPFMNGCVEVETLLTPSELLKLCKKIEYEELQRVKHFDNGPRTIDLDIVMFLNSAGEDIIVNEPDLNIPHPRMLERTFVLEPLCELISPVHLHPVTAEPIVDHLKQLYDKQHDEDTLWKLVPLPYRSGVEPRFLKFKTATKLDEFTGETNRITVSPTYIMAIFNATPDSFSDGGEHFADIESQLNDIIKLCKDALYLHESVIIDVGGCSTRPNSIQASEEEEIRRSIPLIKAIRESTELPQDKVILSIDTYRSNVAKEAIKVGVDIINDISGGLFDSNMFAVIAENPEICYILSHTRGDISTMNRLAHYENFALGDSIQQEFVHNTDIQQLDDLKDKTVLIRNVGQEIGERYIKAIDNGVKRWQILIDPGLGFAKTWKQNLQIIRHIPILKNYSFTMNSNNSQVYVNLRNMPVLLGPSRKKFIGHITKDVDAKQRDFATGAVVASCIGFGSDMVRVHDVKNCSKSIKLADAIYKGL', 'MNLVGSYAHHHHHHHSHPPHPMLHEPFLFGPASRCHQERPYFQSWLLSPADAAPDFPAGGPPPTTAVAAAAYGPDARPSQSPGRLEALGSRLPKRKGSGPKKERRRTESINSAFAELRECIPNVPADTKLSKIKTLRLATSYIAYLMDVLAKDAQAGDPEAFKAELKKTDGGRESKRKRELPQQPESFPPASGPGEKRIKGRTGWPQQVWALELN', 'MKTLLLTLVVVTIVCLDFGHTMICYNQQSSQPPTTTTCSEGQCYKQRWRDHRGWRTERGCGCPKAIPEVKLNCCKTDRCN', 'MAFLKKSLFLVLFLGLVSLSICEEEKRETEEEENDQEEDDKSEEKRFLSLLPSLVSGAVSLVKKL', 'MELEQLKKSVEENPDDSSLQFELGLYLWDNGGDSEKAAEHFVLSAKSDPNNAVAFKYLGHYYSRVTLDLNRAAKCYQRAVLINPNDSDSGEALCDLFDRQGKEILEIAVCRDASEKSPKAFWAFCRLGYIQLHQKKWSEAVQSLQHAIRGYPTMSDLWEALGLAYQRLGMFTAAIKAYGRAIELDETKIFALVESANIFLMLGSYRKGVELFEQALKISPQNISVLYGLASGLLSWSKECINLGAFGWAASLLEDARKAAKASSELASSMSCIWKLHGDIQLTYARCFPWSGGTENSEFTLKTFSDSILSWRSICYSAALSAKASYQRALHLAPWQANVYTDIAITCDLVSSLSDDSDTSSSWKLPEKMVLGALLLECENSEFWVALGCMSDNSALKLHALIRALHLDVSLAVAWAFMGQIFRESDEMKFAKQAFDCARSIDPTLALPWAGSADTYARESTSDEAFESCLRAAQISPLAEFQVGLAWLALLQGNISSPQIFACIEQAVQRSMPKDFFQSSKISFIVSAIHSLDQSDRLQSIVASTRSYITSQEEIVAMHYLIALSKLLKTGAGDFLGYEKGIAHLSKAIHMYPHSNLIRNLLGYILLAGEGMKDACTASRCCIINVSECANKEGLKSALEVLGGGSVACNVIGNTAPRFSFPTCHCQYLNAPVVVVELQRFLHQEPSNSSVRYLLILNLVQKAREQRFPRQLCRAIERLISVALSDETCSKEGEYKKFQLLLCASEISLQMGNIAESINHARKASSLSLPSSYLFLGHLQLCRAYAANGSTKNMQEEYRACLELKTDSNIGWICLKLIESQYNLEPDANLLEMSLQECSSQKKNSWKEWMAVYSLARGLDSTGKKDFFSAEEFLAQACSLLNSESCLLLCHGAVCMELARQSNDSQFLSLAVKSLSKVQASSLFPLPIVYTLLAQAHGSLGSKEKWEKNLRLEWFCWPPEMRPAEVYFQMHILARQSEDRPETTSGIENYQSPEKWVIRAIHTDPSCRRYWKVLDKLVQHPM', 'MATTSSNPLLLSSNFLGSQIIISAPTPKTTTKSLPFSVISRKRYQISQSEKLMKSLPSQAALAALLFSSSSPQALAVNEPVQPPAPTITAEAQSPNLSTFGQNVLMTAPNPQAQSSDLPDGTQWRYSEFLNAVKKGKVERVKFSKDGSVLQLTAVDNRRATVIVPNDPDLIDILAMNGVDISVSEGEGGNGLFDFIGNLLFPLLAFGGLFYLFRGGQGGAGGPGGLGGPMDFGRSKSKFQEVPETGVTFGDVAGADQAKLELQEVVDFLKNPDKYTALGAKIPKGCLLVGPPGTGKTLLARAVAGEAGVPFFSCAASEFVELFVGVGASRVRDLFEKAKSKAPCIVFIDEIDAVGRQRGAGMGGGNDEREQTINQLLTEMDGFSGNSGVIVLAATNRPDVLDSALLRPGRFDRQVTVDRPDVAGRVQILKVHSRGKAIGKDVDYEKVARRTPGFTGADLQNLMNEAAILAARRELKEISKDEISDALERIIAGPEKKNAVVSEEKKRLVAYHEAGHALVGALMPEYDPVAKISIIPRGQAGGLTFFAPSEERLESGLYSRSYLENQMAVALGGRVAEEVIFGDENVTTGASNDFMQVSRVARQMVERFGFSKKIGQVAVGGAGGNPFLGQSMSSQKDYSMATADVVDAEVRELVEKAYVRAKEIITTQIDILHKLAQLLIEKETVDGEEFMSLFIDGQAELYV', 'MATTSTTGSTLLQPLSNAVQLPIDQVNFVVCQLFALLAAVWFRTYLHSSKTSSFIRHVVATLLGLYLAFFCFGWYALHFLVQSGISYCIMIIAGVESMQQCCFVFALGYLSVCQITRVYIFDYGQYSADFSGPMMIITQKITSLAYEIHDGMFRKDEELTPSQRGLAVRRMPSLLEYVSYTCNFMGILAGPLCSYKDYIAFIEGRASHVAQPSENGKDEQHGKADPSPNAAVTEKLLVCGLSLLFHLTISNMLPVEYNIDEHFQATASWPTKATYLYVSLLAARPKYYFAWTLADAINNAAGFGFRGYDKNGVARWDLISNLRIQQIEMSTSFKMFLDNWNIQTALWLKRVCYERATFSPTIQTFFLSAIWHGVYPGYYLTFLTGVLMTLAARAVRNNFRHYFLEPPQLKLFYDLITWVATQITISYTVVPFVLLSIKPSFTFYSSWYYCLHVCSILVLLLLPVKKSQRRTSTQENVHLSQAKKFDERDNPLGQNSFSTMNNVCNQNRDTGSRHSSLT', 'MPIFTLNTNIKATDVPSDFLSSTSALVGNILSKPGSYVAVHINTDQQLSFGGSTKPAAFGTLMSIGGIEPSRNRDHSAKLFDHLNKKLGIPKNRMYIHFVNLNGDDVGWNGTT', 'MLLHLCSVKNLYQNRFLGLAAMASPSRNSQSRRRCKEPLRYSYNPDQFHNIDIRNGAHDAITIPRSTSDTDLVTSDSRSTLMVSSSYYSIGHSQDLVIHWDIKEEVDAGDWIGMYLIGEVSSENFLDYKNRGVNGSHRGQIIWKIDASSYFVESETKICFKYYHGVSGALRATTPSVTVKNSAAPIFKGIGSEETAQSQGSRRLISFSLSDFQAMGLKKGMFFNPDPYLKISIQPGKHSIFPALPHHGQERRSTIIGNTVNPIWQAEHFSFVSLPTDVLEIEVKDKFAKSRPIIKRFLGKLSMPVQRLLERHAIGDRVVSYTLGRRLPTDHVSGQLQFRFEITSSIHADDEEISLSAEPESSAETQDSIMNSMVGNSNGEPSGDATEFCKDAKPESPSEGNGVNSSENQNQEHAGPVEEAAGAMEARDGSNVSEAPEEPGELQDPEQHDTQPTLSAEEVAEGLPLDEDSPSSLLPEENTALGSKVEEETVPENGAREEEMQKGKDEEEEEEGHSLIAAIRSQHQHESLPLAYNDKIVAFLRQPNIFEMLQERQPSLARNHTLREKIHYIRTEGNHGLDKLSCDADLVILLSLFEEEIMSYVPLQSAFHPGYSFSPRCSPCSSPQNSPGLQRASARAPSPYRRDFEAKLRNFYRKLEAKGFGQGPGKIKLIIRRDHLLEGTFNQVMAYSRKELQRNKLYITFVGEEGLDYSGPSREFFFLLSQELFNPYYGLFEYSANDTYTVQISPMSAFVENYLEWFRFSGRILGLALIHQYLLDAFFTRPFYKGLLKLPCDLSDLEYLDEEFHQSLQWMKDNNITDILDLTFTVNEEVFGQVTERELKSGGANTQVTEKNKKEYIERMVKWRVERGVVQQTEALLRGFYEVVDSRLVSVFDARELELVIAGTAEIDLNDWRNNTEYRGGYHDGHLVIRWFWAAVERFNNEQRLRLLQFVTGTSSVPYEGFAALRGSNGLRRFCIEKWGKITSLPRAHTCFNRLDLPPYPSYSMLYEKLLTAVEETSTFGL', 'MSDVDNWEPVSDNEDSTDSVKQLGPPFEHASNNDNAGDTEAESLQEVPLNTETNDVRKNLVVITNQSAADEHPTEIKHDQSRTSSTSSFFSGMISSFKSNVPSPVSRSTTPTSPVSQPSIISHRREPSMGSKRRSSRRISNATIAEIGSPLQQVEKPDEVKTRLTPSQMKEDNYDHRRFVEERYMDTPYHYASEQRNKDFHETFKSVPKDDRLLDDFNCGLNRELLYQGKLYITETHLCFNSNVLGWIAKVLIAFEDVTFMEKTSAAGLFPSAISIETKMGKTLFNGFISRDAAFGLMKEVWSRTLLQKDMASENINTKAEKSGNGKEIDDAINSIDEENNDKDANDNDTNENDDENISTNETTPNSTSSSPDKEKEKAYKLRADSSYQYDGPIYHHSTSFPAEPMANNEFVLKELPFDCAPGILFEIMFNSEQNEFLLDFLRGQEGSQITTIPNFTSIDGSSMTLKREYSYEKALHFPAGPKSTTCYVAEVIKRKDPDTYYEVISSIRTPNVPSGGSFSTKTRYLIRWNDEITCLLRVSFWVEWTGSSWIKGMVENGCKNGQLEAAQLMERILSKFIKNNVEECQITISKEEEEQDDKEVKNKLKEVDLEQPREAVVTAPAIAEQQGLKVTMETWLFLYLIVVVLLLFNLFYIRSIAVSLHQLVKLQLVELK', 'MDPSEYFAGGNPSDQQNQKRQLQICGPRPSPLSVHKDSHKIKKPPKHPAPPPNRDQPPPYIPREPVVIYAVSPKVVHATASEFMNVVQRLTGISSGVFLESGGGGDVSPAARLASTENASPRGGKEPAARDETVEINTAMEEAAEFGGYAPGILSPSPALLPTASTGIFSPMYHQGGMFSPAIPLGLFSPAGFMSPFRSPGFTSLVASPTFADFFSHIWDQ', 'MSSKLPKKSEENLELPTFPASEESLSRSEKLKYVFVRNPFIPLGCLMTVGTFLASGYYIRRENHLMANKFMRYRVMSQGFTLAALAFSVLFIGPPRREAPSNSSGSINSEIK', 'MVNLGNAVRSLLMHLIGLLVWQFDISISPVAAIVTDTFNSSDGGRLFQFPDGVQNWPALSIVVIIIMTIGGNILVIMAVSMEKKLHNATNYFLMSLAIADMLVGLLVMPLSLLAILYDYVWPLPRYLCPVWISLDVLFSTASIMHLCAISLDRYVAIRNPIEHSRFNSRTKAIMKIAIVWAISIGVSVPIPVIGLRDESKVFVNNTTCVLNDPNFVLIGSFVAFFIPLTIMVITYFLTIYVLRRQTLMLLRGHTEEELANMSLNFLNCCCKKNGGEEENAPNPNPDQKPRRKKKEKRPRGTMQAINNEKKASKVLGIVFFVFLIMWCPFFITNILSVLCGKACNQKLMEKLLNVFVWIGYVCSGINPLVYTLFNKIYRRAFSKYLRCDYKPDKKPPVRQIPRVAATALSGRELNVNIYRHTNERVARKANDPEPGIEMQVENLELPVNPSNVVSERISS', 'MAPCRTMMVLLCFVLFLAASSSCVAAARIGATMEMKKNIKRLTFKNSHIFGYLPKGVPIPPSAPSKRHNSFVNSLP', 'MASLAQQLAGGLRCPPLSNSNLSKPFSPKHTLKPRFSPIVSAVAVSNAQTRERQKLKQLFEDAYERCRNAPMEGVSFTIDDFHTALDKYDFNSEMGSRVKGTVFCTDANGALVDITAKSSAYLPLAEACIYRIKNVEEAGIIPGVREEFVIIGENEADDSLILSLRQIQYELAWERCRQLQAEDVVVKGKIVGANKGGVVALVEGLRGFVPFSQISSKSSAEELLEKEIPLKFVEVDEEQSRLVMSNRKAMADSQAQLGIGSVVTGTVQSLKPYGAFIDIGGINGLLHVSQISHDRVSDIATVLQPGDTLKVMILSHDRERGRVSLSTKKLEPTPGDMIRNPKLVFEKAEEMAQTFRQRIAQAEAMARADMLRFQPESGLTLSSDGILGPLTSDLPAEGLDLSVVPPAVE'), ('', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       dtype=torch.float64), [tensor([[ 0.3601,  0.5185, -0.8991,  ...,  0.2122,  0.5215,  0.2403],\n",
      "        [ 0.3283,  0.4804, -0.8103,  ...,  0.3416,  0.5335,  0.2967],\n",
      "        [ 0.2632,  0.4100, -0.6313,  ...,  0.5729,  0.5462,  0.4238],\n",
      "        ...,\n",
      "        [ 0.1564,  0.2851, -0.3136,  ...,  0.9764,  0.6207,  0.6323],\n",
      "        [ 0.3552,  0.5103, -0.8845,  ...,  0.2320,  0.5267,  0.2502],\n",
      "        [ 0.2037,  0.3384, -0.4518,  ...,  0.8273,  0.5885,  0.5295]]), tensor([[ 0.3438,  0.8455, -0.5087,  ...,  2.7631,  1.8673,  1.2842],\n",
      "        [ 0.2306,  0.8265, -0.3482,  ...,  2.1132,  1.2660,  1.4791],\n",
      "        [ 0.0935,  0.7628, -0.4200,  ...,  1.7653,  1.3132,  1.2090],\n",
      "        ...,\n",
      "        [ 0.1551,  0.5208, -0.0616,  ...,  1.1085,  0.9847,  0.7215],\n",
      "        [ 0.3164,  0.8481, -0.5176,  ...,  2.0319,  1.5185,  1.2667],\n",
      "        [ 0.1375,  0.6109, -0.2062,  ...,  1.2735,  1.1829,  0.9147]]), tensor([[ 0.2205,  0.7222, -0.5038,  ...,  2.6590,  1.6537,  0.9145],\n",
      "        [ 0.2446,  0.8905, -0.1342,  ...,  1.7197,  1.2963,  1.1148],\n",
      "        [-0.3177,  0.7723, -0.2838,  ...,  1.5125,  1.2950,  0.7556],\n",
      "        ...,\n",
      "        [ 0.1089,  0.4643, -0.1341,  ...,  1.0225,  0.7668,  0.4445],\n",
      "        [ 0.1214,  0.3843, -0.3447,  ...,  2.1447,  1.4803,  0.7525],\n",
      "        [ 0.0507,  0.6036, -0.2908,  ...,  0.9738,  1.0894,  0.6777]]), tensor([[-0.3558, -0.2455, -0.7280,  ...,  2.4676,  1.6494,  0.6347],\n",
      "        [-0.3066, -0.5293, -0.1811,  ...,  2.1088,  1.2837,  0.7973],\n",
      "        [-0.5508, -0.2309, -0.1907,  ...,  1.7663,  1.1925,  0.7387],\n",
      "        ...,\n",
      "        [-0.1602, -0.1659, -0.0343,  ...,  0.9068,  0.8665,  0.3520],\n",
      "        [-0.0557, -0.4557, -0.1636,  ...,  2.1657,  1.2342,  0.3647],\n",
      "        [-0.1875, -0.1985, -0.1727,  ...,  1.1516,  0.9022,  0.5808]]), tensor([[ 1.0373,  0.0056, -0.5625,  ...,  2.1196,  1.0956,  0.3914],\n",
      "        [ 0.6481, -0.3093, -0.1902,  ...,  1.9483,  1.1084,  0.5432],\n",
      "        [ 0.2597, -0.2409, -0.0613,  ...,  1.6403,  1.0353,  0.4620],\n",
      "        ...,\n",
      "        [ 0.3141, -0.5637,  0.1471,  ...,  0.6898,  0.7237,  0.4194],\n",
      "        [ 1.4790, -0.1637, -0.3771,  ...,  2.0989,  1.5257,  0.6442],\n",
      "        [ 0.6531, -0.1907, -0.1253,  ...,  1.2172,  0.9117,  0.5574]]), tensor([[ 0.6868, -0.3298,  0.2181,  ...,  2.1515,  1.8178,  0.9526],\n",
      "        [ 1.0968,  0.0884,  0.1255,  ...,  1.3761,  1.4362,  0.9102],\n",
      "        [ 0.2940, -0.6977,  0.6388,  ...,  0.6689,  0.8242,  0.9503],\n",
      "        ...,\n",
      "        [ 0.3624, -0.4118, -0.2089,  ...,  0.8404,  0.4797,  0.3217],\n",
      "        [ 1.2882, -0.6133,  1.4731,  ...,  1.9205,  1.6548,  0.1904],\n",
      "        [ 0.4796, -0.5403,  0.2434,  ...,  1.3918,  0.7522,  0.5041]]), tensor([[ 0.2699, -0.3215, -0.7027,  ...,  1.5120,  1.7442,  1.4779],\n",
      "        [ 2.1171, -0.2033, -0.3648,  ...,  1.0312,  1.6023,  1.3150],\n",
      "        [ 0.1769, -0.7384,  0.4121,  ..., -0.1904,  1.2317,  1.5532],\n",
      "        ...,\n",
      "        [ 0.0981, -0.4639, -0.4277,  ...,  0.2812,  0.8953,  0.2785],\n",
      "        [ 0.6522, -0.9399,  0.5812,  ...,  0.8662,  1.6527,  0.9701],\n",
      "        [ 0.7879, -0.5006, -0.4116,  ...,  0.3083,  1.2053,  0.7391]])]]\n",
      "tensor([[ 0.3601,  0.5185, -0.8991,  ...,  0.2122,  0.5215,  0.2403],\n",
      "        [ 0.3283,  0.4804, -0.8103,  ...,  0.3416,  0.5335,  0.2967],\n",
      "        [ 0.2632,  0.4100, -0.6313,  ...,  0.5729,  0.5462,  0.4238],\n",
      "        ...,\n",
      "        [ 0.1564,  0.2851, -0.3136,  ...,  0.9764,  0.6207,  0.6323],\n",
      "        [ 0.3552,  0.5103, -0.8845,  ...,  0.2320,  0.5267,  0.2502],\n",
      "        [ 0.2037,  0.3384, -0.4518,  ...,  0.8273,  0.5885,  0.5295]]) tensor([[4],\n",
      "        [9],\n",
      "        [1],\n",
      "        [9],\n",
      "        [1],\n",
      "        [7],\n",
      "        [5],\n",
      "        [7],\n",
      "        [8],\n",
      "        [1],\n",
      "        [1],\n",
      "        [7],\n",
      "        [3],\n",
      "        [1],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7],\n",
      "        [6],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7],\n",
      "        [9],\n",
      "        [6],\n",
      "        [1],\n",
      "        [0],\n",
      "        [9],\n",
      "        [2],\n",
      "        [3],\n",
      "        [9],\n",
      "        [7],\n",
      "        [7],\n",
      "        [0],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [9],\n",
      "        [7],\n",
      "        [6],\n",
      "        [1],\n",
      "        [7],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [7],\n",
      "        [3],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [9],\n",
      "        [2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [7],\n",
      "        [6],\n",
      "        [0],\n",
      "        [3],\n",
      "        [9]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=0'>1</a>\u001b[0m Run_Pytorch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=1'>2</a>\u001b[0m         dataset_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/annotation/scl/balanced.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=2'>3</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mesm1_t6_43M_UR50S\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=3'>4</a>\u001b[0m         reset_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=4'>5</a>\u001b[0m         resample_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=5'>6</a>\u001b[0m         embed_batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=6'>7</a>\u001b[0m         flatten_emb \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=7'>8</a>\u001b[0m         embed_folder\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39membeddings/annotation/scl/balanced\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=8'>9</a>\u001b[0m         seq_start_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=9'>10</a>\u001b[0m         seq_end_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=10'>11</a>\u001b[0m         loader_batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=11'>12</a>\u001b[0m         worker_seed\u001b[39m=\u001b[39;49m RAND_SEED,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=12'>13</a>\u001b[0m         if_encode_all \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=13'>14</a>\u001b[0m         learning_rate \u001b[39m=\u001b[39;49m \u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=14'>15</a>\u001b[0m         lr_decay \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=15'>16</a>\u001b[0m         epochs \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=16'>17</a>\u001b[0m         early_stop \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=17'>18</a>\u001b[0m         tolerance \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=18'>19</a>\u001b[0m         min_epoch \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=19'>20</a>\u001b[0m         device \u001b[39m=\u001b[39;49m DEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=20'>21</a>\u001b[0m         all_plot_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mresults/learning_curves\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=21'>22</a>\u001b[0m         all_result_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mresults/train_val_test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=22'>23</a>\u001b[0m         \u001b[39m# **encoder_params\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=23'>24</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 18\u001b[0m in \u001b[0;36mRun_Pytorch.__init__\u001b[0;34m(self, dataset_path, encoder_name, reset_param, resample_param, embed_batch_size, flatten_emb, embed_folder, seq_start_idx, seq_end_idx, loader_batch_size, worker_seed, if_encode_all, if_multiprocess, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, device, all_plot_folder, all_result_folder, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=153'>154</a>\u001b[0m \u001b[39mfor\u001b[39;00m embed_layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_emb_layer):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=154'>155</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning pytorch model for layer \u001b[39m\u001b[39m{\u001b[39;00membed_layer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=155'>156</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_pytorch_layer(embed_layer)\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 18\u001b[0m in \u001b[0;36mRun_Pytorch.run_pytorch_layer\u001b[0;34m(self, embed_layer)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=173'>174</a>\u001b[0m model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=175'>176</a>\u001b[0m criterion\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=177'>178</a>\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m train(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=178'>179</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=179'>180</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=180'>181</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=181'>182</a>\u001b[0m     val_loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_val_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=182'>183</a>\u001b[0m     encoder_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=183'>184</a>\u001b[0m     embed_layer\u001b[39m=\u001b[39;49membed_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=184'>185</a>\u001b[0m     reset_param\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=185'>186</a>\u001b[0m     resample_param\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resample_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=186'>187</a>\u001b[0m     embed_batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embed_batch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=187'>188</a>\u001b[0m     flatten_emb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flatten_emb,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=188'>189</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=189'>190</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learning_rate,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=190'>191</a>\u001b[0m     lr_decay\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lr_decay,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=191'>192</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_epochs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=192'>193</a>\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_early_stop,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=193'>194</a>\u001b[0m     tolerance\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tolerance,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=194'>195</a>\u001b[0m     min_epoch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_min_epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=195'>196</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=196'>197</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=198'>199</a>\u001b[0m \u001b[39m# record the losses\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=199'>200</a>\u001b[0m result_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=200'>201</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlosses\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mtrain_losses\u001b[39m\u001b[39m\"\u001b[39m: train_losses, \u001b[39m\"\u001b[39m\u001b[39mval_losses\u001b[39m\u001b[39m\"\u001b[39m: val_losses}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=201'>202</a>\u001b[0m }\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, train_loader, val_loader, encoder_name, embed_layer, reset_param, resample_param, embed_batch_size, flatten_emb, device, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=205'>206</a>\u001b[0m min_val_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mInf\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=207'>208</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=209'>210</a>\u001b[0m     train_losses[epoch] \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=210'>211</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=211'>212</a>\u001b[0m         loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=212'>213</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49mencoder_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=213'>214</a>\u001b[0m         embed_layer\u001b[39m=\u001b[39;49membed_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=214'>215</a>\u001b[0m         reset_param\u001b[39m=\u001b[39;49mreset_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=215'>216</a>\u001b[0m         resample_param\u001b[39m=\u001b[39;49mresample_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=216'>217</a>\u001b[0m         embed_batch_size\u001b[39m=\u001b[39;49membed_batch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=217'>218</a>\u001b[0m         flatten_emb\u001b[39m=\u001b[39;49mflatten_emb,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=218'>219</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=219'>220</a>\u001b[0m         criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=220'>221</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=221'>222</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=222'>223</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=224'>225</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=225'>226</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=226'>227</a>\u001b[0m         loader\u001b[39m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=236'>237</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=237'>238</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=238'>239</a>\u001b[0m     val_losses[epoch] \u001b[39m=\u001b[39m val_loss\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb Cell 18\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, encoder_name, embed_layer, reset_param, resample_param, embed_batch_size, flatten_emb, device, criterion, optimizer, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=141'>142</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m criterion \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=144'>145</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, y\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=146'>147</a>\u001b[0m     \u001b[39mif\u001b[39;00m optimizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/annotation_pytorch.ipynb#ch0000038vscode-remote?line=147'>148</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/modules/loss.py:1047\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   1046\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, Tensor)\n\u001b[0;32m-> 1047\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1048\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/functional.py:2690\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2688\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2689\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2690\u001b[0m \u001b[39mreturn\u001b[39;00m nll_loss(log_softmax(\u001b[39minput\u001b[39;49m, \u001b[39m1\u001b[39;49m), target, weight, \u001b[39mNone\u001b[39;49;00m, ignore_index, \u001b[39mNone\u001b[39;49;00m, reduction)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/nn/functional.py:2385\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2381\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2382\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected input batch_size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) to match target batch_size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), target\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m   2383\u001b[0m     )\n\u001b[1;32m   2384\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 2385\u001b[0m     ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n\u001b[1;32m   2386\u001b[0m \u001b[39melif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m   2387\u001b[0m     ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mnll_loss2d(\u001b[39minput\u001b[39m, target, weight, _Reduction\u001b[39m.\u001b[39mget_enum(reduction), ignore_index)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "Run_Pytorch(\n",
    "        dataset_path=\"data/annotation/scl/balanced.csv\",\n",
    "        encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "        reset_param = False,\n",
    "        resample_param = False,\n",
    "        embed_batch_size = 128,\n",
    "        flatten_emb = \"mean\",\n",
    "        embed_folder= \"embeddings/annotation/scl/balanced\",\n",
    "        seq_start_idx = False,\n",
    "        seq_end_idx = False,\n",
    "        loader_batch_size = 64,\n",
    "        worker_seed= RAND_SEED,\n",
    "        if_encode_all = False,\n",
    "        learning_rate = 1e-4,\n",
    "        lr_decay = 0.1,\n",
    "        epochs = 2,\n",
    "        early_stop = True,\n",
    "        tolerance = 10,\n",
    "        min_epoch = 5,\n",
    "        device = DEVICE,\n",
    "        all_plot_folder = \"results/learning_curves\",\n",
    "        all_result_folder = \"results/train_val_test\",\n",
    "        # **encoder_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 2/27 [00:00<00:01, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27/27 [00:01<00:00, 13.96it/s]\n"
     ]
    }
   ],
   "source": [
    "val_ds = ProtranDataset(\n",
    "                dataset_path=\"data/annotation/scl/balanced.csv\",\n",
    "                subset=\"\",\n",
    "                encoder_name=\"\",\n",
    "                reset_param=False,\n",
    "                resample_param=False,\n",
    "                embed_batch_size=64,\n",
    "                flatten_emb=\"flatten\",\n",
    "                embed_path=None,\n",
    "                seq_start_idx=0,\n",
    "                seq_end_idx=False,\n",
    "                # **encoder_params,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:00<00:00, 177.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gb1_val = ProtranDataset(\n",
    "                dataset_path=\"data/proeng/gb1/sampled.csv\",\n",
    "                subset=\"val\",\n",
    "                encoder_name=\"\",\n",
    "                reset_param=False,\n",
    "                resample_param=False,\n",
    "                embed_batch_size=64,\n",
    "                flatten_emb=\"flatten\",\n",
    "                embed_path=None,\n",
    "                seq_start_idx=0,\n",
    "                seq_end_idx=False,\n",
    "                # **encoder_params,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, C=0.1, multi_class=\"multinomial\", max_iter=1000).fit(val_ds.layer0, y)\n",
    "pred_y = clf.predict(val_ds.layer0)\n",
    "pred_prob = clf.predict_proba(val_ds.layer0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.097391642777984, 1.0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, pred_y), log_loss(y, pred_prob), roc_auc_score(y, pred_prob, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for run sklearn models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.utils import get_folder_file_names, pickle_save, ndcg_scale\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO, MAX_SEQ_LEN\n",
    "from scr.params.sys import RAND_SEED, SKLEARN_ALPHAS\n",
    "from scr.encoding.encoding_classes import ESMEncoder, CARPEncoder, OnehotEncoder\n",
    "from scr.preprocess.data_process import ProtranDataset\n",
    "\n",
    "# seed\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "\n",
    "\n",
    "class RunSK:\n",
    "    \"\"\"A class for running sklearn models\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_path: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        alphas: np.ndarray | int = SKLEARN_ALPHAS,\n",
    "        sklearn_state: int = RAND_SEED,\n",
    "        sklearn_params: dict | None = None,\n",
    "        all_result_folder: str = \"results/sklearn\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_path: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - alphas: np.ndarray, arrays of alphas to be tested\n",
    "        - sklearn_state: int = RAND_SEED, seed the ridge or logistic regression\n",
    "        - sklearn_params: dict | None = None, other ridge or logistic regression args\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.encoder_name = encoder_name\n",
    "        self.reset_param = reset_param\n",
    "        self.resample_param = resample_param\n",
    "        self.flatten_emb = flatten_emb\n",
    "\n",
    "        if not isinstance(alphas, np.ndarray):\n",
    "            alphas = np.array([alphas])\n",
    "        self.alphas = alphas\n",
    "\n",
    "        self.sklearn_state = sklearn_state\n",
    "        self.sklearn_params = sklearn_params\n",
    "        self.all_result_folder = all_result_folder\n",
    "\n",
    "        if self.reset_param and \"-rand\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-rand\"\n",
    "\n",
    "        if self.resample_param and \"-stat\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-stat\"\n",
    "\n",
    "        # loader has ALL embedding layers\n",
    "        self.train_ds, self.val_ds, self.test_ds = (\n",
    "            ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_path=embed_path,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                **encoder_params,\n",
    "            )\n",
    "            for subset in [\"train\", \"val\", \"test\"]\n",
    "        )\n",
    "\n",
    "        # pick ridge regression if y numerical\n",
    "        if self.val_ds.y.dtype.kind in \"iufc\":\n",
    "            self.sklearn_model = Ridge\n",
    "\n",
    "        # pick logistic regression if y is categorical\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            self.train_ds.y, self.val_ds.y, self.test_ds.y = [\n",
    "                le.fit_transform(y.flatten())\n",
    "                for y in [self.train_ds.y, self.val_ds.y, self.test_ds.y]\n",
    "            ]\n",
    "            self.sklearn_model = LogisticRegression\n",
    "            # convert alpha to C\n",
    "            self.alphas = 1 / self.alphas\n",
    "            # add other params\n",
    "            if self.sklearn_params is None:\n",
    "                self.sklearn_params[\"multi_class\"] = \"multinomial\"\n",
    "                self.sklearn_params[\"max_iter\"] = 1000\n",
    "\n",
    "        all_sklearn_results = {}\n",
    "\n",
    "        # TODO for easier total_emb_layer\n",
    "        if self.encoder_name in TRANSFORMER_INFO.keys():\n",
    "            total_emb_layer = TRANSFORMER_INFO[encoder_name][1] + 1\n",
    "        elif self.encoder_name in CARP_INFO.keys():\n",
    "            total_emb_layer = CARP_INFO[encoder_name][1]\n",
    "        else:\n",
    "            # for onehot\n",
    "            self.encoder_name = \"onehot\"\n",
    "            total_emb_layer = 1\n",
    "\n",
    "        for layer in range(total_emb_layer):\n",
    "            all_sklearn_results[layer] = self.run_sklearn_layer(embed_layer=layer,)\n",
    "\n",
    "        self._all_sklearn_results = all_sklearn_results\n",
    "\n",
    "    def sk_test(\n",
    "        self, model: sklearn.linear_model, ds: ProtranDataset, embed_layer: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for testing sklearn models for a specific layer of embeddings\n",
    "\n",
    "        Args:\n",
    "        - model: sklearn.linear_model, trained model\n",
    "        - ds: ProtranDataset, train, val, or test dataset\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - np.concatenate(pred): np.ndarray, 1D predicted fitness values\n",
    "        - np.concatenate(true): np.ndarry, 1D true fitness values\n",
    "        - \n",
    "        \"\"\"\n",
    "\n",
    "        if self.sklearn_model == Ridge:\n",
    "            pred_prob = None\n",
    "        else:\n",
    "            pred_prob = model.predict_proba(\n",
    "                getattr(ds, \"layer\" + str(embed_layer)).cpu().numpy()\n",
    "            ).squeeze()\n",
    "\n",
    "        return (\n",
    "            model.predict(\n",
    "                getattr(ds, \"layer\" + str(embed_layer)).cpu().numpy()\n",
    "            ).squeeze(),\n",
    "            ds.y.squeeze(),\n",
    "            pred_prob,\n",
    "        )\n",
    "\n",
    "    def pick_model(\n",
    "        self, embed_layer: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for picking the best model for given alaphs, meaning\n",
    "        lower train_mse and higher test_ndcg\n",
    "        NOTE: alphas tuning is NOT currently optimal\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - sklearn.linear_model, the model with the best alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # init values for comparison\n",
    "        if self.sklearn_model == Ridge:\n",
    "            best_mse = np.Inf\n",
    "            best_ndcg = -1\n",
    "            best_rho = -1\n",
    "        else:\n",
    "            best_loss = np.Inf\n",
    "            best_acc = 0\n",
    "            best_auc = 0\n",
    "\n",
    "        best_model = None\n",
    "\n",
    "        # loop through all alphas\n",
    "        for alpha in self.alphas:\n",
    "\n",
    "            # init model for each alpha\n",
    "            if self.sklearn_params is None:\n",
    "                self.sklearn_params = {}\n",
    "            model = self.sklearn_model(\n",
    "                alpha=alpha, random_state=self.sklearn_state, **self.sklearn_params\n",
    "            )\n",
    "\n",
    "            # fit the model for a given layer of embedding\n",
    "            fitness_scaler = StandardScaler()\n",
    "            model.fit(\n",
    "                getattr(self.train_ds, \"layer\" + str(embed_layer)).cpu().numpy(),\n",
    "                fitness_scaler.fit_transform(self.train_ds.y),\n",
    "            )\n",
    "\n",
    "            # eval the model with train and test\n",
    "            train_pred, train_true, train_prob = self.sk_test(\n",
    "                model, self.train_ds, embed_layer=embed_layer\n",
    "            )\n",
    "            val_pred, val_true, val_prob = self.sk_test(\n",
    "                model, self.val_ds, embed_layer=embed_layer\n",
    "            )\n",
    "\n",
    "            if self.sklearn_model == Ridge:\n",
    "                # calc the metrics\n",
    "                train_mse = mean_squared_error(train_true, train_pred)\n",
    "                val_ndcg = ndcg_scale(val_true, val_pred)\n",
    "                val_rho = spearmanr(val_true, val_pred)[0]\n",
    "\n",
    "                # update the model if it has lower train_mse and higher val_ndcg\n",
    "                if train_mse < best_mse and val_ndcg > best_ndcg:\n",
    "                    best_model = model\n",
    "                    best_mse = train_mse\n",
    "                    best_ndcg = val_ndcg\n",
    "                    best_rho = val_rho\n",
    "\n",
    "            else:\n",
    "                # calc the metrics\n",
    "                train_loss = log_loss(train_true, train_prob)\n",
    "                val_acc = accuracy_score(val_true, val_pred)\n",
    "                val_auc = roc_auc_score(val_true, val_prob, multi_class=\"ovo\")\n",
    "\n",
    "                # update the model if it has lower log_loss and higher val_auc\n",
    "                if train_loss < best_loss and val_auc > best_auc:\n",
    "                    best_loss = train_loss\n",
    "                    best_acc = val_acc\n",
    "                    best_auc = val_auc\n",
    "\n",
    "        print(f\"best model is {best_model}\")\n",
    "        return best_model\n",
    "\n",
    "    def run_sklearn_layer(\n",
    "        self, embed_layer: int,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running ridge or logistics regression for a given layer of embedding\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"train\": {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"\"\"\n",
    "\n",
    "        # train and get the best alpha\n",
    "        best_model = self.pick_model(embed_layer=embed_layer,)\n",
    "\n",
    "        # init dict for resulted outputs\n",
    "        result_dict = {}\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, ds in zip(\n",
    "            [\"train\", \"val\", \"test\"], [self.train_ds, self.val_ds, self.test_ds],\n",
    "        ):\n",
    "            pred, true, prob = self.sk_test(best_model, ds, embed_layer=embed_layer)\n",
    "\n",
    "            if self.sklearn_model == Ridge:\n",
    "                result_dict[subset] = {\n",
    "                    \"mse\": mean_squared_error(true, pred),\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"ndcg\": ndcg_scale(true, pred),\n",
    "                    \"rho\": spearmanr(true, pred),\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                result_dict[subset] = {\n",
    "                    \"log\": log_loss(true, prob),\n",
    "                    \"pred\": pred,\n",
    "                    \"prob\": prob,\n",
    "                    \"true\": true,\n",
    "                    \"acc\": accuracy_score(true, pred),\n",
    "                    \"rocauc\": roc_auc_score(true, prob, multi_class=\"ovo\"),\n",
    "                }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=self.all_result_folder,\n",
    "            dataset_path=self.dataset_path,\n",
    "            encoder_name=self.encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self.flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    @property\n",
    "    def all_sklearn_results(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"layer#\": {\n",
    "                        \"train\": {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"val\":   {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"test\":  {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        }\n",
    "        \"\"\"\n",
    "        return self._all_sklearn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      " 50%|     | 74/149 [23:26<23:58, 19.18s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "scl = RunSK(\n",
    "        dataset_path=\"data/annotation/scl/balanced.csv\",\n",
    "        encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "        reset_param=False,\n",
    "        resample_param = False,\n",
    "        embed_batch_size = 64,\n",
    "        flatten_emb = \"flatten\",\n",
    "        embed_path = None,\n",
    "        seq_start_idx = False,\n",
    "        seq_end_idx = False,\n",
    "        alphas = SKLEARN_ALPHAS,\n",
    "        sklearn_state = RAND_SEED,\n",
    "        sklearn_params = None,\n",
    "        all_result_folder = \"results/sklearn-test\",\n",
    "        # **encoder_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
