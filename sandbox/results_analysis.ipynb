{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.analysis.perlayer import LayerLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n"
     ]
    }
   ],
   "source": [
    "if defaultdict(dict):\n",
    "    print(\"not empty\")\n",
    "else:\n",
    "    print(\"empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyzing per layer output\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "from scr.utils import pickle_load, get_filename, checkNgen_folder\n",
    "\n",
    "\n",
    "class LayerLoss:\n",
    "    \"\"\"A class for handling layer analysis\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_path: str = \"results/sklearn\",\n",
    "        output_path: str = \"results/sklearn_layer\",\n",
    "        metric_dict: dict[list[str]] = {\n",
    "            \"proeng\": [\"train_mse\", \"test_ndcg\", \"test_rho\"],\n",
    "            \"annotation\": [\"train_cross-entropy\", \"test_acc\", \"test_rocauc\"],\n",
    "        },\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - input_path: str = \"results/sklearn\",\n",
    "        - output_path: str = \"results/sklearn_layer\"\n",
    "        - metric_dict: list[str] = [\"train_mse\", \"test_ndcg\", \"test_rho\"]\n",
    "        \"\"\"\n",
    "        # get rid of the last \"/\" if any\n",
    "        self._input_path = os.path.normpath(input_path)\n",
    "        # get the list of subfolders for each dataset\n",
    "        self._dataset_folders = glob(f\"{self._input_path}/*/*/*/*/*\")\n",
    "        # glob(\"results/train_val_test/*/*/*/*/*\")\n",
    "\n",
    "        # get rid of the last \"/\" if any\n",
    "        self._output_path = os.path.normpath(output_path)\n",
    "        self._metric_dict = metric_dict\n",
    "\n",
    "        # init a dictionary for recording outputs\n",
    "        self._onehot_baseline_dict = defaultdict(dict)\n",
    "        self._layer_analysis_dict = defaultdict(dict)\n",
    "        self._rand_layer_analysis_dict = defaultdict(dict)\n",
    "        self._stat_layer_analysis_dict = defaultdict(dict)\n",
    "\n",
    "        for dataset_folder in self._dataset_folders:\n",
    "            # dataset_folder = \"results/train_val_test/proeng/gb1/two_vs_rest/esm1b_t33_650M_UR50S/max\"\n",
    "            # get the details for the dataset such as proeng/gb1/two_vs_rest\n",
    "            task_subfolder = dataset_folder.split(self._input_path + \"/\")[-1]\n",
    "            # task_subfolder = \"proeng/gb1/two_vs_rest/esm1b_t33_650M_UR50S/max\"\n",
    "            task, dataset, split, encoder_name, flatten_emb = task_subfolder.split(\"/\")\n",
    "\n",
    "            # get number of metircs\n",
    "            metric_numb = len(self._metric_dict[task])\n",
    "\n",
    "            # parse results for plotting the collage and onehot\n",
    "            self._layer_analysis_dict[f\"{task}_{dataset}_{split}_{flatten_emb}\"][\n",
    "                encoder_name\n",
    "            ] = self.parse_result_dicts(\n",
    "                dataset_folder, task, dataset, split, encoder_name, flatten_emb\n",
    "            )\n",
    "\n",
    "            # check if reset param experimental results exist\n",
    "            reset_param_path = f\"{self._input_path}-rand\"\n",
    "\n",
    "            if os.path.exists(reset_param_path):\n",
    "                self._rand_layer_analysis_dict[\n",
    "                    f\"{task}_{dataset}_{split}_{flatten_emb}\"\n",
    "                ][encoder_name] = self.parse_result_dicts(\n",
    "                    dataset_folder.replace(self._input_path, reset_param_path),\n",
    "                    task,\n",
    "                    dataset,\n",
    "                    split,\n",
    "                    encoder_name,\n",
    "                    flatten_emb,\n",
    "                )\n",
    "                add_rand = True\n",
    "            else:\n",
    "                add_rand = False\n",
    "\n",
    "            # check if resample param experimental results exist\n",
    "            resample_param_path = f\"{self._input_path}-stat\"\n",
    "\n",
    "            if os.path.exists(resample_param_path):\n",
    "                self._stat_layer_analysis_dict[\n",
    "                    f\"{task}_{dataset}_{split}_{flatten_emb}\"\n",
    "                ][encoder_name] = self.parse_result_dicts(\n",
    "                    dataset_folder.replace(self._input_path, resample_param_path),\n",
    "                    task,\n",
    "                    dataset,\n",
    "                    split,\n",
    "                    encoder_name,\n",
    "                    flatten_emb,\n",
    "                )\n",
    "                add_stat = True\n",
    "            else:\n",
    "                add_stat = False\n",
    "\n",
    "            # check if resample param experimental results exist\n",
    "            onehot_path = f\"{self._input_path}-onehot\"\n",
    "\n",
    "            if os.path.exists(onehot_path):\n",
    "                self._onehot_baseline_dict[\n",
    "                    f\"{task}_{dataset}_{split}\"\n",
    "                ] = self.parse_result_dicts(\n",
    "                    dataset_folder.replace(self._input_path, onehot_path)\n",
    "                    .replace(encoder_name, \"onehot\")\n",
    "                    .replace(flatten_emb, \"flatten\"),\n",
    "                    task,\n",
    "                    dataset,\n",
    "                    split,\n",
    "                    \"onehot\",\n",
    "                    \"flatten\",\n",
    "                )\n",
    "                add_onehot = True\n",
    "            else:\n",
    "                add_onehot = False\n",
    "\n",
    "        collage_folder = os.path.join(self._output_path, \"collage\")\n",
    "        checkNgen_folder(collage_folder)\n",
    "\n",
    "        for collage_name, encoder_dict in self._layer_analysis_dict.items():\n",
    "\n",
    "            onehot_name = \"_\".join(collage_name.split(\"_\")[:-1])\n",
    "\n",
    "            if set(list(TRANSFORMER_INFO.keys())) == set(encoder_dict.keys()):\n",
    "                # set the key rankings to default\n",
    "                encoder_names = list(TRANSFORMER_INFO.keys())\n",
    "                encoder_label = \"esm\"\n",
    "            elif set(list(CARP_INFO.keys())) == set(encoder_dict.keys()):\n",
    "                # set the key rankings to default\n",
    "                encoder_names = list(CARP_INFO.keys())\n",
    "                encoder_label = \"carp\"\n",
    "            else:\n",
    "                encoder_names = list(set(encoder_dict.keys()))\n",
    "\n",
    "            fig, axs = plt.subplots(\n",
    "                metric_numb,\n",
    "                len(encoder_names),\n",
    "                sharey=\"row\",\n",
    "                sharex=\"col\",\n",
    "                figsize=(20, 10),\n",
    "            )\n",
    "            for m, metric in enumerate(self._metric_dict[task]):\n",
    "                for n, encoder_name in enumerate(encoder_names):\n",
    "                    axs[m, n].plot(\n",
    "                        encoder_dict[encoder_name][metric], label=encoder_label\n",
    "                    )\n",
    "\n",
    "                    # overlay onehot baseline\n",
    "                    if add_onehot:\n",
    "                        axs[m, n].axhline(\n",
    "                            self._onehot_baseline_dict[onehot_name][metric],\n",
    "                            label=\"onehot\",\n",
    "                            color=\"#D3D3D3\",  # light grey\n",
    "                            linestyle=\"dotted\",\n",
    "                        )\n",
    "\n",
    "                    # overlay random init\n",
    "                    if add_rand:\n",
    "                        axs[m, n].plot(\n",
    "                            self._rand_layer_analysis_dict[collage_name][encoder_name][\n",
    "                                metric\n",
    "                            ],\n",
    "                            label=\"random init\",\n",
    "                            color=\"#D3D3D3\",  # light grey\n",
    "                        )\n",
    "\n",
    "                    # overlay stat init\n",
    "                    if add_stat:\n",
    "                        axs[m, n].plot(\n",
    "                            self._stat_layer_analysis_dict[collage_name][encoder_name][\n",
    "                                metric\n",
    "                            ],\n",
    "                            label=\"stat transfer\",\n",
    "                            color=\"#A9A9A9\",  # dark grey\n",
    "                            # linestyle=\"dotted\",\n",
    "                        )\n",
    "\n",
    "            # add xlabels\n",
    "            for ax in axs[metric_numb - 1]:\n",
    "                ax.set_xlabel(\"layers\", fontsize=16)\n",
    "                ax.tick_params(axis=\"x\", labelsize=16)\n",
    "\n",
    "            # add column names\n",
    "            for ax, col in zip(axs[0], encoder_names):\n",
    "                ax.set_title(col, fontsize=16)\n",
    "\n",
    "            # add row names\n",
    "            for ax, row in zip(axs[:, 0], self._metric_dict[task]):\n",
    "                ax.set_ylabel(row.replace(\"_\", \" \"), fontsize=16)\n",
    "                ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "            # set the plot yticks\n",
    "            plt.gca().yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n",
    "\n",
    "            # add legend\n",
    "            handles, labels = axs[0, 0].get_legend_handles_labels()\n",
    "            fig.legend(\n",
    "                handles,\n",
    "                labels,\n",
    "                loc=\"upper left\",\n",
    "                bbox_to_anchor=[0.05, 1.025],\n",
    "                fontsize=16,\n",
    "                frameon=False,\n",
    "                ncol=2,\n",
    "            )\n",
    "\n",
    "            # add whole plot level title\n",
    "            fig.suptitle(\n",
    "                collage_name.replace(\"_\", \" \"), y=1.0025, fontsize=24, fontweight=\"bold\"\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "\n",
    "            for plot_ext in [\".svg\", \".png\"]:\n",
    "                plt.savefig(\n",
    "                    os.path.join(collage_folder, collage_name + plot_ext),\n",
    "                    bbox_inches=\"tight\",\n",
    "                )\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    def parse_result_dicts(\n",
    "        self,\n",
    "        folder_path: str,\n",
    "        task: str,\n",
    "        dataset: str,\n",
    "        split: str,\n",
    "        encoder_name: str,\n",
    "        flatten_emb: bool | str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parse the output result dictionaries for plotting\n",
    "\n",
    "        Args:\n",
    "        - folder_path: str, the folder path for the datasets\n",
    "\n",
    "        Returns:\n",
    "        - dict, encode name as key with a dict as its value\n",
    "            where metric name as keys and the array of losses as values\n",
    "        - str, details for collage plot\n",
    "        \"\"\"\n",
    "\n",
    "        # get the list of output pickle files\n",
    "        pkl_list = glob(f\"{folder_path}/*.pkl\")\n",
    "\n",
    "        # get the max layer number for the array\n",
    "        if encoder_name in TRANSFORMER_INFO.keys():\n",
    "            max_layer_numb = TRANSFORMER_INFO[encoder_name][1] + 1\n",
    "        elif encoder_name in CARP_INFO.keys():\n",
    "            max_layer_numb = CARP_INFO[encoder_name][1]\n",
    "        else:\n",
    "            max_layer_numb = 1\n",
    "\n",
    "        # init the ouput dict\n",
    "        output_numb_dict = {\n",
    "            metric: np.zeros([max_layer_numb]) for metric in self._metric_dict[task]\n",
    "        }\n",
    "\n",
    "        # loop through the list of the pickle files\n",
    "        for pkl_file in pkl_list:\n",
    "            # get the layer number\n",
    "            layer_numb = int(get_filename(pkl_file).split(\"-\")[-1].split(\"_\")[-1])\n",
    "            # load the result dictionary\n",
    "            result_dict = pickle_load(pkl_file)\n",
    "\n",
    "            # populate the processed dictionary\n",
    "            for metric in self._metric_dict[task]:\n",
    "                subset, kind = metric.split(\"_\")\n",
    "                if kind == \"rho\":\n",
    "                    output_numb_dict[metric][layer_numb] = result_dict[subset][kind][0]\n",
    "                else:\n",
    "                    output_numb_dict[metric][layer_numb] = result_dict[subset][kind]\n",
    "\n",
    "        # get some details for plotting and saving\n",
    "        output_subfolder = checkNgen_folder(\n",
    "            folder_path.replace(self._input_path, self._output_path)\n",
    "        )\n",
    "\n",
    "        for metric in output_numb_dict.keys():\n",
    "\n",
    "            plot_name = f\"{encoder_name}_{flatten_emb}_{metric}\"\n",
    "            plot_prefix = f\"{task}_{dataset}_{split}\"\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(output_numb_dict[metric])\n",
    "            plt.title(f\"{plot_prefix} \\n {plot_name}\")\n",
    "            plt.xlabel(\"layers\")\n",
    "            plt.ylabel(\"loss\")\n",
    "\n",
    "            for plot_ext in [\".svg\", \".png\"]:\n",
    "                plt.savefig(\n",
    "                    os.path.join(output_subfolder, plot_name + plot_ext),\n",
    "                    bbox_inches=\"tight\",\n",
    "                )\n",
    "            plt.close()\n",
    "\n",
    "        return output_numb_dict\n",
    "\n",
    "    @property\n",
    "    def layer_analysis_dict(self) -> dict:\n",
    "        \"\"\"Return a dict with dataset name as the key\"\"\"\n",
    "        return self._layer_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_analysis_dict = LayerLoss(\n",
    "    input_path=\"results/train_val_test\",\n",
    "    output_path=\"results/analysis_layer\",\n",
    ").layer_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1b_t33_650M_UR50S ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1b_t33_650M_UR50S/mean ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1_t6_43M_UR50S ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1_t6_43M_UR50S/mean ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1_t12_85M_UR50S ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1_t12_85M_UR50S/mean ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1_t34_670M_UR50S ...\n",
      "Making results/sklearn-scaley-noloader-rand_layer/proeng/gb1/sampled/esm1_t34_670M_UR50S/mean ...\n"
     ]
    }
   ],
   "source": [
    "layer_analysis_dict = LayerLoss(\n",
    "    input_path=\"results/sklearn-scaley-noloader-rand\",\n",
    "    output_path=\"results/sklearn-scaley-noloader-rand_layer\",\n",
    ").layer_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_analysis_dict = LayerLoss(\n",
    "    input_path=\"results/sklearn-scaley-noloader\",\n",
    "    output_path=\"results/sklearn-scaley-noloader_layer\",\n",
    ").layer_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_analysis_dict = LayerLoss(\n",
    "    input_path=\"results/sklearn-scaley-fixa-noloader\",\n",
    "    output_path=\"results/sklearn-scaley-fixa-noloader_layer\",\n",
    ").layer_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_analysis_dict = LayerLoss(\n",
    "    input_path=\"results/sklearn-scaley-fixa\",\n",
    "    output_path=\"results/sklearn-scaley-fixa_layer\",\n",
    ").layer_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['proeng_gb1_two_vs_rest_mean', 'proeng_gb1_low_vs_high_mean', 'proeng_aav_one_vs_many_mean']),\n",
       " dict_keys(['esm1b_t33_650M_UR50S', 'esm1_t6_43M_UR50S', 'esm1_t12_85M_UR50S', 'esm1_t34_670M_UR50S']),\n",
       " dict_keys(['train_mse', 'test_ndcg', 'test_rho']))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_analysis_dict.keys(), layer_analysis_dict[\"proeng_gb1_low_vs_high_mean\"].keys(), layer_analysis_dict[\"proeng_gb1_low_vs_high_mean\"][\"esm1b_t33_650M_UR50S\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making results/sklearn-scaley-noloader_layer-onehot ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1 ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/sampled ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/sampled/onehot ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/sampled/onehot/flatten ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/two_vs_rest ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/two_vs_rest/onehot ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/two_vs_rest/onehot/flatten ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/low_vs_high ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/low_vs_high/onehot ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/gb1/low_vs_high/onehot/flatten ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/aav ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/aav/one_vs_many ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/aav/one_vs_many/onehot ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/aav/one_vs_many/onehot/flatten ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/aav/two_vs_many ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/aav/two_vs_many/onehot ...\n",
      "Making results/sklearn-scaley-noloader_layer-onehot/proeng/aav/two_vs_many/onehot/flatten ...\n"
     ]
    }
   ],
   "source": [
    "layer_analysis_dict = LayerLoss(\n",
    "    input_path=\"results/sklearn-scaley-noloader\",\n",
    "    output_path=\"results/sklearn-scaley-noloader_layer\",\n",
    ").layer_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
