{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>ss3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AETVESCLAKSHTENSFTNVXKDDKTLDRYANYEGCLWNATGVVVC...</td>\n",
       "      <td>[2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASQEISKSIYTCNDNQVXEVIYVNTEAGNAYAIISQVNEXIPXRLX...</td>\n",
       "      <td>[2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGSSHHHHHHSSGRENLYFQGXNISEINGFEVTGFVVRTTNADEXN...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNADYNRLSVPGNVIGKGGNAVVYEDAEDATKVLKMFTTSQSNEEV...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DTVGRPLPHLAAAMQASGEAVYCDDIPRYENELFLRLVTSTRAHAK...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  AETVESCLAKSHTENSFTNVXKDDKTLDRYANYEGCLWNATGVVVC...   \n",
       "1  ASQEISKSIYTCNDNQVXEVIYVNTEAGNAYAIISQVNEXIPXRLX...   \n",
       "2  XGSSHHHHHHSSGRENLYFQGXNISEINGFEVTGFVVRTTNADEXN...   \n",
       "3  SNADYNRLSVPGNVIGKGGNAVVYEDAEDATKVLKMFTTSQSNEEV...   \n",
       "4  DTVGRPLPHLAAAMQASGEAVYCDDIPRYENELFLRLVTSTRAHAK...   \n",
       "\n",
       "                                                 ss3  split  \n",
       "0  [2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, ...  train  \n",
       "1  [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, ...  train  \n",
       "2  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  train  \n",
       "3  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, ...  train  \n",
       "4  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, ...  train  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/structure/secondary_structure/tape_ss3.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 0],\n",
       "       [1, 2, 1],\n",
       "       [2, 1, 0],\n",
       "       [0, 0, 2],\n",
       "       [0, 2, 1],\n",
       "       [0, 0, 2],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 2],\n",
       "       [2, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "X, y1 = make_classification(n_samples=10, n_features=100,\n",
    "                            n_informative=30, n_classes=3,\n",
    "                            random_state=1)\n",
    "y2 = shuffle(y1, random_state=1)\n",
    "y3 = shuffle(y1, random_state=2)\n",
    "Y = np.vstack((y1, y2, y3)).T\n",
    "n_samples, n_features = X.shape # 10,100\n",
    "n_outputs = Y.shape[1] # 3\n",
    "n_classes = 3\n",
    "forest = RandomForestClassifier(random_state=1)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\n",
    "multi_target_forest.fit(X, Y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 100), (10, 3), (10,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n"
     ]
    }
   ],
   "source": [
    "X, y1 = make_classification(n_samples=10, n_features=100,\n",
    "                            n_informative=30, n_classes=3,\n",
    "                            random_state=1)\n",
    "ys = [shuffle(y1, random_state=i) for i in range(99)]\n",
    "Y = np.vstack((y1, ys)).T\n",
    "print(Y.shape)\n",
    "n_samples, n_features = X.shape # 10,100\n",
    "n_outputs = Y.shape[1] # 3\n",
    "n_classes = 3\n",
    "forest = RandomForestClassifier(random_state=1)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\n",
    "Y_pred = multi_target_forest.fit(X, Y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 100), (10, 100), (10,), (10, 100))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, y1.shape, Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.encoding.encoding_classes import ESMEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 756, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_batch_encoder = ESMEncoder(\n",
    "    encoder_name=\"esm1_t6_43M_UR50S\",\n",
    ").encode(mut_seqs=list(df.sequence), batch_size=64, flatten_emb=False)\n",
    "one_mean_emb = next(mean_batch_encoder)\n",
    "one_mean_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 756, 768)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = one_mean_emb[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print np_y shape for dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[3, 3, 3, ..., 0, 0, 0],\n",
       "        [3, 3, 3, ..., 0, 0, 0],\n",
       "        [3, 3, 3, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [3, 2, 2, ..., 0, 0, 0],\n",
       "        [3, 3, 3, ..., 0, 0, 0],\n",
       "        [3, 3, 2, ..., 0, 0, 0]]),\n",
       " (11497, 1632))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_y = df[\"ss3\"].apply(lambda x: np.array(x[1:-1].split(\", \")).astype(\"int\")) + 1\n",
    "max_len = max(len(y) for y in np_y)\n",
    "print(\"print np_y shape for dataset\")\n",
    "ss3_y = np.stack([np.pad(i, pad_width=(0, max_len - len(i)), constant_values=0,) for i in np_y])\n",
    "ss3_y, ss3_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3, ..., 0, 0, 0],\n",
       "       [3, 3, 3, ..., 0, 0, 0],\n",
       "       [3, 3, 3, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [3, 3, 3, ..., 0, 0, 0],\n",
       "       [3, 3, 2, ..., 0, 0, 0],\n",
       "       [3, 3, 3, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss3_y[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.arange(ss3_y[:64].max()+1) == ss3_y[:64][...,None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         ...,\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         ...,\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         ...,\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         ...,\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 1, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         ...,\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 0, 0]]]),\n",
       " (64, 1632, 4))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.arange(ss3_y[:64].max()+1) == ss3_y[:64][...,None]).astype(int), (np.arange(ss3_y[:64].max()+1) == ss3_y[:64][...,None]).astype(int).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3486)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss3_oh_y = OneHotEncoder().fit_transform(ss3_y[:64]).toarray()\n",
    "ss3_oh_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 0., 0., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 0., 0., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss3_oh_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000052vscode-remote?line=0'>1</a>\u001b[0m lb\u001b[39m.\u001b[39;49mtransform(ss3_y[:\u001b[39m64\u001b[39;49m])\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:343\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[1;32m    324\u001b[0m     \u001b[39m\"\"\"Transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[39m    The output of transform is sometimes referred to by some authors as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39m        will be of CSR format.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    345\u001b[0m     y_is_multilabel \u001b[39m=\u001b[39m type_of_target(y)\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    346\u001b[0m     \u001b[39mif\u001b[39;00m y_is_multilabel \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[39m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/utils/validation.py:1041\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     attrs \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(estimator)\n\u001b[1;32m   1038\u001b[0m              \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m attrs:\n\u001b[0;32m-> 1041\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "lb.transform(ss3_y[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelMultiClass(nn.Module):\n",
    "    \"\"\"Multi label multi class\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, numb_class: int) -> None:\n",
    "        super(MultiLabelMultiClass, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, numb_class)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        print(\"multilabel output inside forward\")\n",
    "        # print(self.softmax(self.linear(x)).shape)\n",
    "        # return self.softmax(self.linear(x))\n",
    "        print(self.softmax(self.linear(x)), )\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "    @property\n",
    "    def model_name(self) -> str:\n",
    "        \"\"\"Return the name of the model\"\"\"\n",
    "        return \"MultiLabelMultiClass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.8619e-01,  3.0612e-01,  6.2143e-01,  8.3901e-01],\n",
       "          [ 3.3634e-01,  3.6491e-01,  5.6606e-01,  9.6666e-01],\n",
       "          [ 2.6897e-01,  1.8964e-01,  6.1311e-01,  8.9628e-01],\n",
       "          ...,\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02]],\n",
       " \n",
       "         [[ 2.8619e-01,  3.0612e-01,  6.2143e-01,  8.3901e-01],\n",
       "          [ 4.9324e-02,  2.6449e-01,  6.4939e-01,  7.8951e-01],\n",
       "          [ 2.8519e-01,  2.9909e-01,  4.2499e-01,  1.1066e+00],\n",
       "          ...,\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02]],\n",
       " \n",
       "         [[ 2.5869e-01,  5.3333e-01,  5.1984e-01,  6.6602e-01],\n",
       "          [ 2.1767e-01,  3.6173e-01,  7.1477e-01,  8.3051e-01],\n",
       "          [-2.3775e-03,  1.5438e-01,  6.6957e-01,  8.6444e-01],\n",
       "          ...,\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.9253e-01,  3.3674e-01,  5.2125e-01,  7.7971e-01],\n",
       "          [ 4.9324e-02,  2.6449e-01,  6.4939e-01,  7.8951e-01],\n",
       "          [ 1.9503e-01, -4.3826e-02,  5.0746e-01,  8.0577e-01],\n",
       "          ...,\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02]],\n",
       " \n",
       "         [[ 2.9253e-01,  3.3674e-01,  5.2125e-01,  7.7971e-01],\n",
       "          [ 2.2134e-01,  2.0492e-01,  6.8834e-01,  8.8620e-01],\n",
       "          [ 1.3666e-01,  2.1918e-01,  6.4225e-01,  8.7659e-01],\n",
       "          ...,\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02]],\n",
       " \n",
       "         [[ 2.8619e-01,  3.0612e-01,  6.2143e-01,  8.3901e-01],\n",
       "          [ 9.1597e-04,  1.5527e-01,  5.2169e-01,  8.7534e-01],\n",
       "          [-5.0786e-02,  4.5155e-02,  5.4186e-01,  9.5027e-01],\n",
       "          ...,\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02],\n",
       "          [-5.0892e-03, -3.3327e-02, -2.2007e-02,  1.2481e-02]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([64, 756, 4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Linear(768, 4)\n",
    "y = m(torch.from_numpy(one_mean_emb[0]))\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1357099/2949005056.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nn.Softmax()(y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0159, 0.0147, 0.0166, 0.0163],\n",
       "         [0.0180, 0.0180, 0.0152, 0.0175],\n",
       "         [0.0183, 0.0168, 0.0160, 0.0153],\n",
       "         ...,\n",
       "         [0.0155, 0.0155, 0.0157, 0.0156],\n",
       "         [0.0157, 0.0155, 0.0158, 0.0156],\n",
       "         [0.0156, 0.0154, 0.0157, 0.0157]],\n",
       "\n",
       "        [[0.0159, 0.0147, 0.0166, 0.0163],\n",
       "         [0.0135, 0.0162, 0.0165, 0.0146],\n",
       "         [0.0186, 0.0188, 0.0133, 0.0189],\n",
       "         ...,\n",
       "         [0.0155, 0.0155, 0.0157, 0.0156],\n",
       "         [0.0157, 0.0155, 0.0158, 0.0156],\n",
       "         [0.0156, 0.0154, 0.0157, 0.0157]],\n",
       "\n",
       "        [[0.0155, 0.0184, 0.0150, 0.0137],\n",
       "         [0.0160, 0.0179, 0.0176, 0.0153],\n",
       "         [0.0140, 0.0162, 0.0170, 0.0148],\n",
       "         ...,\n",
       "         [0.0155, 0.0155, 0.0157, 0.0156],\n",
       "         [0.0157, 0.0155, 0.0158, 0.0156],\n",
       "         [0.0156, 0.0154, 0.0157, 0.0157]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0160, 0.0151, 0.0150, 0.0153],\n",
       "         [0.0135, 0.0162, 0.0165, 0.0146],\n",
       "         [0.0170, 0.0133, 0.0144, 0.0140],\n",
       "         ...,\n",
       "         [0.0155, 0.0155, 0.0157, 0.0156],\n",
       "         [0.0157, 0.0155, 0.0158, 0.0156],\n",
       "         [0.0156, 0.0154, 0.0157, 0.0157]],\n",
       "\n",
       "        [[0.0160, 0.0151, 0.0150, 0.0153],\n",
       "         [0.0160, 0.0153, 0.0171, 0.0161],\n",
       "         [0.0160, 0.0173, 0.0165, 0.0150],\n",
       "         ...,\n",
       "         [0.0155, 0.0155, 0.0157, 0.0156],\n",
       "         [0.0157, 0.0155, 0.0158, 0.0156],\n",
       "         [0.0156, 0.0154, 0.0157, 0.0157]],\n",
       "\n",
       "        [[0.0159, 0.0147, 0.0166, 0.0163],\n",
       "         [0.0129, 0.0146, 0.0145, 0.0160],\n",
       "         [0.0133, 0.0145, 0.0149, 0.0162],\n",
       "         ...,\n",
       "         [0.0155, 0.0155, 0.0157, 0.0156],\n",
       "         [0.0157, 0.0155, 0.0158, 0.0156],\n",
       "         [0.0156, 0.0154, 0.0157, 0.0157]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1939, 0.1978, 0.2712, 0.3371],\n",
       "         [0.1936, 0.1992, 0.2436, 0.3636],\n",
       "         [0.1920, 0.1774, 0.2709, 0.3596],\n",
       "         ...,\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562]],\n",
       "\n",
       "        [[0.1939, 0.1978, 0.2712, 0.3371],\n",
       "         [0.1624, 0.2014, 0.2959, 0.3404],\n",
       "         [0.1839, 0.1865, 0.2115, 0.4181],\n",
       "         ...,\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562]],\n",
       "\n",
       "        [[0.1954, 0.2572, 0.2537, 0.2937],\n",
       "         [0.1772, 0.2046, 0.2912, 0.3270],\n",
       "         [0.1537, 0.1798, 0.3009, 0.3657],\n",
       "         ...,\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.2028, 0.2120, 0.2550, 0.3302],\n",
       "         [0.1624, 0.2014, 0.2959, 0.3404],\n",
       "         [0.2002, 0.1576, 0.2736, 0.3686],\n",
       "         ...,\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562]],\n",
       "\n",
       "        [[0.2028, 0.2120, 0.2550, 0.3302],\n",
       "         [0.1811, 0.1781, 0.2888, 0.3520],\n",
       "         [0.1712, 0.1860, 0.2839, 0.3589],\n",
       "         ...,\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562]],\n",
       "\n",
       "        [[0.1939, 0.1978, 0.2712, 0.3371],\n",
       "         [0.1601, 0.1868, 0.2694, 0.3837],\n",
       "         [0.1508, 0.1660, 0.2728, 0.4104],\n",
       "         ...,\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562],\n",
       "         [0.2517, 0.2447, 0.2475, 0.2562]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6403, -1.6203, -1.3050, -1.0874],\n",
       "         [-1.6420, -1.6134, -1.4123, -1.0117],\n",
       "         [-1.6500, -1.7293, -1.3059, -1.0227],\n",
       "         ...,\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620]],\n",
       "\n",
       "        [[-1.6403, -1.6203, -1.3050, -1.0874],\n",
       "         [-1.8179, -1.6027, -1.2178, -1.0777],\n",
       "         [-1.6934, -1.6795, -1.5536, -0.8719],\n",
       "         ...,\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620]],\n",
       "\n",
       "        [[-1.6326, -1.3580, -1.3715, -1.2253],\n",
       "         [-1.7307, -1.5866, -1.2336, -1.1179],\n",
       "         [-1.8729, -1.7161, -1.2009, -1.0061],\n",
       "         ...,\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.5953, -1.5511, -1.3666, -1.1081],\n",
       "         [-1.8179, -1.6027, -1.2178, -1.0777],\n",
       "         [-1.6087, -1.8475, -1.2962, -0.9979],\n",
       "         ...,\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620]],\n",
       "\n",
       "        [[-1.5953, -1.5511, -1.3666, -1.1081],\n",
       "         [-1.7089, -1.7254, -1.2419, -1.0441],\n",
       "         [-1.7647, -1.6822, -1.2591, -1.0248],\n",
       "         ...,\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620]],\n",
       "\n",
       "        [[-1.6403, -1.6203, -1.3050, -1.0874],\n",
       "         [-1.8322, -1.6779, -1.3114, -0.9578],\n",
       "         [-1.8917, -1.7958, -1.2990, -0.8906],\n",
       "         ...,\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620],\n",
       "         [-1.3795, -1.4078, -1.3965, -1.3620]]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LogSoftmax(dim=2)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1,  ..., 3, 3, 3],\n",
       "         [1, 1, 1,  ..., 3, 3, 3],\n",
       "         [1, 1, 1,  ..., 3, 3, 3],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 3, 3, 3],\n",
       "         [1, 1, 1,  ..., 3, 3, 3],\n",
       "         [1, 1, 1,  ..., 3, 3, 3]]),\n",
       " torch.Size([64, 756]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(y, dim=2), torch.argmax(y, dim=2).shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "        2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
       "        3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 1, 1, 1, 1,\n",
       "        1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(y, dim=2)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9566000000000001"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2810 + 0.2385 + 0.2313 + 0.2058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2432 + 0.2856 + 0.1944 + 0.2473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998999999999999"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1000 + 0.4497 + 0.1851 + 0.2651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0137, 0.0172, 0.0139, 0.0154],\n",
       "          [0.0192, 0.0170, 0.0148, 0.0139],\n",
       "          [0.0146, 0.0171, 0.0145, 0.0147],\n",
       "          ...,\n",
       "          [0.0156, 0.0156, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0157, 0.0154]],\n",
       " \n",
       "         [[0.0137, 0.0172, 0.0139, 0.0154],\n",
       "          [0.0141, 0.0167, 0.0177, 0.0171],\n",
       "          [0.0173, 0.0154, 0.0152, 0.0145],\n",
       "          ...,\n",
       "          [0.0156, 0.0156, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0157, 0.0154]],\n",
       " \n",
       "         [[0.0228, 0.0119, 0.0158, 0.0107],\n",
       "          [0.0160, 0.0123, 0.0171, 0.0159],\n",
       "          [0.0139, 0.0174, 0.0176, 0.0180],\n",
       "          ...,\n",
       "          [0.0156, 0.0156, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0157, 0.0154]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0170, 0.0178, 0.0143, 0.0183],\n",
       "          [0.0141, 0.0167, 0.0177, 0.0171],\n",
       "          [0.0178, 0.0145, 0.0167, 0.0168],\n",
       "          ...,\n",
       "          [0.0156, 0.0156, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0157, 0.0154]],\n",
       " \n",
       "         [[0.0170, 0.0178, 0.0143, 0.0183],\n",
       "          [0.0144, 0.0175, 0.0137, 0.0162],\n",
       "          [0.0170, 0.0187, 0.0154, 0.0140],\n",
       "          ...,\n",
       "          [0.0156, 0.0156, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0157, 0.0154]],\n",
       " \n",
       "         [[0.0137, 0.0172, 0.0139, 0.0154],\n",
       "          [0.0173, 0.0146, 0.0187, 0.0149],\n",
       "          [0.0170, 0.0152, 0.0186, 0.0157],\n",
       "          ...,\n",
       "          [0.0156, 0.0156, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0156, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0157, 0.0154]]], grad_fn=<SoftmaxBackward>),\n",
       " torch.Size([64, 756, 4]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = nn.Softmax(dim=0)\n",
    "y_cls = sm(y)\n",
    "y_cls, y_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, ...\n",
       "1    [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, ...\n",
       "2    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...\n",
       "3    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, ...\n",
       "Name: ss3, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ss3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df.ss3[:4].to_numpy()\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['2', '2', '2', '0', '0', '0', '0', '0', '2', '2', '2', '2', '1',\n",
       "              '1', '1', '1', '1', '1', '2', '2', '1', '1', '2', '2', '2', '2',\n",
       "              '2', '2', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '1',\n",
       "              '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '1',\n",
       "              '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2',\n",
       "              '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '2',\n",
       "              '2', '2', '2', '2', '2', '2', '2', '1', '2', '2', '2', '2', '1',\n",
       "              '2', '2', '2', '2', '2', '2', '1', '1', '1', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '1', '1', '1', '2', '2', '1', '1', '1', '1', '1',\n",
       "              '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "              '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1',\n",
       "              '1', '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '2',\n",
       "              '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '1', '1', '2', '2', '2', '2', '2', '2', '1', '1',\n",
       "              '1', '1', '2', '2', '2', '2', '2', '2', '2'], dtype='<U1')      ,\n",
       "       array(['2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '2',\n",
       "              '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2',\n",
       "              '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '2', '2', '1',\n",
       "              '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '1',\n",
       "              '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '1', '1',\n",
       "              '1', '1', '1', '2', '2', '1', '1', '1', '1', '1', '1', '2', '2',\n",
       "              '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2',\n",
       "              '2', '2', '2', '2', '2', '2', '2'], dtype='<U1')                ,\n",
       "       array(['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "              '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "              '2', '0', '0', '0', '0', '0', '2', '2', '2', '2', '2', '2', '0',\n",
       "              '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '0', '0',\n",
       "              '0', '2', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1',\n",
       "              '2', '2', '2', '2', '2', '2', '2', '2', '2', '1', '1', '1', '1',\n",
       "              '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "              '2', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '1', '1',\n",
       "              '1', '1', '1', '1', '2', '2', '2', '2', '0', '0', '0', '0', '0',\n",
       "              '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '2', '1', '2', '2', '2', '1', '1', '1', '1', '1',\n",
       "              '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "              '2'], dtype='<U1')                                              ,\n",
       "       array(['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '1', '1', '1',\n",
       "              '1', '1', '1', '1', '2', '2', '2', '1', '1', '1', '1', '1', '1',\n",
       "              '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '2', '2',\n",
       "              '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "              '0', '0', '0', '0', '0', '2', '2', '2', '2', '1', '1', '1', '1',\n",
       "              '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1',\n",
       "              '2', '2', '2', '2', '1', '1', '2', '2', '2', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "              '0', '0', '0', '0', '2', '2', '2', '2', '1', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '1', '1', '1', '1', '2', '2', '2', '2', '1', '1',\n",
       "              '1', '1', '2', '2', '2', '2', '1', '2', '2', '2', '2', '2', '2',\n",
       "              '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "              '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '2'],\n",
       "             dtype='<U1')                                                      ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2', '2', '2', '0', '0', '0', '0', '0', '2', '2', '2', '2', '1',\n",
       "        '1', '1', '1', '1', '1', '2', '2', '1', '1', '2', '2', '2', '2',\n",
       "        '2', '2', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '1', '2', '2', '2', '2', '1',\n",
       "        '2', '2', '2', '2', '2', '2', '1', '1', '1', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '1', '1', '1', '2', '2', '1', '1', '1', '1', '1',\n",
       "        '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '1', '1', '2', '2', '2', '2', '2', '2', '1', '1',\n",
       "        '1', '1', '2', '2', '2', '2', '2', '2', '2'],\n",
       "       ['2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '2',\n",
       "        '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2',\n",
       "        '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '2', '2', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '1',\n",
       "        '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '1', '1',\n",
       "        '1', '1', '1', '2', '2', '1', '1', '1', '1', '1', '1', '2', '2',\n",
       "        '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '2', '0', '0', '0', '0', '0', '2', '2', '2', '2', '2', '2', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '0', '0',\n",
       "        '0', '2', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '2', '2', '2', '2', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '1', '2', '2', '2', '1', '1', '1', '1', '1',\n",
       "        '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '2', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '2', '2', '2', '1', '1', '1', '1', '1', '1',\n",
       "        '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '2', '2',\n",
       "        '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '2', '2', '2', '2', '1', '1', '1', '1',\n",
       "        '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '2', '2', '2', '2', '1', '1', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '2', '2', '2', '2', '1', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '1', '1', '1', '1', '2', '2', '2', '2', '1', '1',\n",
       "        '1', '1', '2', '2', '2', '2', '1', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '2',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "        '-', '-', '-', '-', '-', '-', '-', '-', '-']], dtype='<U1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([np.pad(y, pad_width=(0, X.shape[1] - len(y)), constant_values=-1) for y in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad wiht -1\n",
    "np.pad(y, pad_width=((0, X.shape[1] - len(y)), (-1 , -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-processing the dataset\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scr.utils import pickle_save, pickle_load, replace_ext\n",
    "from scr.params.sys import RAND_SEED\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO, MAX_SEQ_LEN\n",
    "from scr.preprocess.seq_loader import SeqLoader\n",
    "from scr.encoding.encoding_classes import (\n",
    "    AbstractEncoder,\n",
    "    ESMEncoder,\n",
    "    CARPEncoder,\n",
    "    OnehotEncoder,\n",
    ")\n",
    "\n",
    "\n",
    "def get_mut_name(mut_seq: str, parent_seq: str) -> str:\n",
    "    \"\"\"\n",
    "    A function for returning the mutant name\n",
    "\n",
    "    Args:\n",
    "    - mut_seq: str, the full mutant sequence\n",
    "    - parent_seq: str, the full parent sequence\n",
    "\n",
    "    Returns:\n",
    "    - str, parent, indel, or mutant name in the format of\n",
    "        ParentAAMutLocMutAA:ParentAAMutLocMutAA:..., ie. W39W:D40G:G41C:V54Q\n",
    "    \"\"\"\n",
    "\n",
    "    mut_list = []\n",
    "    if parent_seq == mut_seq:\n",
    "        return \"parent\"\n",
    "    elif len(parent_seq) == len(mut_seq):\n",
    "        for i, (p, m) in enumerate(zip(list(parent_seq), list(mut_seq))):\n",
    "            if p != m:\n",
    "                mut_list.append(f\"{p}{i+1}{m}\")\n",
    "        return \":\".join(mut_list)\n",
    "    else:\n",
    "        return \"indel\"\n",
    "\n",
    "\n",
    "class AddMutInfo:\n",
    "    \"\"\"A class for appending mutation info for mainly protein engineering tasks\"\"\"\n",
    "\n",
    "    def __init__(self, parent_seq_path: str, csv_path: str):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - parent_seq_path: str, path for the parent sequence\n",
    "        - csv_path: str, path for the fitness csv file\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the parent sequence from the fasta file\n",
    "        self._parent_seq = SeqLoader(parent_seq_path=parent_seq_path)\n",
    "\n",
    "        # load the dataframe\n",
    "        self._init_df = pd.read_csv(csv_path)\n",
    "\n",
    "        self._df = self._init_df.copy()\n",
    "        # add a column with the mutant names\n",
    "        self._df[\"mut_name\"] = self._init_df[\"sequence\"].apply(\n",
    "            get_mut_name, parent_seq=self._parent_seq\n",
    "        )\n",
    "        # add a column with the number of mutations\n",
    "        self._df[\"mut_numb\"] = (\n",
    "            self._df[\"mut_name\"].str.split(\":\").map(len, na_action=\"ignore\")\n",
    "        )\n",
    "\n",
    "        # get the pickle file path\n",
    "        self._pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "\n",
    "        pickle_save(what2save=self._df, where2save=self._pkl_path)\n",
    "\n",
    "    @property\n",
    "    def parent_seq(self) -> str:\n",
    "        \"\"\"Return the parent sequence\"\"\"\n",
    "        return self._parent_seq\n",
    "\n",
    "    @property\n",
    "    def pkl_path(self) -> str:\n",
    "        \"\"\"Return the pkl file path for the processed dataframe\"\"\"\n",
    "        return self._pkl_path\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the processed dataframe\"\"\"\n",
    "        return self._df\n",
    "\n",
    "\n",
    "def std_ssdf(\n",
    "    ssdf_path: str = \"data/structure/secondary_structure/tape_ss3.csv\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A function that standardize secondary structure dataset\n",
    "    to set up as columns as sequence, target, set, validation\n",
    "    where set is train or test and add validation as true\n",
    "    \"\"\"\n",
    "    folder_path = os.path.dirname(ssdf_path)\n",
    "\n",
    "    df = pd.read_csv(ssdf_path)\n",
    "\n",
    "    # convert the string into numpy array\n",
    "    # df[\"ss3\"] = df[\"ss3\"].apply(lambda x: np.array(x[1:-1].split(\", \")))\n",
    "\n",
    "    # add validation column\n",
    "    df[\"validation\"] = df[\"split\"].apply(lambda x: True if x == \"valid\" else \"\")\n",
    "    # now replace valid to train\n",
    "    df = df.replace(\"valid\", \"train\")\n",
    "    # rename all columns\n",
    "    df.columns = [\"sequence\", \"target\", \"set\", \"validation\"]\n",
    "\n",
    "    # get all kinds of test sets\n",
    "    ss_tests = set(df[\"set\"].unique()) - set([\"train\"])\n",
    "\n",
    "    for ss_test in ss_tests:\n",
    "        df.loc[~df[\"set\"].isin(set(ss_tests) - set([ss_test]))].replace(\n",
    "            ss_test, \"test\"\n",
    "        ).to_csv(os.path.join(folder_path, ss_test + \".csv\"), index=False)\n",
    "\n",
    "\n",
    "def split_train_val_test_df(df: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Return split dataframe for training, validation, and testing\n",
    "\n",
    "    Args:\n",
    "    - df: pd.DataFrame, input dataframe\n",
    "\n",
    "    Returns:\n",
    "    - a list of dataframes for train, val, test\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] != True)],\n",
    "        df.loc[(df[\"set\"] == \"train\") & (df[\"validation\"] == True)],\n",
    "        df.loc[(df[\"set\"] == \"test\")],\n",
    "    )\n",
    "\n",
    "\n",
    "class DatasetInfo:\n",
    "    \"\"\"\n",
    "    A class returns the information of a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, the path for the csv\n",
    "        \"\"\"\n",
    "        self._df = pd.read_csv(dataset_path)\n",
    "\n",
    "    def get_model_type(self) -> str:\n",
    "        # pick linear regression if y numerical\n",
    "        if self._df.target.dtype.kind in \"iufc\":\n",
    "            return \"LinearRegression\"\n",
    "        else:\n",
    "            # ss3\n",
    "            if \"[\" in self._df.target[0]:\n",
    "                return \"MultiLabelMultiClass\"\n",
    "            # annotation\n",
    "            else:\n",
    "                return \"LinearClassifier\"\n",
    "\n",
    "    def get_numb_class(self) -> int:\n",
    "        \"\"\"\n",
    "        A function to get number of class\n",
    "        \"\"\"\n",
    "        # annotation class number\n",
    "        if self.model_type == \"LinearClassifier\":\n",
    "            return self._df.target.nunique()\n",
    "        # ss3 or ss8 secondary structure states plus padding\n",
    "        elif self.model_type == \"MultiLabelMultiClass\":\n",
    "            return len(np.unique(np.array(self._df[\"target\"][0][1:-1].split(\", \")))) + 1\n",
    "\n",
    "    @property\n",
    "    def model_type(self) -> str:\n",
    "        \"\"\"Return the pytorch model type\"\"\"\n",
    "        return self.get_model_type()\n",
    "\n",
    "    @property\n",
    "    def numb_class(self) -> int:\n",
    "        \"\"\"Return number of classes for classification\"\"\"\n",
    "        return self.get_numb_class()\n",
    "\n",
    "\n",
    "class TaskProcess:\n",
    "    \"\"\"A class for handling different downstream tasks\"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_folder: str, a folder path with all the tasks as subfolders where\n",
    "            all the subfolders have datasets as the subsubfolders, ie\n",
    "\n",
    "            {data_folder}/\n",
    "                proeng/\n",
    "                    aav/\n",
    "                        one_vs_many.csv\n",
    "                        two_vs_many.csv\n",
    "                        P03135.fasta\n",
    "                    thermo/\n",
    "                        mixed.csv\n",
    "        \"\"\"\n",
    "\n",
    "        if data_folder[-1] == \"/\":\n",
    "            self._data_folder = data_folder\n",
    "        else:\n",
    "            self._data_folder = data_folder + \"/\"\n",
    "\n",
    "        # sumamarize all files i nthe data folder\n",
    "        self._sum_file_df = self.sum_files()\n",
    "\n",
    "    def sum_files(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Summarize all files in the data folder\n",
    "\n",
    "        Returns:\n",
    "        - A dataframe with \"task\", \"dataset\", \"split\",\n",
    "            \"csv_path\", \"fasta_path\", \"pkl_path\" as columns, ie.\n",
    "            (proeng, gb1, low_vs_high, data/proeng/gb1/low_vs_high.csv,\n",
    "            data/proeng/gb1/5LDE_1.fasta)\n",
    "            note that csv_path is the list of lmdb files for the structure task\n",
    "        \"\"\"\n",
    "        dataset_folders = glob(f\"{self._data_folder}*/*\")\n",
    "        # need a list of tuples in the order of:\n",
    "        # (task, dataset, split, csv_path, fasta_path)\n",
    "        list_for_df = []\n",
    "        for dataset_folder in dataset_folders:\n",
    "            _, task, dataset = dataset_folder.split(\"/\")\n",
    "\n",
    "            csv_paths = glob(f\"{dataset_folder}/*.csv\")\n",
    "\n",
    "            if task == \"structure\":\n",
    "                csv_paths = set(csv_paths) - set(glob(f\"{dataset_folder}/tape_ss3.csv\"))\n",
    "\n",
    "            fasta_paths = glob(f\"{dataset_folder}/*.fasta\")\n",
    "            pkl_paths = glob(f\"{dataset_folder}/*.pkl\")\n",
    "\n",
    "            assert len(csv_paths) >= 1, \"Less than one csv\"\n",
    "            assert len(fasta_paths) <= 1, \"More than one fasta\"\n",
    "\n",
    "            for csv_path in csv_paths:\n",
    "                # if parent seq fasta exists\n",
    "                if len(fasta_paths) == 1:\n",
    "                    fasta_path = fasta_paths[0]\n",
    "\n",
    "                    # if no existing pkl file, generate and save\n",
    "                    if len(pkl_paths) == 0:\n",
    "                        print(f\"Adding mutation info to {csv_path}...\")\n",
    "                        pkl_path = AddMutInfo(\n",
    "                            parent_seq_path=fasta_path, csv_path=csv_path\n",
    "                        ).pkl_path\n",
    "                    # pkl file exits\n",
    "                    else:\n",
    "                        pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "                # no parent fasta no pkl file\n",
    "                else:\n",
    "                    fasta_path = \"\"\n",
    "                    pkl_path = \"\"\n",
    "\n",
    "                train_df, val_df, test_df = split_train_val_test_df(\n",
    "                    pd.read_csv(csv_path)\n",
    "                )\n",
    "\n",
    "                list_for_df.append(\n",
    "                    tuple(\n",
    "                        [\n",
    "                            task,\n",
    "                            dataset,\n",
    "                            os.path.basename(os.path.splitext(csv_path)[0]),\n",
    "                            len(train_df),\n",
    "                            len(val_df),\n",
    "                            len(test_df),\n",
    "                            csv_path,\n",
    "                            fasta_path,\n",
    "                            pkl_path,\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            list_for_df,\n",
    "            columns=[\n",
    "                \"task\",\n",
    "                \"dataset\",\n",
    "                \"split\",\n",
    "                \"train_numb\",\n",
    "                \"val_numb\",\n",
    "                \"test_numb\",\n",
    "                \"csv_path\",\n",
    "                \"fasta_path\",\n",
    "                \"pkl_path\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def sum_file_df(self) -> pd.DataFrame:\n",
    "        \"\"\"A summary table for all files in the data folder\"\"\"\n",
    "        return self._sum_file_df\n",
    "\n",
    "\n",
    "class ProtranDataset(Dataset):\n",
    "\n",
    "    \"\"\"A dataset class for processing protein transfer data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        subset: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str = None,\n",
    "        embed_layer: int | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        if_encode_all: bool = True,\n",
    "        **encoder_params,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format, ie\n",
    "            \"data/proeng/gb1/low_vs_high.csv\"\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - subset: str, train, val, test\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding folder, ie\n",
    "            \"embeddings/proeng/gb1/low_vs_high\"\n",
    "            for which then can add the subset to be, ie\n",
    "            \"embeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/test/embedding.h5\"\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - if_encode_all: bool = True, if encode full dataset all layers on the fly\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        # with additional info mut_name, mut_numb\n",
    "        if os.path.splitext(dataset_path)[-1] in [\".pkl\", \".PKL\", \"\"]:\n",
    "            self._df = pickle_load(dataset_path)\n",
    "            self._add_mut_info = True\n",
    "        # without such info\n",
    "        else:\n",
    "            self._df = pd.read_csv(dataset_path)\n",
    "            self._ds_info = DatasetInfo(dataset_path)\n",
    "            self._model_type = self._ds_info.model_type\n",
    "            self._numb_class = self._ds_info.numb_class\n",
    "\n",
    "            self._add_mut_info = False\n",
    "\n",
    "        assert \"set\" in self._df.columns, f\"set is not a column in {dataset_path}\"\n",
    "        assert (\n",
    "            \"validation\" in self._df.columns\n",
    "        ), f\"validation is not a column in {dataset_path}\"\n",
    "\n",
    "        \"\"\"\n",
    "        self._df_train = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] != True)\n",
    "        ]\n",
    "        self._df_val = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] == True)\n",
    "        ]\n",
    "        self._df_test = self._df.loc[(self._df[\"set\"] == \"test\")]\n",
    "        \"\"\"\n",
    "\n",
    "        self._df_train, self._df_val, self._df_test = split_train_val_test_df(self._df)\n",
    "\n",
    "        self._df_dict = {\n",
    "            \"train\": self._df_train,\n",
    "            \"val\": self._df_val,\n",
    "            \"test\": self._df_test,\n",
    "        }\n",
    "\n",
    "        assert subset in list(\n",
    "            self._df_dict.keys()\n",
    "        ), \"split can only be 'train', 'val', or 'test'\"\n",
    "        self._subset = subset\n",
    "\n",
    "        self._subdf_len = len(self._df_dict[self._subset])\n",
    "\n",
    "        # not specified seq start will be from 0\n",
    "        if seq_start_idx == False:\n",
    "            self._seq_start_idx = 0\n",
    "        else:\n",
    "            self._seq_start_idx = int(seq_start_idx)\n",
    "        # not specified seq end will be the full sequence length\n",
    "        if seq_end_idx == False:\n",
    "            self._seq_end_idx = -1\n",
    "            self._max_seq_len = self._df.sequence.str.len().max()\n",
    "        else:\n",
    "            self._seq_end_idx = int(seq_end_idx)\n",
    "            self._max_seq_len = self._seq_end_idx - self._seq_start_idx\n",
    "\n",
    "        # get unencoded string of input sequence\n",
    "        # will need to convert data type\n",
    "        self.sequence = self._get_column_value(\"sequence\")\n",
    "\n",
    "        self.if_encode_all = if_encode_all\n",
    "        self._embed_folder = embed_folder\n",
    "\n",
    "        self._encoder_name = encoder_name\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        # get the encoder class\n",
    "        if self._encoder_name in TRANSFORMER_INFO.keys():\n",
    "            encoder_class = ESMEncoder\n",
    "        elif self._encoder_name in CARP_INFO.keys():\n",
    "            encoder_class = CARPEncoder\n",
    "        else:\n",
    "            encoder_class = OnehotEncoder\n",
    "            encoder_params[\"max_seq_len\"] = self._max_seq_len\n",
    "\n",
    "        # get the encoder\n",
    "        self._encoder = encoder_class(\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            **encoder_params,\n",
    "        )\n",
    "        self._total_emb_layer = self._encoder.total_emb_layer\n",
    "        self._embed_layer = embed_layer\n",
    "\n",
    "        # encode all and load in memory\n",
    "        if self.if_encode_all or (\n",
    "            self._embed_folder is None and self._embed_layer is None\n",
    "        ):\n",
    "            print(\"Encoding all...\")\n",
    "            # encode the sequences without the mut_name\n",
    "            # init an empty dict with empty list to append emb\n",
    "            encoded_dict = defaultdict(list)\n",
    "\n",
    "            # use the encoder generator for batch emb\n",
    "            # assume no labels included\n",
    "            for encoded_batch_dict in self._encoder.encode(\n",
    "                mut_seqs=self.sequence,\n",
    "                batch_size=embed_batch_size,\n",
    "                flatten_emb=self._flatten_emb,\n",
    "            ):\n",
    "\n",
    "                for layer, emb in encoded_batch_dict.items():\n",
    "                    encoded_dict[layer].append(emb)\n",
    "\n",
    "            # assign each layer as its own variable\n",
    "            for layer, emb in encoded_dict.items():\n",
    "                setattr(self, \"layer\" + str(layer), np.vstack(emb))\n",
    "\n",
    "        # load full one layer embedding\n",
    "        if self._embed_folder is not None and self._embed_layer is not None:\n",
    "\n",
    "            emb_table = tables.open_file(\n",
    "                os.path.join(\n",
    "                    self._embed_folder,\n",
    "                    self._encoder_name,\n",
    "                    self._flatten_emb,\n",
    "                    self._subset,\n",
    "                    \"embedding.h5\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            emb_table.flush()\n",
    "\n",
    "            print(f\"setting attr for layer {self._embed_layer}\")\n",
    "\n",
    "            setattr(\n",
    "                self,\n",
    "                \"layer\" + str(self._embed_layer),\n",
    "                getattr(emb_table.root, \"layer\" + str(self._embed_layer))[:],\n",
    "            )\n",
    "\n",
    "            emb_table.close()\n",
    "        # get and format the fitness or secondary structure values\n",
    "        # can be numbers or string\n",
    "        # will need to convert data type\n",
    "        # make 1D tensor 2D\n",
    "        self.y = np.expand_dims(self._get_column_value(\"target\"), 1)\n",
    "\n",
    "        # add mut_name and mut_numb for relevant proeng datasets\n",
    "        if self._add_mut_info:\n",
    "            self.mut_name = self._get_column_value(\"mut_name\")\n",
    "            self.mut_numb = self._get_column_value(\"mut_numb\")\n",
    "        else:\n",
    "            self.mut_name = [\"\"] * self._subdf_len\n",
    "            self.mut_numb = [np.nan] * self._subdf_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the selected subset of the dataframe\"\"\"\n",
    "        return self._subdf_len\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Return the item in the order of\n",
    "        target (y), sequence, mut_name (optional), mut_numb (optional),\n",
    "        embedding per layer upto the max number of layer for the encoder\n",
    "\n",
    "        Args:\n",
    "        - idx: int\n",
    "        \"\"\"\n",
    "        if self.if_encode_all and self._embed_folder is None:\n",
    "\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "                *(\n",
    "                    getattr(self, \"layer\" + str(layer))[idx]\n",
    "                    for layer in range(self._total_emb_layer)\n",
    "                ),\n",
    "            )\n",
    "        elif self._embed_folder is not None:\n",
    "            # load the .h5 file with the embeddings\n",
    "            \"\"\"\n",
    "            gb1_emb = tables.open_file(\"embeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/test/embedding.h5\")\n",
    "            gb1_emb.flush()\n",
    "            gb1_emb.root.layer0[0:5]\n",
    "            \"\"\"\n",
    "            # return all\n",
    "            if self._embed_layer is None:\n",
    "\n",
    "                emb_table = tables.open_file(\n",
    "                    os.path.join(\n",
    "                        self._embed_folder,\n",
    "                        self._encoder_name,\n",
    "                        self._flatten_emb,\n",
    "                        self._subset,\n",
    "                        \"embedding.h5\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                emb_table.flush()\n",
    "\n",
    "                layer_embs = [\n",
    "                    getattr(emb_table.root, \"layer\" + str(layer))[idx]\n",
    "                    for layer in range(self._total_emb_layer)\n",
    "                ]\n",
    "\n",
    "                emb_table.close()\n",
    "\n",
    "                return (\n",
    "                    self.y[idx],\n",
    "                    self.sequence[idx],\n",
    "                    self.mut_name[idx],\n",
    "                    self.mut_numb[idx],\n",
    "                    layer_embs,\n",
    "                )\n",
    "            # only pick particular embeding layer\n",
    "            else:\n",
    "                \"\"\"\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"layer\" + str(self._embed_layer),\n",
    "                    getattr(emb_table.root, \"layer\" + str(self._embed_layer)),\n",
    "                )\"\"\"\n",
    "\n",
    "                return (\n",
    "                    self.y[idx],\n",
    "                    self.sequence[idx],\n",
    "                    self.mut_name[idx],\n",
    "                    self.mut_numb[idx],\n",
    "                    # getattr(emb_table.root, \"layer\" + str(self._embed_layer))[idx]\n",
    "                    getattr(self, \"layer\" + str(self._embed_layer))[idx],\n",
    "                )\n",
    "        else:\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "            )\n",
    "\n",
    "    def _get_column_value(self, column_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Check and return the column values of the selected dataframe subset\n",
    "\n",
    "        Args:\n",
    "        - column_name: str, the name of the dataframe column\n",
    "        \"\"\"\n",
    "        if column_name in self._df.columns:\n",
    "            y = self._df_dict[self._subset][column_name]\n",
    "\n",
    "            if column_name == \"sequence\":\n",
    "\n",
    "                return (\n",
    "                    # self._df_dict[self._subset][\"sequence\"]\n",
    "                    y.astype(str)\n",
    "                    .str[self._seq_start_idx : self._seq_end_idx]\n",
    "                    .apply(\n",
    "                        lambda x: x[: int(MAX_SEQ_LEN // 2)]\n",
    "                        + x[-int(MAX_SEQ_LEN // 2) :]\n",
    "                        if len(x) > MAX_SEQ_LEN\n",
    "                        else x\n",
    "                    )\n",
    "                    .values\n",
    "                )\n",
    "            elif column_name == \"target\" and self._model_type == \"LinearClassifier\":\n",
    "                print(\"Converting classes into int...\")\n",
    "                le = LabelEncoder()\n",
    "                return le.fit_transform(y.values.flatten())\n",
    "            elif column_name == \"target\" and self._model_type == \"MultiLabelMultiClass\":\n",
    "                print(\"Converting ss3/ss8 into np.array...\")\n",
    "       \n",
    "                np_y = y.apply(lambda x: np.array(x[1:-1].split(\", \")).astype(\"int\")) + 1\n",
    "                max_len = max(len(y) for y in np_y)\n",
    "                print(\"print np_y shape for dataset\")\n",
    "                print(np.stack([np.pad(i, pad_width=(0, max_len - len(i)), constant_values=0,) for i in np_y]).shape)\n",
    "                return np.stack([np.pad(i, pad_width=(0, max_len - len(i)), constant_values=0,) for i in np_y])\n",
    "            else:\n",
    "                return y.values\n",
    "\n",
    "    @property\n",
    "    def df_full(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the full loaded dataset\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def df_train(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_train\n",
    "\n",
    "    @property\n",
    "    def df_val(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for validation only\"\"\"\n",
    "        return self._df_val\n",
    "\n",
    "    @property\n",
    "    def df_test(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_test\n",
    "\n",
    "    @property\n",
    "    def max_seq_len(self) -> int:\n",
    "        \"\"\"Longest sequence length\"\"\"\n",
    "        return self._max_seq_len\n",
    "\n",
    "\n",
    "def split_protrain_loader(\n",
    "    dataset_path: str,\n",
    "    encoder_name: str,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 128,\n",
    "    flatten_emb: bool | str = False,\n",
    "    embed_folder: str | None = None,\n",
    "    embed_layer: int | None = None,\n",
    "    seq_start_idx: bool | int = False,\n",
    "    seq_end_idx: bool | int = False,\n",
    "    subset_list: list[str] = [\"train\", \"val\", \"test\"],\n",
    "    loader_batch_size: int = 64,\n",
    "    worker_seed: int = RAND_SEED,\n",
    "    if_encode_all: bool = True,\n",
    "    **encoder_params,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    A function encode and load the data from a path\n",
    "\n",
    "    Args:\n",
    "    - dataset_path: str, full path to the dataset, in pkl or panda readable format, ie\n",
    "        \"data/proeng/gb1/low_vs_high.csv\"\n",
    "        columns include: sequence, target, set, validation,\n",
    "        mut_name (optional), mut_numb (optional)\n",
    "    - encoder_name: str, the name of the encoder\n",
    "    - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "    - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "    - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "    - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "    - embed_folder: str = None, path to presaved embedding\n",
    "    - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "    - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "    - subset_list: list of str, train, val, test\n",
    "    - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "    - worker_seed: int, the seed for dataloader\n",
    "    - if_encode_all: bool = True, if encode full dataset all layers on the fly\n",
    "    - encoder_params: kwarg, additional parameters for encoding\n",
    "    \"\"\"\n",
    "\n",
    "    assert set(subset_list) <= set(\n",
    "        [\"train\", \"val\", \"test\"]\n",
    "    ), \"subset_list can only contain terms with in be 'train', 'val', or 'test'\"\n",
    "\n",
    "    # specify no shuffling for validation and test\n",
    "    if_shuffle_list = [True if subset == \"train\" else False for subset in subset_list]\n",
    "\n",
    "    return (\n",
    "        DataLoader(\n",
    "            dataset=ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_folder=embed_folder,\n",
    "                embed_layer=embed_layer,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                if_encode_all=if_encode_all,\n",
    "                **encoder_params,\n",
    "            ),\n",
    "            batch_size=loader_batch_size,\n",
    "            shuffle=if_shuffle,\n",
    "            worker_init_fn=worker_seed,\n",
    "        )\n",
    "        for subset, if_shuffle in zip(subset_list, if_shuffle_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A script with model training and testing details assuming\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "\n",
    "# seed everything\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "torch.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed_all(RAND_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_x_y(\n",
    "    model, device, batch, embed_layer: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    A function process x and y from the loader\n",
    "\n",
    "    Args:\n",
    "    -\n",
    "\n",
    "    Returns:\n",
    "    - x\n",
    "    - y\n",
    "    \"\"\"\n",
    "\n",
    "    # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "    # or: y, seuqence, mut_name, mut_numb, layer0, ...\n",
    "    embs = batch[4]\n",
    "    if torch.is_tensor(embs):\n",
    "        x = embs\n",
    "    else:\n",
    "        x = embs[embed_layer]\n",
    "    y = batch[0]\n",
    "\n",
    "    \"\"\"\n",
    "    # process y depends on model type\n",
    "    # annotation classification\n",
    "    if model.model_name == \"LinearClassifier\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y.flatten())\n",
    "    \"\"\"\n",
    "\n",
    "    # ss3 / ss8 type\n",
    "    \"\"\"if model.model_name == \"MultiLabelMultiClass\":\n",
    "        # convert the y into np.arrays with -1 padding to the same length\n",
    "        y = np.stack(\n",
    "            [\n",
    "                np.pad(i, pad_width=(0, x.shape[1] - len(i)), constant_values=-1,)\n",
    "                for i in y\n",
    "            ]\n",
    "        )\"\"\"\n",
    "    \n",
    "    print(\"x shape for batch\")\n",
    "    print(x.shape, x.dtype)\n",
    "\n",
    "    if len(y.shape) == 3:\n",
    "        y = torch.squeeze(y)\n",
    "    x = x.to(torch.float32)\n",
    "    y = y.to(torch.float32)\n",
    "    \"\"\"if not torch.is_floating_point(y):\n",
    "        y = y.to(x.dtype)\"\"\"\n",
    "    print(\"print y shape for batch\")\n",
    "    print(y.shape, y.dtype)\n",
    "    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 0,\n",
    "    flatten_emb: bool | str = False,\n",
    "    # if_encode_all: bool = True,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    criterion: nn.Module | None = None,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    **encoder_params,\n",
    ") -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    Runs one epoch.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - device: torch.device or str\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - optimizer: optional torch.optim.Optimizer, must also provide criterion,\n",
    "        only provided for training\n",
    "\n",
    "    Returns: \n",
    "    - float, average loss over batches\n",
    "    \"\"\"\n",
    "    if optimizer is not None:\n",
    "        assert criterion is not None\n",
    "        model.train()\n",
    "        is_train = True\n",
    "    else:\n",
    "        model.eval()\n",
    "        is_train = False\n",
    "\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        # if not if_encode_all:\n",
    "        # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "\n",
    "        for batch in loader:\n",
    "            print(\"run epoch...\")\n",
    "\n",
    "            x, y = get_x_y(model, device, batch, embed_layer)\n",
    "\n",
    "            \"\"\"\n",
    "            x = batch[4][embed_layer]\n",
    "            y = batch[0]\n",
    "\n",
    "            # process y depends on model type\n",
    "            # annotation classification\n",
    "            if model.model_name == \"LinearClassifier\":\n",
    "                le = LabelEncoder()\n",
    "                y = le.fit_transform(y.flatten())\n",
    "            # ss3 / ss8 type\n",
    "            elif model.model_name == \"MultiLabelMultiClass\":\n",
    "                # convert the y into np.arrays with -1 padding to the same length\n",
    "                y = np.stack(\n",
    "                    [\n",
    "                        np.pad(\n",
    "                            np.array(i[1:-1].split(\", \")),\n",
    "                            pad_width=(0, x.shape[1] - len(i)),\n",
    "                            constant_values=-1,\n",
    "                        )\n",
    "                        for i in y\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \"\"\"\n",
    "            print(model)\n",
    "            outputs = model(x)\n",
    "            print(\"run epoch\")\n",
    "            print(f\"run epoch output size {outputs.shape}\")\n",
    "            print(\"y shape and (np.arange(y.max()+1) == y[...,None])\")\n",
    "            print(y.shape, (torch.arange(y.max()+1) == y[...,None]).to(torch.float32).shape)\n",
    "\n",
    "            if criterion is not None:\n",
    "                if model.model_name == \"LinearRegression\":\n",
    "                    loss = criterion(outputs, y.float())\n",
    "                elif model.model_name == \"LinearClassifier\":\n",
    "                    loss = criterion(outputs, y.squeeze())\n",
    "                elif model.model_name == \"MultiLabelMultiClass\":\n",
    "                    loss = criterion(outputs, (torch.arange(y.max()+1) == y[...,None]).to(torch.float32))\n",
    "                    # (np.arange(y.max()+1) == y[...,None]).astype(int)\n",
    "\n",
    "                if optimizer is not None:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "    return cum_loss / len(loader)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    encoder_name: str,\n",
    "    embed_layer: int,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 0,\n",
    "    flatten_emb: bool | str = False,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    learning_rate: float = 1e-4,\n",
    "    lr_decay: float = 0.1,\n",
    "    epochs: int = 100,\n",
    "    early_stop: bool = True,\n",
    "    tolerance: int = 10,\n",
    "    min_epoch: int = 5,\n",
    "    **encoder_params,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - train_loader: torch.utils.data.DataLoader, \n",
    "    - val_loader: torch.utils.data.DataLoader, \n",
    "    - criterion: nn.Module, loss function, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float, factor by which to decay LR on plateau\n",
    "    - epochs: int, number of epochs to train for\n",
    "    - early_stop: bool = True,\n",
    "\n",
    "    Returns: \n",
    "    - tuple of np.ndarray, (train_losses, val_losses)\n",
    "        train/val_losses: np.ndarray, shape [epochs], entries are average loss\n",
    "        over batches for that epoch\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=lr_decay\n",
    "    )\n",
    "\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "\n",
    "    # init for early stopping\n",
    "    counter = 0\n",
    "    min_val_loss = np.Inf\n",
    "    print(\"inside train before run_epoch\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        train_losses[epoch] = run_epoch(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            encoder_name=encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            embed_batch_size=embed_batch_size,\n",
    "            flatten_emb=flatten_emb,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        val_loss = run_epoch(\n",
    "            model=model,\n",
    "            loader=val_loader,\n",
    "            encoder_name=encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            embed_batch_size=embed_batch_size,\n",
    "            flatten_emb=flatten_emb,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            optimizer=None,\n",
    "            **encoder_params,\n",
    "        )\n",
    "        val_losses[epoch] = val_loss\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if early_stop:\n",
    "            # when val loss decrease, reset min loss and counter\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if epoch > min_epoch and counter == tolerance:\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    embed_layer: int,\n",
    "    criterion: nn.Module | None,\n",
    "    device: torch.device | str = DEVICE,\n",
    "    print_every: int = 1000,\n",
    ") -> tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Runs one epoch of testing, returning predictions and labels.\n",
    "    \n",
    "    Args:\n",
    "    - model: nn.Module, already moved to device\n",
    "    - device: torch.device or str\n",
    "    - loader: torch.utils.data.DataLoader\n",
    "    - criterion: optional nn.Module, loss function, already moved to device\n",
    "    - print_every: int, how often (number of batches) to print avg loss\n",
    "    \n",
    "    Returns: tuple (avg_loss, preds, labels)\n",
    "    - avg_loss: float, average loss per training example \n",
    "    - preds: np.ndarray, shape [num_examples, ...], predictions over dataset\n",
    "    - labels: np.ndarray, shape [num_examples, ...], dataset labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    msg = \"[{step:5d}] loss: {loss:.3f}\"\n",
    "\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    pred_probs = []\n",
    "    pred_classes = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(tqdm(loader)):\n",
    "            # for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\n",
    "            x, y = get_x_y(model, device, batch, embed_layer)\n",
    "\n",
    "            \"\"\"\n",
    "            x = batch[4][embed_layer]\n",
    "            y = batch[0]\n",
    "\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \"\"\"\n",
    "            print(\"before model(x)\")\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x)\n",
    "            print(\"after model(x) give pred shape\")\n",
    "\n",
    "            print(outputs.shape)\n",
    "\n",
    "            # append results\n",
    "            labels.append(y.detach().cpu().squeeze().numpy())\n",
    "\n",
    "            # append class\n",
    "            if model.model_name == \"LinearClassifier\" or \"MultiLabelMultiClass\":\n",
    "                pred_classes.append(\n",
    "                    outputs.detach()\n",
    "                    .cpu()\n",
    "                    .data.max(1, keepdim=True)[1]\n",
    "                    .squeeze()\n",
    "                    .numpy()\n",
    "                )\n",
    "                # pred_probs.append(outputs.detach().cpu().squeeze().numpy())\n",
    "            # else:\n",
    "            pred_probs.append(outputs.detach().cpu().squeeze().numpy())\n",
    "\n",
    "            if criterion is not None:\n",
    "                if model.model_name == \"LinearRegression\":\n",
    "                    loss = criterion(outputs, y)\n",
    "                elif model.model_name == \"LinearClassifier\" or \"MultiLabelMultiClass\":\n",
    "                    loss = criterion(outputs, y.squeeze())\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "                if ((i + 1) % print_every == 0) or (i + 1 == len(loader)):\n",
    "                    tqdm.write(msg.format(step=i + 1, loss=cum_loss / len(loader)))\n",
    "\n",
    "    avg_loss = cum_loss / len(loader)\n",
    "\n",
    "    if pred_classes == []:\n",
    "        pred_classes_conc = pred_classes\n",
    "    else:\n",
    "        pred_classes_conc = np.concatenate(pred_classes)\n",
    "\n",
    "    return (\n",
    "        avg_loss,\n",
    "        np.concatenate(pred_probs),\n",
    "        pred_classes_conc,\n",
    "        np.concatenate(labels),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for running pytorch models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent import futures\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import ndcg_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.params.aa import AA_NUMB\n",
    "from scr.params.sys import RAND_SEED, DEVICE\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO, MAX_SEQ_LEN\n",
    "\n",
    "# from scr.preprocess.data_process import split_protrain_loader, DatasetInfo\n",
    "from scr.encoding.encoding_classes import OnehotEncoder, ESMEncoder, CARPEncoder, get_emb_info\n",
    "# from scr.model.pytorch_model import LinearRegression, LinearClassifier\n",
    "\n",
    "# from scr.model.train_test import train, test\n",
    "from scr.vis.learning_vis import plot_lc\n",
    "from scr.utils import get_folder_file_names, pickle_save, get_default_output_path\n",
    "\n",
    "\n",
    "class Run_Pytorch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        loader_batch_size: int = 64,\n",
    "        worker_seed: int = RAND_SEED,\n",
    "        if_encode_all: bool = True,\n",
    "        if_multiprocess: bool = False,\n",
    "        learning_rate: float = 1e-4,\n",
    "        lr_decay: float = 0.1,\n",
    "        epochs: int = 100,\n",
    "        early_stop: bool = True,\n",
    "        tolerance: int = 10,\n",
    "        min_epoch: int = 5,\n",
    "        device: torch.device | str = DEVICE,\n",
    "        all_plot_folder: str = \"results/learning_curves\",\n",
    "        all_result_folder: str = \"results/train_val_test\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running pytorch model\n",
    "\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        \n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "        - worker_seed: int, the seed for dataloader\n",
    "        - learning_rate: float\n",
    "        - lr_decay: float, factor by which to decay LR on plateau\n",
    "        - epochs: int, number of epochs to train for\n",
    "        - device: torch.device or str\n",
    "        - all_plot_folder: str, the parent folder path for saving all the learning curves\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "\n",
    "        Returns:\n",
    "        - result_dict: dict, with the keys and dict values\n",
    "            \"losses\": {\"train_losses\": np.ndarray, \"val_losses\": np.ndarray}\n",
    "            \"train\": {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float, \n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._dataset_path = dataset_path\n",
    "        self._encoder_name = encoder_name\n",
    "\n",
    "        if self._encoder_name not in (list(TRANSFORMER_INFO.keys()) + list(CARP_INFO.keys())):\n",
    "            self._encoder_name = \"onehot\"\n",
    "\n",
    "        self._reset_param = reset_param\n",
    "        self._resample_param = resample_param\n",
    "        self._embed_batch_size = embed_batch_size\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._lr_decay = lr_decay\n",
    "        self._epochs = epochs\n",
    "        self._early_stop = early_stop\n",
    "        self._tolerance = tolerance\n",
    "        self._min_epoch = min_epoch\n",
    "        self._device = device\n",
    "        self._all_plot_folder = all_plot_folder\n",
    "        self._all_result_folder = all_result_folder\n",
    "        self._encoder_params = encoder_params\n",
    "\n",
    "        self._ds_info = DatasetInfo(self._dataset_path)\n",
    "        self._model_type = self._ds_info.model_type\n",
    "        self._numb_class = self._ds_info.numb_class\n",
    "\n",
    "        self._train_loader, self._val_loader, self._test_loader = split_protrain_loader(\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            embed_folder=embed_folder,\n",
    "            seq_start_idx=seq_start_idx,\n",
    "            seq_end_idx=seq_end_idx,\n",
    "            subset_list=[\"train\", \"val\", \"test\"],\n",
    "            loader_batch_size=loader_batch_size,\n",
    "            worker_seed=worker_seed,\n",
    "            if_encode_all=if_encode_all,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        encoder_name, encoder_class, total_emb_layer = get_emb_info(encoder_name)\n",
    "\n",
    "        if encoder_class == ESMEncoder:\n",
    "            self._encoder_info_dict = TRANSFORMER_INFO\n",
    "        elif encoder_class == CARPEncoder:\n",
    "            self._encoder_info_dict = CARP_INFO\n",
    "        elif encoder_class == OnehotEncoder:\n",
    "            #TODO aultoto\n",
    "            if self._flatten_emb == False:\n",
    "                self._encoder_info_dict = {\"onehot\": (AA_NUMB, )}\n",
    "            else:\n",
    "                self._encoder_info_dict = {\"onehot\": (MAX_SEQ_LEN*22, )}\n",
    "\n",
    "        if if_multiprocess:\n",
    "            print(\"Running different emb layer in parallel...\")\n",
    "            # add the thredpool max_workers=None\n",
    "            with futures.ProcessPoolExecutor(max_workers=os.cpu_count() - 1) as pool:\n",
    "                # for each layer train the model and save the model\n",
    "                for embed_layer in tqdm(range(total_emb_layer)):\n",
    "                    pool.submit(self.run_pytorch_layer, embed_layer)\n",
    "\n",
    "        else:\n",
    "            for embed_layer in range(total_emb_layer):\n",
    "                print(f\"Running pytorch model for layer {embed_layer}\")\n",
    "                self.run_pytorch_layer(embed_layer)\n",
    "\n",
    "    def run_pytorch_layer(self, embed_layer):\n",
    "\n",
    "        # init model based on datasets\n",
    "        if self._model_type == \"LinearRegression\":\n",
    "            model = LinearRegression(\n",
    "                input_dim=self._encoder_info_dict[self._encoder_name][0], output_dim=1\n",
    "            )\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "        elif self._model_type == \"LinearClassifier\":\n",
    "            model = LinearClassifier(\n",
    "                input_dim=self._encoder_info_dict[self._encoder_name][0],\n",
    "                numb_class=self._numb_class,\n",
    "            )\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif self._model_type == \"MultiLabelMultiClass\":\n",
    "            model = MultiLabelMultiClass(\n",
    "                input_dim=self._encoder_info_dict[self._encoder_name][0],\n",
    "                numb_class=self._numb_class,\n",
    "            )\n",
    "            # criterion = nn.CrossEntropyLoss()\n",
    "            criterion = nn.BCELoss()\n",
    "\n",
    "        model.to(self._device, non_blocking=True)\n",
    "\n",
    "        criterion.to(self._device, non_blocking=True)\n",
    "\n",
    "        train_losses, val_losses = train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=self._train_loader,\n",
    "            val_loader=self._val_loader,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            reset_param=self._reset_param,\n",
    "            resample_param=self._resample_param,\n",
    "            embed_batch_size=self._embed_batch_size,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            device=self._device,\n",
    "            learning_rate=self._learning_rate,\n",
    "            lr_decay=self._lr_decay,\n",
    "            epochs=self._epochs,\n",
    "            early_stop=self._early_stop,\n",
    "            tolerance=self._tolerance,\n",
    "            min_epoch=self._min_epoch,\n",
    "            **self._encoder_params,\n",
    "        )\n",
    "\n",
    "        # record the losses\n",
    "        result_dict = {\n",
    "            \"losses\": {\"train_losses\": train_losses, \"val_losses\": val_losses}\n",
    "        }\n",
    "\n",
    "        plot_lc(\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "            all_plot_folder=get_default_output_path(self._all_plot_folder),\n",
    "        )\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, loader in zip(\n",
    "            [\"train\", \"val\", \"test\"],\n",
    "            [self._train_loader, self._val_loader, self._test_loader],\n",
    "        ):\n",
    "\n",
    "            loss, pred, cls, true = test(\n",
    "                model=model,\n",
    "                loader=loader,\n",
    "                embed_layer=embed_layer,\n",
    "                device=self._device,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "\n",
    "            if model.model_name == \"LinearRegression\":\n",
    "                result_dict[subset] = {\n",
    "                    \"mse\": loss,\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"ndcg\": ndcg_score(true[None, :], pred[None, :]),\n",
    "                    \"rho\": spearmanr(true, pred),\n",
    "                }\n",
    "\n",
    "            elif model.model_name == \"LinearClassifier\" or \"MultiLabelMultiClass\":\n",
    "                result_dict[subset] = {\n",
    "                    \"cross-entropy\": loss,\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"acc\": accuracy_score(true, cls),\n",
    "                    \"rocauc\": roc_auc_score(\n",
    "                        true,\n",
    "                        nn.Softmax(dim=1)(torch.from_numpy(pred)).numpy(),\n",
    "                        multi_class=\"ovr\",\n",
    "                    )\n",
    "                    # \"rocauc\": eval_rocauc(true, pred),\n",
    "                }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=get_default_output_path(self._all_result_folder),\n",
    "            dataset_path=self._dataset_path,\n",
    "            encoder_name=self._encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self._flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n",
      "Encoding all...\n",
      "Converting ss3/ss8 into np.array...\n",
      "print np_y shape for dataset\n",
      "(8678, 1632)\n",
      "Generating onehot upto 0 layer embedding ...\n",
      "Encoding all...\n",
      "Converting ss3/ss8 into np.array...\n",
      "print np_y shape for dataset\n",
      "(2170, 1228)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n",
      "Encoding all...\n",
      "Converting ss3/ss8 into np.array...\n",
      "print np_y shape for dataset\n",
      "(21, 1494)\n",
      "Running pytorch model for layer 0\n",
      "inside train before run_epoch\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2546, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2622, 0.2425, 0.2515, 0.2438],\n",
      "         [0.2748, 0.2466, 0.2417, 0.2369],\n",
      "         ...,\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2465, 0.2128, 0.2732, 0.2675],\n",
      "         [0.2597, 0.2802, 0.2096, 0.2505],\n",
      "         [0.2748, 0.2466, 0.2417, 0.2369],\n",
      "         ...,\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2622, 0.2425, 0.2515, 0.2438],\n",
      "         [0.2441, 0.2596, 0.2665, 0.2298],\n",
      "         [0.2953, 0.2591, 0.2566, 0.1890],\n",
      "         ...,\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2465, 0.2128, 0.2732, 0.2675],\n",
      "         [0.3041, 0.2571, 0.2195, 0.2193],\n",
      "         [0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         ...,\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2765, 0.2651, 0.2166, 0.2418],\n",
      "         [0.2622, 0.2425, 0.2515, 0.2438],\n",
      "         [0.3041, 0.2571, 0.2195, 0.2193],\n",
      "         ...,\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2816, 0.2174, 0.2910, 0.2101],\n",
      "         [0.3041, 0.2571, 0.2195, 0.2193],\n",
      "         [0.2465, 0.2128, 0.2732, 0.2675],\n",
      "         ...,\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2640, 0.2584, 0.2447, 0.2329]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2465, 0.2128, 0.2732, 0.2675],\n",
      "         [0.2547, 0.2836, 0.2095, 0.2522],\n",
      "         [0.2748, 0.2466, 0.2417, 0.2370],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2465, 0.2128, 0.2732, 0.2675],\n",
      "         [0.2322, 0.2732, 0.2509, 0.2437],\n",
      "         [0.2598, 0.2802, 0.2095, 0.2505],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         [0.2765, 0.2651, 0.2166, 0.2418],\n",
      "         [0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2622, 0.2426, 0.2514, 0.2438],\n",
      "         [0.2622, 0.2426, 0.2514, 0.2438],\n",
      "         [0.2748, 0.2466, 0.2417, 0.2370],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2465, 0.2128, 0.2732, 0.2675],\n",
      "         [0.2547, 0.2836, 0.2095, 0.2522],\n",
      "         [0.2982, 0.2066, 0.2612, 0.2340],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329]],\n",
      "\n",
      "        [[0.2816, 0.2174, 0.2909, 0.2101],\n",
      "         [0.2765, 0.2651, 0.2166, 0.2418],\n",
      "         [0.2322, 0.2732, 0.2509, 0.2437],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2329]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.3041, 0.2571, 0.2194, 0.2194],\n",
      "         [0.2953, 0.2591, 0.2566, 0.1890],\n",
      "         [0.2817, 0.2173, 0.2909, 0.2101],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328]],\n",
      "\n",
      "        [[0.2546, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2787, 0.2313, 0.2169, 0.2731],\n",
      "         [0.2765, 0.2651, 0.2165, 0.2418],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328]],\n",
      "\n",
      "        [[0.2546, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2817, 0.2173, 0.2909, 0.2101],\n",
      "         [0.2123, 0.2937, 0.2596, 0.2344],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2598, 0.2802, 0.2095, 0.2505],\n",
      "         [0.2598, 0.2802, 0.2095, 0.2505],\n",
      "         [0.2622, 0.2426, 0.2514, 0.2438],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328]],\n",
      "\n",
      "        [[0.2817, 0.2173, 0.2909, 0.2101],\n",
      "         [0.2546, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2598, 0.2802, 0.2095, 0.2505],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328]],\n",
      "\n",
      "        [[0.2465, 0.2128, 0.2731, 0.2675],\n",
      "         [0.3041, 0.2571, 0.2194, 0.2194],\n",
      "         [0.2355, 0.2779, 0.2525, 0.2342],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2447, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2817, 0.2173, 0.2909, 0.2101],\n",
      "         [0.2498, 0.2257, 0.2730, 0.2515],\n",
      "         [0.2498, 0.2257, 0.2730, 0.2515],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2465, 0.2129, 0.2731, 0.2675],\n",
      "         [0.2547, 0.2836, 0.2095, 0.2522],\n",
      "         [0.2622, 0.2426, 0.2514, 0.2438],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         [0.2123, 0.2937, 0.2596, 0.2344],\n",
      "         [0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3041, 0.2571, 0.2194, 0.2194],\n",
      "         [0.2546, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2598, 0.2802, 0.2095, 0.2505],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         [0.2817, 0.2173, 0.2909, 0.2101],\n",
      "         [0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         [0.3007, 0.2072, 0.2513, 0.2409],\n",
      "         [0.2817, 0.2173, 0.2909, 0.2101],\n",
      "         ...,\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2641, 0.2584, 0.2446, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2546, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2123, 0.2937, 0.2596, 0.2344],\n",
      "         [0.3007, 0.2072, 0.2512, 0.2409],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2465, 0.2129, 0.2731, 0.2676],\n",
      "         [0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         [0.3007, 0.2072, 0.2512, 0.2409],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2465, 0.2129, 0.2731, 0.2676],\n",
      "         [0.2598, 0.2802, 0.2094, 0.2506],\n",
      "         [0.2748, 0.2466, 0.2416, 0.2370],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2465, 0.2129, 0.2731, 0.2676],\n",
      "         [0.2817, 0.2173, 0.2908, 0.2101],\n",
      "         [0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2817, 0.2173, 0.2908, 0.2101],\n",
      "         [0.2546, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2547, 0.2836, 0.2095, 0.2522],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2165, 0.2418],\n",
      "         [0.2748, 0.2466, 0.2416, 0.2370],\n",
      "         [0.2498, 0.2257, 0.2729, 0.2516],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2465, 0.2129, 0.2730, 0.2676],\n",
      "         [0.2817, 0.2173, 0.2908, 0.2102],\n",
      "         [0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2465, 0.2129, 0.2730, 0.2676],\n",
      "         [0.2748, 0.2467, 0.2415, 0.2370],\n",
      "         [0.2442, 0.2595, 0.2663, 0.2299],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2465, 0.2129, 0.2730, 0.2676],\n",
      "         [0.2623, 0.2426, 0.2513, 0.2438],\n",
      "         [0.2748, 0.2467, 0.2415, 0.2370],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2465, 0.2129, 0.2730, 0.2676],\n",
      "         [0.2912, 0.2384, 0.2133, 0.2571],\n",
      "         [0.2547, 0.2835, 0.2095, 0.2523],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2817, 0.2173, 0.2908, 0.2102],\n",
      "         [0.2442, 0.2595, 0.2663, 0.2299],\n",
      "         [0.2817, 0.2173, 0.2908, 0.2102],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2123, 0.2937, 0.2595, 0.2344],\n",
      "         [0.3007, 0.2072, 0.2512, 0.2409],\n",
      "         [0.2787, 0.2313, 0.2169, 0.2731],\n",
      "         ...,\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328],\n",
      "         [0.2642, 0.2584, 0.2446, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         [0.2623, 0.2426, 0.2513, 0.2439],\n",
      "         [0.3042, 0.2571, 0.2193, 0.2194],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         [0.2818, 0.2173, 0.2908, 0.2102],\n",
      "         [0.2323, 0.2733, 0.2507, 0.2437],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2730, 0.2676],\n",
      "         [0.2912, 0.2384, 0.2133, 0.2571],\n",
      "         [0.3007, 0.2072, 0.2512, 0.2409],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2730, 0.2676],\n",
      "         [0.2598, 0.2802, 0.2094, 0.2506],\n",
      "         [0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2818, 0.2173, 0.2908, 0.2102],\n",
      "         [0.3042, 0.2571, 0.2193, 0.2194],\n",
      "         [0.2466, 0.2129, 0.2730, 0.2676],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2818, 0.2173, 0.2908, 0.2102],\n",
      "         [0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         [0.2466, 0.2129, 0.2730, 0.2676],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2818, 0.2172, 0.2908, 0.2102],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         [0.2547, 0.2946, 0.2337, 0.2171],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         [0.2466, 0.2129, 0.2729, 0.2676],\n",
      "         [0.2124, 0.2938, 0.2595, 0.2344],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2729, 0.2676],\n",
      "         [0.2124, 0.2938, 0.2595, 0.2344],\n",
      "         [0.2912, 0.2384, 0.2133, 0.2571],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2729, 0.2676],\n",
      "         [0.2818, 0.2172, 0.2908, 0.2102],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2818, 0.2172, 0.2908, 0.2102],\n",
      "         [0.2547, 0.2946, 0.2337, 0.2171],\n",
      "         [0.2623, 0.2426, 0.2512, 0.2439],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2466, 0.2129, 0.2729, 0.2676],\n",
      "         [0.2818, 0.2172, 0.2907, 0.2102],\n",
      "         [0.2623, 0.2426, 0.2512, 0.2439],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2818, 0.2172, 0.2907, 0.2102],\n",
      "         [0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         [0.3007, 0.2072, 0.2512, 0.2409],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2729, 0.2676],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         [0.2466, 0.2129, 0.2729, 0.2676],\n",
      "         [0.2443, 0.2594, 0.2663, 0.2300],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2818, 0.2172, 0.2907, 0.2102],\n",
      "         [0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.3042, 0.2572, 0.2192, 0.2194],\n",
      "         [0.2766, 0.2651, 0.2164, 0.2419],\n",
      "         [0.2818, 0.2172, 0.2907, 0.2102],\n",
      "         ...,\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2643, 0.2583, 0.2446, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.3042, 0.2572, 0.2192, 0.2194],\n",
      "         [0.3007, 0.2072, 0.2511, 0.2409],\n",
      "         [0.2124, 0.2938, 0.2594, 0.2345],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2336, 0.2171],\n",
      "         [0.2912, 0.2384, 0.2134, 0.2570],\n",
      "         [0.2912, 0.2384, 0.2134, 0.2570],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2787, 0.2313, 0.2169, 0.2731],\n",
      "         [0.2499, 0.2258, 0.2727, 0.2516],\n",
      "         [0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2729, 0.2676],\n",
      "         [0.2124, 0.2938, 0.2594, 0.2345],\n",
      "         [0.2623, 0.2426, 0.2511, 0.2439],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2819, 0.2172, 0.2907, 0.2102],\n",
      "         [0.2766, 0.2651, 0.2163, 0.2419],\n",
      "         [0.2356, 0.2778, 0.2523, 0.2343],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328]],\n",
      "\n",
      "        [[0.2819, 0.2172, 0.2907, 0.2102],\n",
      "         [0.2766, 0.2651, 0.2163, 0.2419],\n",
      "         [0.3042, 0.2572, 0.2192, 0.2194],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2328]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2466, 0.2129, 0.2728, 0.2676],\n",
      "         [0.2819, 0.2172, 0.2907, 0.2103],\n",
      "         [0.2766, 0.2651, 0.2163, 0.2419],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2163, 0.2419],\n",
      "         [0.2598, 0.2803, 0.2093, 0.2506],\n",
      "         [0.2124, 0.2938, 0.2593, 0.2345],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327]],\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2336, 0.2171],\n",
      "         [0.2766, 0.2651, 0.2163, 0.2419],\n",
      "         [0.2749, 0.2467, 0.2413, 0.2371],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2336, 0.2171],\n",
      "         [0.2323, 0.2733, 0.2505, 0.2438],\n",
      "         [0.3008, 0.2072, 0.2511, 0.2409],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327]],\n",
      "\n",
      "        [[0.2766, 0.2651, 0.2163, 0.2419],\n",
      "         [0.2623, 0.2427, 0.2511, 0.2439],\n",
      "         [0.3042, 0.2572, 0.2191, 0.2194],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327]],\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2728, 0.2676],\n",
      "         [0.2766, 0.2651, 0.2163, 0.2419],\n",
      "         [0.3008, 0.2072, 0.2511, 0.2409],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2446, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2444, 0.2594, 0.2662, 0.2301],\n",
      "         [0.2767, 0.2651, 0.2163, 0.2419],\n",
      "         [0.2623, 0.2427, 0.2511, 0.2439],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.3043, 0.2572, 0.2191, 0.2194],\n",
      "         [0.2767, 0.2651, 0.2163, 0.2419],\n",
      "         [0.2466, 0.2129, 0.2728, 0.2677],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2728, 0.2677],\n",
      "         [0.2324, 0.2733, 0.2505, 0.2438],\n",
      "         [0.2819, 0.2172, 0.2906, 0.2103],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2819, 0.2172, 0.2906, 0.2103],\n",
      "         [0.2356, 0.2778, 0.2522, 0.2343],\n",
      "         [0.2466, 0.2129, 0.2728, 0.2677],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.3043, 0.2572, 0.2191, 0.2194],\n",
      "         [0.2466, 0.2129, 0.2728, 0.2677],\n",
      "         [0.2819, 0.2172, 0.2906, 0.2103],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2466, 0.2129, 0.2728, 0.2677],\n",
      "         [0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         [0.2599, 0.2803, 0.2092, 0.2506],\n",
      "         ...,\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2644, 0.2583, 0.2445, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2547, 0.2946, 0.2336, 0.2172],\n",
      "         [0.3043, 0.2572, 0.2191, 0.2195],\n",
      "         [0.2357, 0.2778, 0.2522, 0.2344],\n",
      "         ...,\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2767, 0.2651, 0.2162, 0.2419],\n",
      "         [0.2623, 0.2427, 0.2510, 0.2439],\n",
      "         [0.3043, 0.2572, 0.2191, 0.2195],\n",
      "         ...,\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2819, 0.2171, 0.2906, 0.2103],\n",
      "         [0.3043, 0.2572, 0.2191, 0.2195],\n",
      "         [0.2499, 0.2258, 0.2726, 0.2517],\n",
      "         ...,\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3008, 0.2072, 0.2511, 0.2410],\n",
      "         [0.2547, 0.2834, 0.2095, 0.2523],\n",
      "         [0.2444, 0.2593, 0.2662, 0.2301],\n",
      "         ...,\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2466, 0.2130, 0.2727, 0.2677],\n",
      "         [0.2623, 0.2427, 0.2510, 0.2439],\n",
      "         [0.2749, 0.2467, 0.2413, 0.2371],\n",
      "         ...,\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2767, 0.2651, 0.2162, 0.2419],\n",
      "         [0.3008, 0.2072, 0.2511, 0.2410],\n",
      "         [0.2851, 0.3067, 0.2114, 0.1968],\n",
      "         ...,\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2583, 0.2445, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2466, 0.2130, 0.2727, 0.2677],\n",
      "         [0.2357, 0.2778, 0.2521, 0.2344],\n",
      "         [0.2357, 0.2778, 0.2521, 0.2344],\n",
      "         ...,\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2466, 0.2130, 0.2727, 0.2677],\n",
      "         [0.2820, 0.2171, 0.2906, 0.2103],\n",
      "         [0.2767, 0.2651, 0.2162, 0.2420],\n",
      "         ...,\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2466, 0.2130, 0.2727, 0.2677],\n",
      "         [0.3043, 0.2572, 0.2190, 0.2195],\n",
      "         [0.2357, 0.2778, 0.2521, 0.2344],\n",
      "         ...,\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2335, 0.2172],\n",
      "         [0.2820, 0.2171, 0.2906, 0.2103],\n",
      "         [0.2767, 0.2651, 0.2162, 0.2420],\n",
      "         ...,\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2906, 0.2103],\n",
      "         [0.2444, 0.2593, 0.2661, 0.2301],\n",
      "         [0.2547, 0.2946, 0.2335, 0.2172],\n",
      "         ...,\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2906, 0.2103],\n",
      "         [0.2357, 0.2778, 0.2521, 0.2344],\n",
      "         [0.3043, 0.2572, 0.2190, 0.2195],\n",
      "         ...,\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2645, 0.2582, 0.2445, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2467, 0.2130, 0.2727, 0.2677],\n",
      "         [0.2767, 0.2651, 0.2162, 0.2420],\n",
      "         [0.3008, 0.2072, 0.2510, 0.2410],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2906, 0.2103],\n",
      "         [0.3008, 0.2072, 0.2510, 0.2410],\n",
      "         [0.2445, 0.2593, 0.2661, 0.2301],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2727, 0.2677],\n",
      "         [0.2820, 0.2171, 0.2906, 0.2103],\n",
      "         [0.2767, 0.2651, 0.2162, 0.2420],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2906, 0.2103],\n",
      "         [0.2767, 0.2651, 0.2162, 0.2420],\n",
      "         [0.2624, 0.2427, 0.2510, 0.2440],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2727, 0.2677],\n",
      "         [0.2912, 0.2385, 0.2134, 0.2569],\n",
      "         [0.2547, 0.2834, 0.2096, 0.2523],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2335, 0.2172],\n",
      "         [0.2547, 0.2834, 0.2096, 0.2523],\n",
      "         [0.2599, 0.2803, 0.2091, 0.2507],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         [0.3008, 0.2072, 0.2510, 0.2410],\n",
      "         [0.3008, 0.2072, 0.2510, 0.2410],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2767, 0.2652, 0.2162, 0.2420],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         [0.2445, 0.2593, 0.2661, 0.2302],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2335, 0.2172],\n",
      "         [0.2357, 0.2778, 0.2521, 0.2344],\n",
      "         [0.2324, 0.2734, 0.2504, 0.2438],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2905, 0.2104],\n",
      "         [0.2767, 0.2652, 0.2162, 0.2420],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2905, 0.2104],\n",
      "         [0.2445, 0.2593, 0.2661, 0.2302],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2726, 0.2677],\n",
      "         [0.2750, 0.2468, 0.2412, 0.2371],\n",
      "         [0.2624, 0.2427, 0.2509, 0.2440],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2467, 0.2130, 0.2726, 0.2677],\n",
      "         [0.2548, 0.2833, 0.2096, 0.2523],\n",
      "         [0.2548, 0.2833, 0.2096, 0.2523],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2905, 0.2104],\n",
      "         [0.2358, 0.2777, 0.2521, 0.2345],\n",
      "         [0.2467, 0.2130, 0.2726, 0.2677],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2905, 0.2104],\n",
      "         [0.2358, 0.2777, 0.2521, 0.2345],\n",
      "         [0.2467, 0.2130, 0.2726, 0.2677],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2767, 0.2652, 0.2161, 0.2420],\n",
      "         [0.2358, 0.2777, 0.2521, 0.2345],\n",
      "         [0.2324, 0.2734, 0.2503, 0.2439],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2820, 0.2171, 0.2905, 0.2104],\n",
      "         [0.2445, 0.2592, 0.2661, 0.2302],\n",
      "         [0.2324, 0.2734, 0.2503, 0.2439],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2334, 0.2172],\n",
      "         [0.2767, 0.2652, 0.2161, 0.2420],\n",
      "         [0.3008, 0.2072, 0.2510, 0.2410],\n",
      "         ...,\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2646, 0.2582, 0.2445, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2821, 0.2171, 0.2905, 0.2104],\n",
      "         [0.2547, 0.2946, 0.2334, 0.2172],\n",
      "         [0.2500, 0.2259, 0.2724, 0.2517],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2725, 0.2677],\n",
      "         [0.2624, 0.2427, 0.2509, 0.2440],\n",
      "         [0.2125, 0.2939, 0.2591, 0.2346],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2547, 0.2946, 0.2334, 0.2172],\n",
      "         [0.2125, 0.2939, 0.2591, 0.2346],\n",
      "         [0.2599, 0.2804, 0.2090, 0.2507],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2821, 0.2171, 0.2905, 0.2104],\n",
      "         [0.2767, 0.2652, 0.2161, 0.2420],\n",
      "         [0.2358, 0.2777, 0.2520, 0.2345],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2725, 0.2677],\n",
      "         [0.2767, 0.2652, 0.2161, 0.2420],\n",
      "         [0.3008, 0.2072, 0.2509, 0.2410],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327]],\n",
      "\n",
      "        [[0.2599, 0.2804, 0.2090, 0.2507],\n",
      "         [0.3043, 0.2573, 0.2189, 0.2195],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2327]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2467, 0.2130, 0.2725, 0.2678],\n",
      "         [0.2125, 0.2939, 0.2590, 0.2346],\n",
      "         [0.2624, 0.2427, 0.2508, 0.2440],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326]],\n",
      "\n",
      "        [[0.2767, 0.2652, 0.2161, 0.2420],\n",
      "         [0.2624, 0.2427, 0.2508, 0.2440],\n",
      "         [0.3043, 0.2573, 0.2189, 0.2195],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2725, 0.2678],\n",
      "         [0.2125, 0.2939, 0.2590, 0.2346],\n",
      "         [0.2821, 0.2170, 0.2904, 0.2104],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2725, 0.2678],\n",
      "         [0.2767, 0.2652, 0.2161, 0.2420],\n",
      "         [0.2913, 0.2385, 0.2134, 0.2568],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326]],\n",
      "\n",
      "        [[0.2548, 0.2833, 0.2096, 0.2523],\n",
      "         [0.2500, 0.2259, 0.2724, 0.2518],\n",
      "         [0.2624, 0.2427, 0.2508, 0.2440],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2725, 0.2678],\n",
      "         [0.2548, 0.2833, 0.2096, 0.2523],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326],\n",
      "         [0.2647, 0.2582, 0.2445, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2467, 0.2130, 0.2725, 0.2678],\n",
      "         [0.3044, 0.2573, 0.2188, 0.2195],\n",
      "         [0.3008, 0.2073, 0.2509, 0.2410],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2953, 0.2591, 0.2565, 0.1890],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         [0.2768, 0.2652, 0.2160, 0.2420],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2750, 0.2468, 0.2410, 0.2372],\n",
      "         [0.3044, 0.2573, 0.2188, 0.2195],\n",
      "         [0.2358, 0.2777, 0.2520, 0.2345],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2324, 0.2734, 0.2502, 0.2439],\n",
      "         [0.2467, 0.2130, 0.2725, 0.2678],\n",
      "         [0.2125, 0.2939, 0.2590, 0.2346],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2467, 0.2130, 0.2725, 0.2678],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         [0.2599, 0.2804, 0.2090, 0.2507],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2768, 0.2652, 0.2160, 0.2420],\n",
      "         [0.2125, 0.2939, 0.2590, 0.2346],\n",
      "         [0.2125, 0.2939, 0.2590, 0.2346],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.3044, 0.2573, 0.2188, 0.2195],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         [0.2548, 0.2833, 0.2096, 0.2524],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2467, 0.2131, 0.2724, 0.2678],\n",
      "         [0.2750, 0.2468, 0.2410, 0.2372],\n",
      "         [0.2446, 0.2592, 0.2660, 0.2303],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2548, 0.2947, 0.2333, 0.2172],\n",
      "         [0.2822, 0.2170, 0.2904, 0.2105],\n",
      "         [0.2768, 0.2652, 0.2160, 0.2420],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2467, 0.2131, 0.2724, 0.2678],\n",
      "         [0.2768, 0.2652, 0.2160, 0.2420],\n",
      "         [0.2865, 0.2408, 0.2588, 0.2139],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2467, 0.2131, 0.2724, 0.2678],\n",
      "         [0.2750, 0.2468, 0.2410, 0.2372],\n",
      "         [0.2625, 0.2427, 0.2507, 0.2440],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2913, 0.2385, 0.2134, 0.2568],\n",
      "         [0.2822, 0.2170, 0.2904, 0.2105],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2582, 0.2444, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2467, 0.2131, 0.2724, 0.2678],\n",
      "         [0.2750, 0.2468, 0.2409, 0.2372],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2467, 0.2131, 0.2724, 0.2678],\n",
      "         [0.3044, 0.2573, 0.2188, 0.2195],\n",
      "         [0.2548, 0.2833, 0.2096, 0.2524],\n",
      "         ...,\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2548, 0.2947, 0.2333, 0.2172],\n",
      "         [0.2600, 0.2804, 0.2089, 0.2507],\n",
      "         [0.2865, 0.2408, 0.2588, 0.2139],\n",
      "         ...,\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2822, 0.2170, 0.2903, 0.2105],\n",
      "         [0.2768, 0.2652, 0.2160, 0.2420],\n",
      "         [0.2358, 0.2777, 0.2519, 0.2345],\n",
      "         ...,\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2822, 0.2170, 0.2903, 0.2105],\n",
      "         [0.2446, 0.2591, 0.2659, 0.2303],\n",
      "         [0.2358, 0.2777, 0.2519, 0.2345],\n",
      "         ...,\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2822, 0.2170, 0.2903, 0.2105],\n",
      "         [0.2358, 0.2777, 0.2519, 0.2345],\n",
      "         [0.2358, 0.2777, 0.2519, 0.2345],\n",
      "         ...,\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2648, 0.2581, 0.2444, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2822, 0.2170, 0.2903, 0.2105],\n",
      "         [0.2548, 0.2947, 0.2333, 0.2172],\n",
      "         [0.2625, 0.2428, 0.2507, 0.2441],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         [0.3044, 0.2573, 0.2187, 0.2196],\n",
      "         [0.2751, 0.2468, 0.2409, 0.2372],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         [0.3044, 0.2573, 0.2187, 0.2196],\n",
      "         [0.2325, 0.2735, 0.2501, 0.2439],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2625, 0.2428, 0.2507, 0.2441],\n",
      "         [0.2625, 0.2428, 0.2507, 0.2441],\n",
      "         [0.2125, 0.2940, 0.2589, 0.2346],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2822, 0.2170, 0.2903, 0.2105],\n",
      "         [0.2865, 0.2408, 0.2588, 0.2139],\n",
      "         [0.2751, 0.2468, 0.2409, 0.2372],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2822, 0.2170, 0.2903, 0.2105],\n",
      "         [0.2600, 0.2804, 0.2089, 0.2507],\n",
      "         [0.2447, 0.2591, 0.2659, 0.2303],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2822, 0.2169, 0.2903, 0.2105],\n",
      "         [0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         [0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2447, 0.2591, 0.2659, 0.2304],\n",
      "         [0.2625, 0.2428, 0.2506, 0.2441],\n",
      "         [0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.3009, 0.2073, 0.2508, 0.2410],\n",
      "         [0.2751, 0.2469, 0.2409, 0.2372],\n",
      "         [0.2953, 0.2591, 0.2565, 0.1890],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2822, 0.2169, 0.2903, 0.2105],\n",
      "         [0.3044, 0.2573, 0.2187, 0.2196],\n",
      "         [0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         [0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         [0.2548, 0.2832, 0.2096, 0.2524],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2548, 0.2947, 0.2333, 0.2172],\n",
      "         [0.3044, 0.2573, 0.2187, 0.2196],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2649, 0.2581, 0.2444, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2823, 0.2169, 0.2903, 0.2106],\n",
      "         [0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         [0.2359, 0.2777, 0.2518, 0.2346],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         [0.2548, 0.2832, 0.2096, 0.2524],\n",
      "         [0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         [0.2625, 0.2428, 0.2506, 0.2441],\n",
      "         [0.2447, 0.2591, 0.2659, 0.2304],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         [0.3044, 0.2573, 0.2187, 0.2196],\n",
      "         [0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2903, 0.2106],\n",
      "         [0.2447, 0.2591, 0.2659, 0.2304],\n",
      "         [0.2823, 0.2169, 0.2903, 0.2106],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2468, 0.2131, 0.2723, 0.2678],\n",
      "         [0.2359, 0.2777, 0.2518, 0.2346],\n",
      "         [0.2359, 0.2777, 0.2518, 0.2346],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2468, 0.2131, 0.2722, 0.2679],\n",
      "         [0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2468, 0.2131, 0.2722, 0.2679],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2447, 0.2590, 0.2658, 0.2304],\n",
      "         [0.2359, 0.2777, 0.2518, 0.2346],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2468, 0.2131, 0.2722, 0.2679],\n",
      "         [0.2548, 0.2832, 0.2096, 0.2524],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         [0.2625, 0.2428, 0.2506, 0.2441],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.3044, 0.2573, 0.2186, 0.2196],\n",
      "         [0.2325, 0.2735, 0.2500, 0.2440],\n",
      "         [0.2548, 0.2947, 0.2332, 0.2173],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2768, 0.2652, 0.2159, 0.2421],\n",
      "         [0.2447, 0.2590, 0.2658, 0.2304],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2326]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2768, 0.2652, 0.2158, 0.2421],\n",
      "         [0.2625, 0.2428, 0.2505, 0.2441],\n",
      "         [0.3044, 0.2574, 0.2186, 0.2196],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325]],\n",
      "\n",
      "        [[0.2468, 0.2131, 0.2722, 0.2679],\n",
      "         [0.3044, 0.2574, 0.2186, 0.2196],\n",
      "         [0.2913, 0.2386, 0.2135, 0.2567],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325]],\n",
      "\n",
      "        [[0.3044, 0.2574, 0.2186, 0.2196],\n",
      "         [0.2359, 0.2777, 0.2518, 0.2346],\n",
      "         [0.2913, 0.2386, 0.2135, 0.2567],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2600, 0.2805, 0.2088, 0.2508],\n",
      "         [0.2126, 0.2940, 0.2587, 0.2347],\n",
      "         [0.2625, 0.2428, 0.2505, 0.2441],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2548, 0.2947, 0.2332, 0.2173],\n",
      "         [0.2768, 0.2652, 0.2158, 0.2421],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2501, 0.2260, 0.2720, 0.2519],\n",
      "         [0.2126, 0.2940, 0.2587, 0.2347],\n",
      "         ...,\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325],\n",
      "         [0.2650, 0.2581, 0.2444, 0.2325]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2468, 0.2131, 0.2722, 0.2679],\n",
      "         [0.2769, 0.2652, 0.2158, 0.2421],\n",
      "         [0.2751, 0.2469, 0.2407, 0.2373],\n",
      "         ...,\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2468, 0.2131, 0.2722, 0.2679],\n",
      "         [0.3045, 0.2574, 0.2186, 0.2196],\n",
      "         [0.2913, 0.2386, 0.2135, 0.2566],\n",
      "         ...,\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2360, 0.2776, 0.2517, 0.2347],\n",
      "         [0.2360, 0.2776, 0.2517, 0.2347],\n",
      "         ...,\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2769, 0.2652, 0.2158, 0.2421],\n",
      "         [0.2865, 0.2408, 0.2588, 0.2139],\n",
      "         [0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         ...,\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2851, 0.3066, 0.2114, 0.1969],\n",
      "         [0.2600, 0.2805, 0.2087, 0.2508],\n",
      "         [0.2600, 0.2805, 0.2087, 0.2508],\n",
      "         ...,\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2823, 0.2169, 0.2902, 0.2106],\n",
      "         [0.2769, 0.2652, 0.2158, 0.2421],\n",
      "         [0.2360, 0.2776, 0.2517, 0.2347],\n",
      "         ...,\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2581, 0.2443, 0.2325]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2824, 0.2168, 0.2901, 0.2107],\n",
      "         [0.2769, 0.2652, 0.2158, 0.2421],\n",
      "         [0.2468, 0.2131, 0.2721, 0.2679],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2824, 0.2168, 0.2901, 0.2107],\n",
      "         [0.2548, 0.2831, 0.2096, 0.2524],\n",
      "         [0.2468, 0.2131, 0.2721, 0.2679],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2824, 0.2168, 0.2901, 0.2107],\n",
      "         [0.2448, 0.2590, 0.2658, 0.2305],\n",
      "         [0.3009, 0.2073, 0.2507, 0.2411],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2126, 0.2941, 0.2587, 0.2347],\n",
      "         [0.2769, 0.2652, 0.2158, 0.2421],\n",
      "         [0.2600, 0.2805, 0.2087, 0.2508],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2824, 0.2168, 0.2901, 0.2107],\n",
      "         [0.2548, 0.2948, 0.2331, 0.2173],\n",
      "         [0.3009, 0.2073, 0.2507, 0.2411],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2824, 0.2168, 0.2901, 0.2107],\n",
      "         [0.2448, 0.2590, 0.2658, 0.2305],\n",
      "         [0.3009, 0.2073, 0.2507, 0.2411],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n",
      "run epoch...\n",
      "x shape for batch\n",
      "torch.Size([64, 1632, 23]) torch.float64\n",
      "print y shape for batch\n",
      "torch.Size([64, 1632]) torch.float32\n",
      "MultiLabelMultiClass(\n",
      "  (linear): Linear(in_features=23, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "multilabel output inside forward\n",
      "tensor([[[0.2769, 0.2652, 0.2157, 0.2421],\n",
      "         [0.3009, 0.2073, 0.2507, 0.2411],\n",
      "         [0.2600, 0.2805, 0.2087, 0.2508],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2824, 0.2168, 0.2901, 0.2107],\n",
      "         [0.2769, 0.2652, 0.2157, 0.2421],\n",
      "         [0.2360, 0.2776, 0.2517, 0.2347],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2769, 0.2652, 0.2157, 0.2421],\n",
      "         [0.2469, 0.2132, 0.2721, 0.2679],\n",
      "         [0.2448, 0.2589, 0.2657, 0.2305],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2824, 0.2168, 0.2901, 0.2107],\n",
      "         [0.2549, 0.2948, 0.2331, 0.2173],\n",
      "         [0.2769, 0.2652, 0.2157, 0.2421],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2549, 0.2948, 0.2331, 0.2173],\n",
      "         [0.2448, 0.2589, 0.2657, 0.2305],\n",
      "         [0.2865, 0.2408, 0.2588, 0.2139],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]],\n",
      "\n",
      "        [[0.2751, 0.2469, 0.2406, 0.2373],\n",
      "         [0.2501, 0.2260, 0.2719, 0.2519],\n",
      "         [0.2865, 0.2408, 0.2588, 0.2139],\n",
      "         ...,\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325],\n",
      "         [0.2651, 0.2580, 0.2443, 0.2325]]], grad_fn=<SoftmaxBackward>)\n",
      "run epoch\n",
      "run epoch output size torch.Size([64, 1632, 4])\n",
      "y shape and (np.arange(y.max()+1) == y[...,None])\n",
      "torch.Size([64, 1632]) torch.Size([64, 1632, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=0'>1</a>\u001b[0m Run_Pytorch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=1'>2</a>\u001b[0m     dataset_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/structure/secondary_structure/casp12.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=2'>3</a>\u001b[0m     encoder_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=3'>4</a>\u001b[0m     reset_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=4'>5</a>\u001b[0m     resample_param \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=5'>6</a>\u001b[0m     embed_batch_size \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=6'>7</a>\u001b[0m     flatten_emb\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=7'>8</a>\u001b[0m     embed_folder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=8'>9</a>\u001b[0m     seq_start_idx\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=9'>10</a>\u001b[0m     seq_end_idx \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=10'>11</a>\u001b[0m     loader_batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=11'>12</a>\u001b[0m     worker_seed \u001b[39m=\u001b[39;49m RAND_SEED,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=12'>13</a>\u001b[0m     if_encode_all \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=13'>14</a>\u001b[0m     learning_rate \u001b[39m=\u001b[39;49m \u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=14'>15</a>\u001b[0m     lr_decay \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=15'>16</a>\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=16'>17</a>\u001b[0m     early_stop \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=17'>18</a>\u001b[0m     tolerance \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=18'>19</a>\u001b[0m     min_epoch \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=19'>20</a>\u001b[0m     device \u001b[39m=\u001b[39;49m DEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=20'>21</a>\u001b[0m     all_plot_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest/learning_curves\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=21'>22</a>\u001b[0m     all_result_folder \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest/train_val_test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=22'>23</a>\u001b[0m     \u001b[39m# **encoder_params,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=23'>24</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb Cell 42\u001b[0m in \u001b[0;36mRun_Pytorch.__init__\u001b[0;34m(self, dataset_path, encoder_name, reset_param, resample_param, embed_batch_size, flatten_emb, embed_folder, seq_start_idx, seq_end_idx, loader_batch_size, worker_seed, if_encode_all, if_multiprocess, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, device, all_plot_folder, all_result_folder, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=165'>166</a>\u001b[0m \u001b[39mfor\u001b[39;00m embed_layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_emb_layer):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=166'>167</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning pytorch model for layer \u001b[39m\u001b[39m{\u001b[39;00membed_layer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=167'>168</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_pytorch_layer(embed_layer)\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb Cell 42\u001b[0m in \u001b[0;36mRun_Pytorch.run_pytorch_layer\u001b[0;34m(self, embed_layer)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=192'>193</a>\u001b[0m model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=194'>195</a>\u001b[0m criterion\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=196'>197</a>\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m train(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=197'>198</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=198'>199</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=199'>200</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=200'>201</a>\u001b[0m     val_loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_val_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=201'>202</a>\u001b[0m     encoder_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=202'>203</a>\u001b[0m     embed_layer\u001b[39m=\u001b[39;49membed_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=203'>204</a>\u001b[0m     reset_param\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=204'>205</a>\u001b[0m     resample_param\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resample_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=205'>206</a>\u001b[0m     embed_batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embed_batch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=206'>207</a>\u001b[0m     flatten_emb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flatten_emb,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=207'>208</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=208'>209</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learning_rate,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=209'>210</a>\u001b[0m     lr_decay\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lr_decay,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=210'>211</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_epochs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=211'>212</a>\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_early_stop,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=212'>213</a>\u001b[0m     tolerance\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tolerance,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=213'>214</a>\u001b[0m     min_epoch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_min_epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=214'>215</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=215'>216</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=217'>218</a>\u001b[0m \u001b[39m# record the losses\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=218'>219</a>\u001b[0m result_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=219'>220</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlosses\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mtrain_losses\u001b[39m\u001b[39m\"\u001b[39m: train_losses, \u001b[39m\"\u001b[39m\u001b[39mval_losses\u001b[39m\u001b[39m\"\u001b[39m: val_losses}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=220'>221</a>\u001b[0m }\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb Cell 42\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, train_loader, val_loader, encoder_name, embed_layer, reset_param, resample_param, embed_batch_size, flatten_emb, device, learning_rate, lr_decay, epochs, early_stop, tolerance, min_epoch, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=230'>231</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minside train before run_epoch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=232'>233</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=234'>235</a>\u001b[0m     train_losses[epoch] \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=235'>236</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=236'>237</a>\u001b[0m         loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=237'>238</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49mencoder_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=238'>239</a>\u001b[0m         embed_layer\u001b[39m=\u001b[39;49membed_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=239'>240</a>\u001b[0m         reset_param\u001b[39m=\u001b[39;49mreset_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=240'>241</a>\u001b[0m         resample_param\u001b[39m=\u001b[39;49mresample_param,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=241'>242</a>\u001b[0m         embed_batch_size\u001b[39m=\u001b[39;49membed_batch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=242'>243</a>\u001b[0m         flatten_emb\u001b[39m=\u001b[39;49mflatten_emb,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=243'>244</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=244'>245</a>\u001b[0m         criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=245'>246</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=246'>247</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=247'>248</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=249'>250</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=250'>251</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=251'>252</a>\u001b[0m         loader\u001b[39m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=261'>262</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=262'>263</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=263'>264</a>\u001b[0m     val_losses[epoch] \u001b[39m=\u001b[39m val_loss\n",
      "\u001b[1;32m/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb Cell 42\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, encoder_name, embed_layer, reset_param, resample_param, embed_batch_size, flatten_emb, device, criterion, optimizer, **encoder_params)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=118'>119</a>\u001b[0m cum_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=120'>121</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(is_train):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=121'>122</a>\u001b[0m     \u001b[39m# if not if_encode_all:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=122'>123</a>\u001b[0m     \u001b[39m# for each batch: y, sequence, mut_name, mut_numb, [layer0, ...]\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=124'>125</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=125'>126</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrun epoch...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgcrazgdl2369/home/t-fli/repo/protein-transfer/sandbox/ss_dataset.ipynb#ch0000027vscode-remote?line=127'>128</a>\u001b[0m         x, y \u001b[39m=\u001b[39m get_x_y(model, device, batch, embed_layer)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 517\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    520\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    521\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    556\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    559\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:47\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 47\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:83\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m     85\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m     85\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:63\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m batch])\n\u001b[1;32m     64\u001b[0m \u001b[39melif\u001b[39;00m elem\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():  \u001b[39m# scalars\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mas_tensor(batch)\n",
      "File \u001b[0;32m/anaconda/envs/protran/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:55\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel)\n\u001b[1;32m     54\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m     56\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m     57\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     59\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Run_Pytorch(\n",
    "    dataset_path=\"data/structure/secondary_structure/casp12.csv\",\n",
    "    encoder_name=\"\",\n",
    "    reset_param = False,\n",
    "    resample_param = False,\n",
    "    embed_batch_size = 0,\n",
    "    flatten_emb= False,\n",
    "    embed_folder=None,\n",
    "    seq_start_idx= False,\n",
    "    seq_end_idx = False,\n",
    "    loader_batch_size = 64,\n",
    "    worker_seed = RAND_SEED,\n",
    "    if_encode_all = True,\n",
    "    learning_rate = 1e-4,\n",
    "    lr_decay = 0.1,\n",
    "    epochs = 50,\n",
    "    early_stop = True,\n",
    "    tolerance = 10,\n",
    "    min_epoch = 5,\n",
    "    device = DEVICE,\n",
    "    all_plot_folder = \"test/learning_curves\",\n",
    "    all_result_folder = \"test/train_val_test\",\n",
    "    # **encoder_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
