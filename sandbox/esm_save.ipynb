{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle(IsDescription):\n",
    "    name      = StringCol(16)   # 16-character String\n",
    "    idnumber  = Int64Col()      # Signed 64-bit integer\n",
    "    ADCcount  = UInt16Col()     # Unsigned short integer\n",
    "    TDCcount  = UInt8Col()      # unsigned byte\n",
    "    grid_i    = Int32Col()      # 32-bit integer\n",
    "    grid_j    = Int32Col()      # 32-bit integer\n",
    "    pressure  = Float32Col()    # float  (single-precision)\n",
    "    energy    = Float64Col()    # double (double-precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file = open_file(\"tutorial1.h5\", mode=\"w\", title=\"Test file\")\n",
    "group = h5file.create_group(\"/\", 'detector', 'Detector information')\n",
    "table = h5file.create_table(group, 'readout', Particle, \"Readout example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = h5file.root.detector.readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/detector/readout (Table(0,)) 'Readout example'\n",
       "  description := {\n",
       "  \"ADCcount\": UInt16Col(shape=(), dflt=0, pos=0),\n",
       "  \"TDCcount\": UInt8Col(shape=(), dflt=0, pos=1),\n",
       "  \"energy\": Float64Col(shape=(), dflt=0.0, pos=2),\n",
       "  \"grid_i\": Int32Col(shape=(), dflt=0, pos=3),\n",
       "  \"grid_j\": Int32Col(shape=(), dflt=0, pos=4),\n",
       "  \"idnumber\": Int64Col(shape=(), dflt=0, pos=5),\n",
       "  \"name\": StringCol(itemsize=16, shape=(), dflt=b'', pos=6),\n",
       "  \"pressure\": Float32Col(shape=(), dflt=0.0, pos=7)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1394,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import tables\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from scr.utils import get_folder_file_names, checkNgen_folder\n",
    "from scr.encoding.encoding_classes import get_emb_info, OnehotEncoder\n",
    "from scr.preprocess.data_process import ProtranDataset\n",
    "\n",
    "\n",
    "class GenerateEmbeddings:\n",
    "    \"\"\"A class for generating and saving embeddings\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        embed_folder: str = \"embeddings\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_path: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - embed_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.encoder_name = encoder_name\n",
    "        self.reset_param = reset_param\n",
    "        self.resample_param = resample_param\n",
    "        self.flatten_emb = flatten_emb\n",
    "\n",
    "        self.embed_folder = embed_folder\n",
    "\n",
    "        if self.reset_param and \"-rand\" not in self.embed_folder:\n",
    "            self.embed_folder = f\"{self.embed_folder}-rand\"\n",
    "\n",
    "        if self.resample_param and \"-stat\" not in self.embed_folder:\n",
    "            self.embed_folder = f\"{self.embed_folder}-stat\"\n",
    "\n",
    "        subset_list = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        self.encoder_name, encoder_class, total_emb_layer = get_emb_info(\n",
    "            self.encoder_name\n",
    "        )\n",
    "\n",
    "        print(encoder_class)\n",
    "\n",
    "        assert encoder_class != OnehotEncoder, \"Generate onehot on the fly instead\"\n",
    "\n",
    "        # get the encoder\n",
    "        self._encoder = encoder_class(\n",
    "            encoder_name=encoder_name,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            **encoder_params,\n",
    "        )\n",
    "\n",
    "        # get the dim of the array to be saved\n",
    "        earray_dim = (0, self._encoder.embed_dim)\n",
    "\n",
    "        dataset_folder, _ = get_folder_file_names(\n",
    "            parent_folder=self.embed_folder,\n",
    "            dataset_path=self.dataset_path,\n",
    "            encoder_name=self.encoder_name,\n",
    "            embed_layer=0,\n",
    "            flatten_emb=self.flatten_emb,\n",
    "        )\n",
    "\n",
    "        # Close all the open files\n",
    "        tables.file._open_files.close_all()\n",
    "\n",
    "        for subset in subset_list:\n",
    "            init_array_list = [None] * total_emb_layer\n",
    "\n",
    "            file_path = os.path.join(\n",
    "                checkNgen_folder(os.path.join(dataset_folder, subset)), \"embedding.h5\"\n",
    "            )\n",
    "\n",
    "            # check all the embedding file h5 files\n",
    "            # to remove old ones before generating new ones\n",
    "            if os.path.isfile(file_path):\n",
    "                print(\"Overwritting {0}\".format(file_path))\n",
    "                os.remove(file_path)\n",
    "\n",
    "            # init file open\n",
    "            f = tables.open_file(file_path, mode=\"a\")\n",
    "            for emb_layer in range(total_emb_layer):\n",
    "                init_array_list[emb_layer] = f.create_earray(\n",
    "                    f.root, \"layer\" + str(emb_layer), tables.Float32Atom(), earray_dim\n",
    "                )\n",
    "\n",
    "            # get the dataset to be encoded\n",
    "            ds = ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_path=None,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                if_encode_all=False,\n",
    "                **encoder_params,\n",
    "            )\n",
    "\n",
    "            # init an empty dict with empty list to append emb\n",
    "            # encoded_dict = defaultdict(list)\n",
    "\n",
    "            # use the encoder generator for batch emb\n",
    "            # assume no labels included\n",
    "            for encoded_batch_dict in self._encoder.encode(\n",
    "                mut_seqs=ds.sequence,\n",
    "                batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "            ):\n",
    "\n",
    "                for emb_layer, emb in encoded_batch_dict.items():\n",
    "                    getattr(f.root, \"layer\" + str(emb_layer)).append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scr.encoding.encoding_classes.ESMEncoder'>\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "Closing remaining open files:embeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/test/embedding.h5...doneembeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/train/embedding.h5...doneembeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/val/embedding.h5...done\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making embeddings/proeng/thermo ...\n",
      "Making embeddings/proeng/thermo/mixed_split ...\n",
      "Making embeddings/proeng/thermo/mixed_split/esm1_t6_43M_UR50S ...\n",
      "Making embeddings/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean ...\n",
      "Making embeddings/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean/train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/175 [00:37<1:49:15, 37.67s/it]"
     ]
    }
   ],
   "source": [
    "GenerateEmbeddings( dataset_path=\"data/proeng/thermo/mixed_split.csv\",\n",
    "                    encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "                    reset_param = False,\n",
    "                    resample_param = False,\n",
    "                    embed_batch_size = 128,\n",
    "                    flatten_emb = \"mean\",\n",
    "                    seq_start_idx = False,\n",
    "                    seq_end_idx= False,\n",
    "                    embed_folder = \"embeddings\",\n",
    "                    # **encoder_params,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
