{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEVLEEPAPGPGGADAAERRGLRRLLLSGFQEELRALLVLAGPAFL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFED...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAKRTFSNLETFLIFLLVMMSAITVALLSLLFITSGTIENHKDLGG...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MGNCQAGHNLHLCLAHHPPLVCATLILLLLGLSGLGLGSFLLTHRT...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDPSKQGTLNRVENSVYRTAFKLRSVQTLCQLDLMDSFLIQQVLWR...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence         target    set  \\\n",
       "0  MEVLEEPAPGPGGADAAERRGLRRLLLSGFQEELRALLVLAGPAFL...  Cell membrane  train   \n",
       "1  MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFED...  Cell membrane  train   \n",
       "2  MAKRTFSNLETFLIFLLVMMSAITVALLSLLFITSGTIENHKDLGG...  Cell membrane  train   \n",
       "3  MGNCQAGHNLHLCLAHHPPLVCATLILLLLGLSGLGLGSFLLTHRT...  Cell membrane  train   \n",
       "4  MDPSKQGTLNRVENSVYRTAFKLRSVQTLCQLDLMDSFLIQQVLWR...  Cell membrane  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/annotation/scl/balanced.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([nan, True], dtype=object), array(['train', 'test'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.validation.unique(), df.set.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cell membrane', 'Cytoplasm', 'Endoplasmic reticulum',\n",
       "       'Golgi apparatus', 'Lysosome/Vacuole', 'Mitochondrion', 'Nucleus',\n",
       "       'Peroxisome', 'Plastid', 'Extracellular'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.preprocess.data_process import ProtranDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/27 [00:00<00:01, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:02<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting classes into int...\n"
     ]
    }
   ],
   "source": [
    "val_ds = ProtranDataset(\n",
    "                dataset_path=\"data/annotation/scl/balanced.csv\",\n",
    "                subset=\"val\",\n",
    "                encoder_name=\"\",\n",
    "                reset_param=False,\n",
    "                resample_param=False,\n",
    "                embed_batch_size=64,\n",
    "                flatten_emb=\"flatten\",\n",
    "                embed_folder=None,\n",
    "                seq_start_idx=0,\n",
    "                seq_end_idx=False,\n",
    "                # **encoder_params,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 161.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating onehot upto 0 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gb1_val = ProtranDataset(\n",
    "                dataset_path=\"data/proeng/gb1/sampled.csv\",\n",
    "                subset=\"val\",\n",
    "                encoder_name=\"\",\n",
    "                reset_param=False,\n",
    "                resample_param=False,\n",
    "                embed_batch_size=64,\n",
    "                flatten_emb=\"flatten\",\n",
    "                embed_folder=None,\n",
    "                seq_start_idx=0,\n",
    "                seq_end_idx=False,\n",
    "                # **encoder_params,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int64'), dtype('float64'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.y.dtype, gb1_val.y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb1_val.y.dtype.kind in 'iufc', val_ds.y.dtype.kind in 'iufc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1678, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e+03, 1.e+02, 1.e+01, 1.e+00, 1.e-01])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/ SKLEARN_ALPHAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, C=0.1, multi_class=\"multinomial\", max_iter=1000).fit(val_ds.layer0, val_ds.y.flatten())\n",
    "pred_y = clf.predict(val_ds.layer0)\n",
    "pred_prob = clf.predict_proba(val_ds.layer0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - pred_y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1678, 10), (1678, 288200), (1678,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob.shape, val_ds.layer0.shape, val_ds.y.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.09828320234083007, 1.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(val_ds.y.flatten(), pred_y), log_loss(val_ds.y.flatten(), pred_prob), roc_auc_score(val_ds.y.flatten(), pred_prob, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(maxlen//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test6789'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 8\n",
    "teststr = \"test0123456789\"\n",
    "teststr[: int(maxlen//2)] + teststr[-int(maxlen//2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for run sklearn models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.utils import get_folder_file_names, pickle_save, ndcg_scale\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO, MAX_SEQ_LEN\n",
    "from scr.params.sys import RAND_SEED, SKLEARN_ALPHAS\n",
    "from scr.encoding.encoding_classes import ESMEncoder, CARPEncoder, OnehotEncoder\n",
    "from scr.preprocess.data_process import ProtranDataset\n",
    "\n",
    "# seed\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "\n",
    "\n",
    "class RunSK:\n",
    "    \"\"\"A class for running sklearn models\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        alphas: np.ndarray | int = SKLEARN_ALPHAS,\n",
    "        sklearn_state: int = RAND_SEED,\n",
    "        sklearn_params: dict | None = None,\n",
    "        all_result_folder: str = \"results/sklearn\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - alphas: np.ndarray, arrays of alphas to be tested\n",
    "        - sklearn_state: int = RAND_SEED, seed the ridge or logistic regression\n",
    "        - sklearn_params: dict | None = None, other ridge or logistic regression args\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.encoder_name = encoder_name\n",
    "        self.reset_param = reset_param\n",
    "        self.resample_param = resample_param\n",
    "        self.flatten_emb = flatten_emb\n",
    "\n",
    "        if not isinstance(alphas, np.ndarray):\n",
    "            alphas = np.array([alphas])\n",
    "        self.alphas = alphas\n",
    "\n",
    "        self.sklearn_state = sklearn_state\n",
    "        self.sklearn_params = sklearn_params\n",
    "        self.all_result_folder = all_result_folder\n",
    "\n",
    "        if self.reset_param and \"-rand\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-rand\"\n",
    "\n",
    "        if self.resample_param and \"-stat\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-stat\"\n",
    "\n",
    "        # loader has ALL embedding layers\n",
    "        self.train_ds, self.val_ds, self.test_ds = (\n",
    "            ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_folder=embed_folder,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                **encoder_params,\n",
    "            )\n",
    "            for subset in [\"train\", \"val\", \"test\"]\n",
    "        )\n",
    "\n",
    "        # pick ridge regression if y numerical\n",
    "        if self.val_ds.y.dtype.kind in \"iufc\":\n",
    "            self.sklearn_model = Ridge\n",
    "\n",
    "        # pick logistic regression if y is categorical\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            self.train_ds.y, self.val_ds.y, self.test_ds.y = [\n",
    "                le.fit_transform(y.flatten())\n",
    "                for y in [self.train_ds.y, self.val_ds.y, self.test_ds.y]\n",
    "            ]\n",
    "            self.sklearn_model = LogisticRegression\n",
    "            # convert alpha to C\n",
    "            self.alphas = 1 / self.alphas\n",
    "            # add other params\n",
    "            if self.sklearn_params is None:\n",
    "                self.sklearn_params[\"multi_class\"] = \"multinomial\"\n",
    "                self.sklearn_params[\"max_iter\"] = 1000\n",
    "\n",
    "        all_sklearn_results = {}\n",
    "\n",
    "        # TODO for easier total_emb_layer\n",
    "        if self.encoder_name in TRANSFORMER_INFO.keys():\n",
    "            total_emb_layer = TRANSFORMER_INFO[encoder_name][1] + 1\n",
    "        elif self.encoder_name in CARP_INFO.keys():\n",
    "            total_emb_layer = CARP_INFO[encoder_name][1]\n",
    "        else:\n",
    "            # for onehot\n",
    "            self.encoder_name = \"onehot\"\n",
    "            total_emb_layer = 1\n",
    "\n",
    "        for layer in range(total_emb_layer):\n",
    "            all_sklearn_results[layer] = self.run_sklearn_layer(embed_layer=layer,)\n",
    "\n",
    "        self._all_sklearn_results = all_sklearn_results\n",
    "\n",
    "    def sk_test(\n",
    "        self, model: sklearn.linear_model, ds: ProtranDataset, embed_layer: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for testing sklearn models for a specific layer of embeddings\n",
    "\n",
    "        Args:\n",
    "        - model: sklearn.linear_model, trained model\n",
    "        - ds: ProtranDataset, train, val, or test dataset\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - np.concatenate(pred): np.ndarray, 1D predicted fitness values\n",
    "        - np.concatenate(true): np.ndarry, 1D true fitness values\n",
    "        - \n",
    "        \"\"\"\n",
    "\n",
    "        if self.sklearn_model == Ridge:\n",
    "            pred_prob = None\n",
    "        else:\n",
    "            pred_prob = model.predict_proba(\n",
    "                getattr(ds, \"layer\" + str(embed_layer)).cpu().numpy()\n",
    "            ).squeeze()\n",
    "\n",
    "        return (\n",
    "            model.predict(\n",
    "                getattr(ds, \"layer\" + str(embed_layer)).cpu().numpy()\n",
    "            ).squeeze(),\n",
    "            ds.y.squeeze(),\n",
    "            pred_prob,\n",
    "        )\n",
    "\n",
    "    def pick_model(\n",
    "        self, embed_layer: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for picking the best model for given alaphs, meaning\n",
    "        lower train_mse and higher test_ndcg\n",
    "        NOTE: alphas tuning is NOT currently optimal\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - sklearn.linear_model, the model with the best alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # init values for comparison\n",
    "        if self.sklearn_model == Ridge:\n",
    "            best_mse = np.Inf\n",
    "            best_ndcg = -1\n",
    "            best_rho = -1\n",
    "        else:\n",
    "            best_loss = np.Inf\n",
    "            best_acc = 0\n",
    "            best_auc = 0\n",
    "\n",
    "        best_model = None\n",
    "\n",
    "        # loop through all alphas\n",
    "        for alpha in self.alphas:\n",
    "\n",
    "            # init model for each alpha\n",
    "            if self.sklearn_params is None:\n",
    "                self.sklearn_params = {}\n",
    "            model = self.sklearn_model(\n",
    "                alpha=alpha, random_state=self.sklearn_state, **self.sklearn_params\n",
    "            )\n",
    "\n",
    "            # fit the model for a given layer of embedding\n",
    "            fitness_scaler = StandardScaler()\n",
    "            model.fit(\n",
    "                getattr(self.train_ds, \"layer\" + str(embed_layer)).cpu().numpy(),\n",
    "                fitness_scaler.fit_transform(self.train_ds.y),\n",
    "            )\n",
    "\n",
    "            # eval the model with train and test\n",
    "            train_pred, train_true, train_prob = self.sk_test(\n",
    "                model, self.train_ds, embed_layer=embed_layer\n",
    "            )\n",
    "            val_pred, val_true, val_prob = self.sk_test(\n",
    "                model, self.val_ds, embed_layer=embed_layer\n",
    "            )\n",
    "\n",
    "            if self.sklearn_model == Ridge:\n",
    "                # calc the metrics\n",
    "                train_mse = mean_squared_error(train_true, train_pred)\n",
    "                val_ndcg = ndcg_scale(val_true, val_pred)\n",
    "                val_rho = spearmanr(val_true, val_pred)[0]\n",
    "\n",
    "                # update the model if it has lower train_mse and higher val_ndcg\n",
    "                if train_mse < best_mse and val_ndcg > best_ndcg:\n",
    "                    best_model = model\n",
    "                    best_mse = train_mse\n",
    "                    best_ndcg = val_ndcg\n",
    "                    best_rho = val_rho\n",
    "\n",
    "            else:\n",
    "                # calc the metrics\n",
    "                train_loss = log_loss(train_true, train_prob)\n",
    "                val_acc = accuracy_score(val_true, val_pred)\n",
    "                val_auc = roc_auc_score(val_true, val_prob, multi_class=\"ovo\")\n",
    "\n",
    "                # update the model if it has lower log_loss and higher val_auc\n",
    "                if train_loss < best_loss and val_auc > best_auc:\n",
    "                    best_loss = train_loss\n",
    "                    best_acc = val_acc\n",
    "                    best_auc = val_auc\n",
    "\n",
    "        print(f\"best model is {best_model}\")\n",
    "        return best_model\n",
    "\n",
    "    def run_sklearn_layer(\n",
    "        self, embed_layer: int,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running ridge or logistics regression for a given layer of embedding\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"train\": {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"\"\"\n",
    "\n",
    "        # train and get the best alpha\n",
    "        best_model = self.pick_model(embed_layer=embed_layer,)\n",
    "\n",
    "        # init dict for resulted outputs\n",
    "        result_dict = {}\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, ds in zip(\n",
    "            [\"train\", \"val\", \"test\"], [self.train_ds, self.val_ds, self.test_ds],\n",
    "        ):\n",
    "            pred, true, prob = self.sk_test(best_model, ds, embed_layer=embed_layer)\n",
    "\n",
    "            if self.sklearn_model == Ridge:\n",
    "                result_dict[subset] = {\n",
    "                    \"mse\": mean_squared_error(true, pred),\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"ndcg\": ndcg_scale(true, pred),\n",
    "                    \"rho\": spearmanr(true, pred),\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                result_dict[subset] = {\n",
    "                    \"log\": log_loss(true, prob),\n",
    "                    \"pred\": pred,\n",
    "                    \"prob\": prob,\n",
    "                    \"true\": true,\n",
    "                    \"acc\": accuracy_score(true, pred),\n",
    "                    \"rocauc\": roc_auc_score(true, prob, multi_class=\"ovo\"),\n",
    "                }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=self.all_result_folder,\n",
    "            dataset_path=self.dataset_path,\n",
    "            encoder_name=self.encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self.flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    @property\n",
    "    def all_sklearn_results(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"layer#\": {\n",
    "                        \"train\": {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"val\":   {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"test\":  {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        }\n",
    "        \"\"\"\n",
    "        return self._all_sklearn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      " 50%|████▉     | 74/149 [23:26<23:58, 19.18s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "scl = RunSK(\n",
    "        dataset_path=\"data/annotation/scl/balanced.csv\",\n",
    "        encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "        reset_param=False,\n",
    "        resample_param = False,\n",
    "        embed_batch_size = 64,\n",
    "        flatten_emb = \"flatten\",\n",
    "        embed_folder = None,\n",
    "        seq_start_idx = False,\n",
    "        seq_end_idx = False,\n",
    "        alphas = SKLEARN_ALPHAS,\n",
    "        sklearn_state = RAND_SEED,\n",
    "        sklearn_params = None,\n",
    "        all_result_folder = \"results/sklearn-test\",\n",
    "        # **encoder_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
