{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-processing the dataset\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Sequence, defaultdict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scr.utils import pickle_save, pickle_load, replace_ext\n",
    "from scr.params.sys import RAND_SEED\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO, MAX_SEQ_LEN\n",
    "from scr.preprocess.seq_loader import SeqLoader\n",
    "from scr.encoding.encoding_classes import (\n",
    "    AbstractEncoder,\n",
    "    ESMEncoder,\n",
    "    CARPEncoder,\n",
    "    OnehotEncoder,\n",
    ")\n",
    "\n",
    "\n",
    "def get_mut_name(mut_seq: str, parent_seq: str) -> str:\n",
    "    \"\"\"\n",
    "    A function for returning the mutant name\n",
    "\n",
    "    Args:\n",
    "    - mut_seq: str, the full mutant sequence\n",
    "    - parent_seq: str, the full parent sequence\n",
    "\n",
    "    Returns:\n",
    "    - str, parent, indel, or mutant name in the format of\n",
    "        ParentAAMutLocMutAA:ParentAAMutLocMutAA:..., ie. W39W:D40G:G41C:V54Q\n",
    "    \"\"\"\n",
    "\n",
    "    mut_list = []\n",
    "    if parent_seq == mut_seq:\n",
    "        return \"parent\"\n",
    "    elif len(parent_seq) == len(mut_seq):\n",
    "        for i, (p, m) in enumerate(zip(list(parent_seq), list(mut_seq))):\n",
    "            if p != m:\n",
    "                mut_list.append(f\"{p}{i+1}{m}\")\n",
    "        return \":\".join(mut_list)\n",
    "    else:\n",
    "        return \"indel\"\n",
    "\n",
    "\n",
    "class AddMutInfo:\n",
    "    \"\"\"A class for appending mutation info for mainly protein engineering tasks\"\"\"\n",
    "\n",
    "    def __init__(self, parent_seq_path: str, csv_path: str):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - parent_seq_path: str, path for the parent sequence\n",
    "        - csv_path: str, path for the fitness csv file\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the parent sequence from the fasta file\n",
    "        self._parent_seq = SeqLoader(parent_seq_path=parent_seq_path)\n",
    "\n",
    "        # load the dataframe\n",
    "        self._init_df = pd.read_csv(csv_path)\n",
    "\n",
    "        self._df = self._init_df.copy()\n",
    "        # add a column with the mutant names\n",
    "        self._df[\"mut_name\"] = self._init_df[\"sequence\"].apply(\n",
    "            get_mut_name, parent_seq=self._parent_seq\n",
    "        )\n",
    "        # add a column with the number of mutations\n",
    "        self._df[\"mut_numb\"] = (\n",
    "            self._df[\"mut_name\"].str.split(\":\").map(len, na_action=\"ignore\")\n",
    "        )\n",
    "\n",
    "        # get the pickle file path\n",
    "        self._pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "\n",
    "        pickle_save(what2save=self._df, where2save=self._pkl_path)\n",
    "\n",
    "    @property\n",
    "    def parent_seq(self) -> str:\n",
    "        \"\"\"Return the parent sequence\"\"\"\n",
    "        return self._parent_seq\n",
    "\n",
    "    @property\n",
    "    def pkl_path(self) -> str:\n",
    "        \"\"\"Return the pkl file path for the processed dataframe\"\"\"\n",
    "        return self._pkl_path\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the processed dataframe\"\"\"\n",
    "        return self._df\n",
    "\n",
    "\n",
    "def std_ssdf(\n",
    "    ssdf_path: str = \"data/structure/secondary_structure/tape_ss3.csv\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A function that standardize secondary structure dataset\n",
    "    to set up as columns as sequence, target, set, validation\n",
    "    where set is train or test and add validation as true\n",
    "    \"\"\"\n",
    "    folder_path = os.path.dirname(ssdf_path)\n",
    "\n",
    "    df = pd.read_csv(ssdf_path)\n",
    "\n",
    "    # convert the string into numpy array\n",
    "    # df[\"ss3\"] = df[\"ss3\"].apply(lambda x: np.array(x[1:-1].split(\", \")))\n",
    "\n",
    "    # add validation column\n",
    "    df[\"validation\"] = df[\"split\"].apply(lambda x: True if x == \"valid\" else \"\")\n",
    "    # now replace valid to train\n",
    "    df = df.replace(\"valid\", \"train\")\n",
    "    # rename all columns\n",
    "    df.columns = [\"sequence\", \"target\", \"set\", \"validation\"]\n",
    "\n",
    "    # get all kinds of test sets\n",
    "    ss_tests = set(df[\"set\"].unique()) - set([\"train\"])\n",
    "\n",
    "    for ss_test in ss_tests:\n",
    "        df.loc[~df[\"set\"].isin(set(ss_tests) - set([ss_test]))].replace(\n",
    "            ss_test, \"test\"\n",
    "        ).to_csv(os.path.join(folder_path, ss_test + \".csv\"), index=False)\n",
    "\n",
    "\n",
    "class DatasetInfo:\n",
    "    \"\"\"\n",
    "    A class returns the information of a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, the path for the csv\n",
    "        \"\"\"\n",
    "        self._df = pd.read_csv(dataset_path)\n",
    "\n",
    "    def get_model_type(self) -> str:\n",
    "        # pick linear regression if y numerical\n",
    "        if self._df.target.dtype.kind in \"iufc\":\n",
    "            return \"LinearRegression\"\n",
    "        else:\n",
    "            # ss3\n",
    "            if \"[\" in self._df.target[0]:\n",
    "                return \"MultiLabelMultiClass\"\n",
    "            # annotation\n",
    "            else:\n",
    "                return \"LinearClassifier\"\n",
    "\n",
    "    def get_numb_class(self) -> int:\n",
    "        \"\"\"\n",
    "        A function to get number of class\n",
    "        \"\"\"\n",
    "        # annotation class number\n",
    "        if self.model_type == \"LinearClassifier\":\n",
    "            return self._df.target.nunique()\n",
    "        # ss3 or ss8 secondary structure states plus padding\n",
    "        elif self.model_type == \"MultiLabelMultiClass\":\n",
    "            return len(np.unique(np.array(self._df[\"target\"][0][1:-1].split(\", \")))) + 1\n",
    "\n",
    "    @property\n",
    "    def model_type(self) -> str:\n",
    "        \"\"\"Return the pytorch model type\"\"\"\n",
    "        return self.get_model_type()\n",
    "\n",
    "    @property\n",
    "    def numb_class(self) -> int:\n",
    "        \"\"\"Return number of classes for classification\"\"\"\n",
    "        return self.get_numb_class()\n",
    "\n",
    "\n",
    "class TaskProcess:\n",
    "    \"\"\"A class for handling different downstream tasks\"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_folder: str, a folder path with all the tasks as subfolders where\n",
    "            all the subfolders have datasets as the subsubfolders, ie\n",
    "\n",
    "            {data_folder}/\n",
    "                proeng/\n",
    "                    aav/\n",
    "                        one_vs_many.csv\n",
    "                        two_vs_many.csv\n",
    "                        P03135.fasta\n",
    "                    thermo/\n",
    "                        mixed.csv\n",
    "        \"\"\"\n",
    "\n",
    "        if data_folder[-1] == \"/\":\n",
    "            self._data_folder = data_folder\n",
    "        else:\n",
    "            self._data_folder = data_folder + \"/\"\n",
    "\n",
    "        # sumamarize all files i nthe data folder\n",
    "        self._sum_file_df = self.sum_files()\n",
    "\n",
    "    def sum_files(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Summarize all files in the data folder\n",
    "\n",
    "        Returns:\n",
    "        - A dataframe with \"task\", \"dataset\", \"split\",\n",
    "            \"csv_path\", \"fasta_path\", \"pkl_path\" as columns, ie.\n",
    "            (proeng, gb1, low_vs_high, data/proeng/gb1/low_vs_high.csv,\n",
    "            data/proeng/gb1/5LDE_1.fasta)\n",
    "            note that csv_path is the list of lmdb files for the structure task\n",
    "        \"\"\"\n",
    "        dataset_folders = glob(f\"{self._data_folder}*/*\")\n",
    "        # need a list of tuples in the order of:\n",
    "        # (task, dataset, split, csv_path, fasta_path)\n",
    "        list_for_df = []\n",
    "        for dataset_folder in dataset_folders:\n",
    "            _, task, dataset = dataset_folder.split(\"/\")\n",
    "            if task == \"structure\":\n",
    "                structure_file_list = [\n",
    "                    file_path\n",
    "                    for file_path in glob(f\"{dataset_folder}/*.*\")\n",
    "                    if os.path.basename(os.path.splitext(file_path)[0]).split(\"_\")[-1]\n",
    "                    in [\"train\", \"valid\", \"cb513\"]\n",
    "                ]\n",
    "                list_for_df.append(\n",
    "                    tuple([task, dataset, \"cb513\", structure_file_list, \"\", \"\"])\n",
    "                )\n",
    "            else:\n",
    "                csv_paths = glob(f\"{dataset_folder}/*.csv\")\n",
    "                fasta_paths = glob(f\"{dataset_folder}/*.fasta\")\n",
    "                pkl_paths = glob(f\"{dataset_folder}/*.pkl\")\n",
    "\n",
    "                assert len(csv_paths) >= 1, \"Less than one csv\"\n",
    "                assert len(fasta_paths) <= 1, \"More than one fasta\"\n",
    "\n",
    "                for csv_path in csv_paths:\n",
    "                    # if parent seq fasta exists\n",
    "                    if len(fasta_paths) == 1:\n",
    "                        fasta_path = fasta_paths[0]\n",
    "\n",
    "                        # if no existing pkl file, generate and save\n",
    "                        if len(pkl_paths) == 0:\n",
    "                            print(f\"Adding mutation info to {csv_path}...\")\n",
    "                            pkl_path = AddMutInfo(\n",
    "                                parent_seq_path=fasta_path, csv_path=csv_path\n",
    "                            ).pkl_path\n",
    "                        # pkl file exits\n",
    "                        else:\n",
    "                            pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "                    # no parent fasta no pkl file\n",
    "                    else:\n",
    "                        fasta_path = \"\"\n",
    "                        pkl_path = \"\"\n",
    "\n",
    "                    list_for_df.append(\n",
    "                        tuple(\n",
    "                            [\n",
    "                                task,\n",
    "                                dataset,\n",
    "                                os.path.basename(os.path.splitext(csv_path)[0]),\n",
    "                                csv_path,\n",
    "                                fasta_path,\n",
    "                                pkl_path,\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            list_for_df,\n",
    "            columns=[\"task\", \"dataset\", \"split\", \"csv_path\", \"fasta_path\", \"pkl_path\"],\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def sum_file_df(self) -> pd.DataFrame:\n",
    "        \"\"\"A summary table for all files in the data folder\"\"\"\n",
    "        return self._sum_file_df\n",
    "\n",
    "\n",
    "class ProtranDataset(Dataset):\n",
    "\n",
    "    \"\"\"A dataset class for processing protein transfer data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        subset: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str = None,\n",
    "        embed_layer: int | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        if_encode_all: bool = True,\n",
    "        **encoder_params,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format, ie\n",
    "            \"data/proeng/gb1/low_vs_high.csv\"\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - subset: str, train, val, test\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding folder, ie\n",
    "            \"embeddings/proeng/gb1/low_vs_high\"\n",
    "            for which then can add the subset to be, ie\n",
    "            \"embeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/test/embedding.h5\"\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - if_encode_all: bool = True, if encode full dataset all layers on the fly\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        # with additional info mut_name, mut_numb\n",
    "        if os.path.splitext(dataset_path)[-1] in [\".pkl\", \".PKL\", \"\"]:\n",
    "            self._df = pickle_load(dataset_path)\n",
    "            self._add_mut_info = True\n",
    "        # without such info\n",
    "        else:\n",
    "            self._df = pd.read_csv(dataset_path)\n",
    "            self._ds_info = DatasetInfo(dataset_path)\n",
    "            self._model_type = self._ds_info.model_type\n",
    "            self._numb_class = self._ds_info.numb_class\n",
    "\n",
    "            self._add_mut_info = False\n",
    "\n",
    "        assert \"set\" in self._df.columns, f\"set is not a column in {dataset_path}\"\n",
    "        assert (\n",
    "            \"validation\" in self._df.columns\n",
    "        ), f\"validation is not a column in {dataset_path}\"\n",
    "\n",
    "        self._df_train = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] != True)\n",
    "        ]\n",
    "        self._df_val = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] == True)\n",
    "        ]\n",
    "        self._df_test = self._df.loc[(self._df[\"set\"] == \"test\")]\n",
    "\n",
    "        self._df_dict = {\n",
    "            \"train\": self._df_train,\n",
    "            \"val\": self._df_val,\n",
    "            \"test\": self._df_test,\n",
    "        }\n",
    "\n",
    "        assert subset in list(\n",
    "            self._df_dict.keys()\n",
    "        ), \"split can only be 'train', 'val', or 'test'\"\n",
    "        self._subset = subset\n",
    "\n",
    "        self._subdf_len = len(self._df_dict[self._subset])\n",
    "\n",
    "        # not specified seq start will be from 0\n",
    "        if seq_start_idx == False:\n",
    "            self._seq_start_idx = 0\n",
    "        else:\n",
    "            self._seq_start_idx = int(seq_start_idx)\n",
    "        # not specified seq end will be the full sequence length\n",
    "        if seq_end_idx == False:\n",
    "            self._seq_end_idx = -1\n",
    "            self._max_seq_len = self._df.sequence.str.len().max()\n",
    "        else:\n",
    "            self._seq_end_idx = int(seq_end_idx)\n",
    "            self._max_seq_len = self._seq_end_idx - self._seq_start_idx\n",
    "\n",
    "        # get unencoded string of input sequence\n",
    "        # will need to convert data type\n",
    "        self.sequence = self._get_column_value(\"sequence\")\n",
    "\n",
    "        self.if_encode_all = if_encode_all\n",
    "        self._embed_folder = embed_folder\n",
    "\n",
    "        self._encoder_name = encoder_name\n",
    "        self._flatten_emb = flatten_emb\n",
    "\n",
    "        # get the encoder class\n",
    "        if self._encoder_name in TRANSFORMER_INFO.keys():\n",
    "            encoder_class = ESMEncoder\n",
    "        elif self._encoder_name in CARP_INFO.keys():\n",
    "            encoder_class = CARPEncoder\n",
    "        else:\n",
    "            encoder_class = OnehotEncoder\n",
    "            encoder_params[\"max_seq_len\"] = self._max_seq_len\n",
    "\n",
    "        # get the encoder\n",
    "        self._encoder = encoder_class(\n",
    "            encoder_name=self._encoder_name,\n",
    "            reset_param=reset_param,\n",
    "            resample_param=resample_param,\n",
    "            **encoder_params,\n",
    "        )\n",
    "        self._total_emb_layer = self._encoder.total_emb_layer\n",
    "        self._embed_layer = embed_layer\n",
    "\n",
    "        # encode all and load in memory\n",
    "        if self.if_encode_all and self._embed_folder is None:\n",
    "            # encode the sequences without the mut_name\n",
    "            # init an empty dict with empty list to append emb\n",
    "            encoded_dict = defaultdict(list)\n",
    "\n",
    "            # use the encoder generator for batch emb\n",
    "            # assume no labels included\n",
    "            for encoded_batch_dict in self._encoder.encode(\n",
    "                mut_seqs=self.sequence,\n",
    "                batch_size=embed_batch_size,\n",
    "                flatten_emb=self._flatten_emb,\n",
    "            ):\n",
    "\n",
    "                for layer, emb in encoded_batch_dict.items():\n",
    "                    encoded_dict[layer].append(emb)\n",
    "\n",
    "            # assign each layer as its own variable\n",
    "            for layer, emb in encoded_dict.items():\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"layer\" + str(layer),\n",
    "                    np.vstack(emb)\n",
    "                    # torch.tensor(np.vstack(emb), dtype=torch.float32),\n",
    "                )\n",
    "\n",
    "        # load full one layer embedding\n",
    "        if self._embed_folder is not None and self._embed_layer is not None:\n",
    "\n",
    "            emb_table = tables.open_file(\n",
    "                os.path.join(\n",
    "                    self._embed_folder,\n",
    "                    self._encoder_name,\n",
    "                    self._flatten_emb,\n",
    "                    self._subset,\n",
    "                    \"embedding.h5\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            emb_table.flush()\n",
    "\n",
    "            setattr(\n",
    "                self,\n",
    "                \"layer\" + str(self._embed_layer),\n",
    "                getattr(emb_table.root, \"layer\" + str(self._embed_layer))[:],\n",
    "            )\n",
    "\n",
    "            emb_table.close()\n",
    "        # get and format the fitness or secondary structure values\n",
    "        # can be numbers or string\n",
    "        # will need to convert data type\n",
    "        # make 1D tensor 2D\n",
    "        self.y = np.expand_dims(self._get_column_value(\"target\"), 1)\n",
    "\n",
    "        # add mut_name and mut_numb for relevant proeng datasets\n",
    "        if self._add_mut_info:\n",
    "            self.mut_name = self._get_column_value(\"mut_name\")\n",
    "            self.mut_numb = self._get_column_value(\"mut_numb\")\n",
    "        else:\n",
    "            self.mut_name = [\"\"] * self._subdf_len\n",
    "            self.mut_numb = [np.nan] * self._subdf_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the selected subset of the dataframe\"\"\"\n",
    "        return self._subdf_len\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Return the item in the order of\n",
    "        target (y), sequence, mut_name (optional), mut_numb (optional),\n",
    "        embedding per layer upto the max number of layer for the encoder\n",
    "\n",
    "        Args:\n",
    "        - idx: int\n",
    "        \"\"\"\n",
    "        if self.if_encode_all and self._embed_folder is None:\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "                *(\n",
    "                    getattr(self, \"layer\" + str(layer))[idx]\n",
    "                    for layer in range(self._total_emb_layer)\n",
    "                ),\n",
    "            )\n",
    "        elif self._embed_folder is not None:\n",
    "            # load the .h5 file with the embeddings\n",
    "            \"\"\"\n",
    "            gb1_emb = tables.open_file(\"embeddings/proeng/gb1/low_vs_high/esm1_t6_43M_UR50S/mean/test/embedding.h5\")\n",
    "            gb1_emb.flush()\n",
    "            gb1_emb.root.layer0[0:5]\n",
    "            \"\"\"\n",
    "            # return all\n",
    "            if self._embed_layer is None:\n",
    "\n",
    "                emb_table = tables.open_file(\n",
    "                    os.path.join(\n",
    "                        self._embed_folder,\n",
    "                        self._encoder_name,\n",
    "                        self._flatten_emb,\n",
    "                        self._subset,\n",
    "                        \"embedding.h5\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                emb_table.flush()\n",
    "\n",
    "                layer_embs = [\n",
    "                    getattr(emb_table.root, \"layer\" + str(layer))[idx]\n",
    "                    for layer in range(self._total_emb_layer)\n",
    "                ]\n",
    "\n",
    "                emb_table.close()\n",
    "\n",
    "                return (\n",
    "                    self.y[idx],\n",
    "                    self.sequence[idx],\n",
    "                    self.mut_name[idx],\n",
    "                    self.mut_numb[idx],\n",
    "                    layer_embs,\n",
    "                )\n",
    "            # only pick particular embeding layer\n",
    "            else:\n",
    "                \"\"\"\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"layer\" + str(self._embed_layer),\n",
    "                    getattr(emb_table.root, \"layer\" + str(self._embed_layer)),\n",
    "                )\"\"\"\n",
    "\n",
    "                return (\n",
    "                    self.y[idx],\n",
    "                    self.sequence[idx],\n",
    "                    self.mut_name[idx],\n",
    "                    self.mut_numb[idx],\n",
    "                    # getattr(emb_table.root, \"layer\" + str(self._embed_layer))[idx]\n",
    "                    getattr(self, \"layer\" + str(self._embed_layer))[idx],\n",
    "                )\n",
    "        else:\n",
    "            return (\n",
    "                self.y[idx],\n",
    "                self.sequence[idx],\n",
    "                self.mut_name[idx],\n",
    "                self.mut_numb[idx],\n",
    "            )\n",
    "\n",
    "    def _get_column_value(self, column_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Check and return the column values of the selected dataframe subset\n",
    "\n",
    "        Args:\n",
    "        - column_name: str, the name of the dataframe column\n",
    "        \"\"\"\n",
    "        if column_name in self._df.columns:\n",
    "            y = self._df_dict[self._subset][column_name]\n",
    "\n",
    "            if column_name == \"sequence\":\n",
    "                return (\n",
    "                    # self._df_dict[self._subset][\"sequence\"]\n",
    "                    y.astype(str)\n",
    "                    .str[self._seq_start_idx : self._seq_end_idx]\n",
    "                    .apply(\n",
    "                        lambda x: x[: int(MAX_SEQ_LEN // 2)]\n",
    "                        + x[-int(MAX_SEQ_LEN // 2) :]\n",
    "                        if len(x) > MAX_SEQ_LEN\n",
    "                        else x\n",
    "                    )\n",
    "                    .values\n",
    "                )\n",
    "            elif column_name == \"target\" and self._model_type == \"LinearClassifier\":\n",
    "                print(\"Converting classes into int...\")\n",
    "                le = LabelEncoder()\n",
    "                return le.fit_transform(y.values.flatten())\n",
    "            elif column_name == \"target\" and self._model_type == \"MultiLabelMultiClass\":\n",
    "                print(\"Converting ss3/ss8 into np.array...\")\n",
    "                return y.apply(lambda x: np.array(x[1:-1].split(\", \"))).values\n",
    "            else:\n",
    "                return y.values\n",
    "\n",
    "    @property\n",
    "    def df_full(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the full loaded dataset\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def df_train(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_train\n",
    "\n",
    "    @property\n",
    "    def df_val(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for validation only\"\"\"\n",
    "        return self._df_val\n",
    "\n",
    "    @property\n",
    "    def df_test(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_test\n",
    "\n",
    "    @property\n",
    "    def max_seq_len(self) -> int:\n",
    "        \"\"\"Longest sequence length\"\"\"\n",
    "        return self._max_seq_len\n",
    "\n",
    "\n",
    "def split_protrain_loader(\n",
    "    dataset_path: str,\n",
    "    encoder_name: str,\n",
    "    reset_param: bool = False,\n",
    "    resample_param: bool = False,\n",
    "    embed_batch_size: int = 128,\n",
    "    flatten_emb: bool | str = False,\n",
    "    embed_folder: str | None = None,\n",
    "    embed_layer: int | None = None,\n",
    "    seq_start_idx: bool | int = False,\n",
    "    seq_end_idx: bool | int = False,\n",
    "    subset_list: list[str] = [\"train\", \"val\", \"test\"],\n",
    "    loader_batch_size: int = 64,\n",
    "    worker_seed: int = RAND_SEED,\n",
    "    if_encode_all: bool = True,\n",
    "    **encoder_params,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    A function encode and load the data from a path\n",
    "\n",
    "    Args:\n",
    "    - dataset_path: str, full path to the dataset, in pkl or panda readable format, ie\n",
    "        \"data/proeng/gb1/low_vs_high.csv\"\n",
    "        columns include: sequence, target, set, validation,\n",
    "        mut_name (optional), mut_numb (optional)\n",
    "    - encoder_name: str, the name of the encoder\n",
    "    - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "    - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "    - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "    - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "    - embed_folder: str = None, path to presaved embedding\n",
    "    - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "    - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "    - subset_list: list of str, train, val, test\n",
    "    - loader_batch_size: int, the batch size for train, val, and test dataloader\n",
    "    - worker_seed: int, the seed for dataloader\n",
    "    - if_encode_all: bool = True, if encode full dataset all layers on the fly\n",
    "    - encoder_params: kwarg, additional parameters for encoding\n",
    "    \"\"\"\n",
    "\n",
    "    assert set(subset_list) <= set(\n",
    "        [\"train\", \"val\", \"test\"]\n",
    "    ), \"subset_list can only contain terms with in be 'train', 'val', or 'test'\"\n",
    "\n",
    "    # specify no shuffling for validation and test\n",
    "    if_shuffle_list = [True if subset == \"train\" else False for subset in subset_list]\n",
    "\n",
    "    return (\n",
    "        DataLoader(\n",
    "            dataset=ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_folder=embed_folder,\n",
    "                embed_layer=embed_layer,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                if_encode_all=if_encode_all,\n",
    "                **encoder_params,\n",
    "            ),\n",
    "            batch_size=loader_batch_size,\n",
    "            shuffle=if_shuffle,\n",
    "            worker_init_fn=worker_seed,\n",
    "        )\n",
    "        for subset, if_shuffle in zip(subset_list, if_shuffle_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for run sklearn (currently ridge) models\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scr.utils import get_folder_file_names, pickle_save, ndcg_scale\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "from scr.params.sys import RAND_SEED, SKLEARN_ALPHAS\n",
    "from scr.encoding.encoding_classes import ESMEncoder, CARPEncoder, OnehotEncoder\n",
    "\n",
    "from scr.preprocess.data_process import ProtranDataset\n",
    "\n",
    "# seed\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "\n",
    "\n",
    "class RunRidge:\n",
    "    \"\"\"A class for running ridge regression\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        all_embed_layers: bool = False,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        if_encode_all: bool = True,\n",
    "        alphas: np.ndarray | int = SKLEARN_ALPHAS,\n",
    "        ridge_state: int = RAND_SEED,\n",
    "        ridge_params: dict | None = None,\n",
    "        all_result_folder: str = \"results/sklearn\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - alphas: np.ndarray, arrays of alphas to be tested\n",
    "        - ridge_state: int = RAND_SEED, seed the ridge regression\n",
    "        - ridge_params: dict | None = None, other ridge regression args\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.encoder_name = encoder_name\n",
    "        self.reset_param = reset_param\n",
    "        self.resample_param = resample_param\n",
    "        self.embed_batch_size = embed_batch_size\n",
    "        self.flatten_emb = flatten_emb\n",
    "        self.embed_folder = embed_folder\n",
    "        self.all_embed_layers = all_embed_layers\n",
    "        self.seq_start_idx = seq_start_idx\n",
    "        self.seq_end_idx = seq_end_idx\n",
    "        self.if_encode_all = if_encode_all\n",
    "        self.encoder_params = encoder_params\n",
    "\n",
    "        if not isinstance(alphas, np.ndarray):\n",
    "            alphas = np.array([alphas])\n",
    "        self.alphas = alphas\n",
    "\n",
    "        self.ridge_state = ridge_state\n",
    "        self.ridge_params = ridge_params\n",
    "        self.all_result_folder = all_result_folder\n",
    "\n",
    "        if self.reset_param and \"-rand\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-rand\"\n",
    "\n",
    "        if self.resample_param and \"-stat\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-stat\"\n",
    "\n",
    "        all_ridge_results = {}\n",
    "\n",
    "        if self.encoder_name in TRANSFORMER_INFO.keys():\n",
    "            total_emb_layer = TRANSFORMER_INFO[encoder_name][1] + 1\n",
    "        elif self.encoder_name in CARP_INFO.keys():\n",
    "            total_emb_layer = CARP_INFO[encoder_name][1]\n",
    "        else:\n",
    "            # for onehot\n",
    "            self.encoder_name = \"onehot\"\n",
    "            total_emb_layer = 1\n",
    "\n",
    "        if self.all_embed_layers:\n",
    "            print(\"loading all embed layers...\")\n",
    "            # loader has ALL embedding layers\n",
    "            self.train_ds, self.val_ds, self.test_ds = (\n",
    "                ProtranDataset(\n",
    "                    dataset_path=self.dataset_path,\n",
    "                    subset=subset,\n",
    "                    encoder_name=self.encoder_name,\n",
    "                    reset_param=self.reset_param,\n",
    "                    resample_param=self.resample_param,\n",
    "                    embed_batch_size=self.embed_batch_size,\n",
    "                    flatten_emb=self.flatten_emb,\n",
    "                    embed_folder=self.embed_folder,\n",
    "                    embed_layer=self.embed_layer,\n",
    "                    seq_start_idx=self.seq_start_idx,\n",
    "                    seq_end_idx=self.seq_end_idx,\n",
    "                    if_encode_all=self.if_encode_all,\n",
    "                    **self.encoder_params,\n",
    "                )\n",
    "                for subset in [\"train\", \"val\", \"test\"]\n",
    "            )\n",
    "\n",
    "        for layer in range(total_emb_layer):\n",
    "            all_ridge_results[layer] = self.run_ridge_layer(embed_layer=layer,)\n",
    "\n",
    "        self._all_ridge_results = all_ridge_results\n",
    "\n",
    "    def sk_test(\n",
    "        self, model: sklearn.linear_model, ds: ProtranDataset, embed_layer: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for testing sklearn models for a specific layer of embeddings\n",
    "\n",
    "        Args:\n",
    "        - model: sklearn.linear_model, trained model\n",
    "        - ds: ProtranDataset, train, val, or test dataset\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - np.concatenate(pred): np.ndarray, 1D predicted fitness values\n",
    "        - np.concatenate(true): np.ndarry, 1D true fitness values\n",
    "        \"\"\"\n",
    "        return (\n",
    "            model.predict(getattr(ds, \"layer\" + str(embed_layer))).squeeze(),\n",
    "            ds.y.squeeze(),\n",
    "        )\n",
    "\n",
    "    def pick_model(self, embed_layer: int, train_ds: Dataset, val_ds: Dataset):\n",
    "        \"\"\"\n",
    "        A function for picking the best model for given alaphs, meaning\n",
    "        lower train_mse and higher test_ndcg\n",
    "        NOTE: alphas tuning is NOT currently optimal\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - sklearn.linear_model, the model with the best alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # init values for comparison\n",
    "        best_mse = np.Inf\n",
    "        best_ndcg = -1\n",
    "        best_rho = -1\n",
    "        best_model = None\n",
    "\n",
    "        # loop through all alphas\n",
    "        for alpha in self.alphas:\n",
    "\n",
    "            # init model for each alpha\n",
    "            if self.ridge_params is None:\n",
    "                self.ridge_params = {}\n",
    "            model = Ridge(\n",
    "                alpha=alpha, random_state=self.ridge_state, **self.ridge_params\n",
    "            )\n",
    "\n",
    "            # fit the model for a given layer of embedding\n",
    "            fitness_scaler = StandardScaler()\n",
    "\n",
    "            if self.all_embed_layers:\n",
    "                train_ds = self.train_ds\n",
    "                val_ds = self.val_ds\n",
    "\n",
    "            model.fit(\n",
    "                getattr(train_ds, \"layer\" + str(embed_layer)),\n",
    "                fitness_scaler.fit_transform(train_ds.y),\n",
    "            )\n",
    "\n",
    "            # eval the model with train and test\n",
    "            train_pred, train_true = self.sk_test(\n",
    "                model, train_ds, embed_layer=embed_layer\n",
    "            )\n",
    "            val_pred, val_true = self.sk_test(model, val_ds, embed_layer=embed_layer)\n",
    "\n",
    "            # calc the metrics\n",
    "            train_mse = mean_squared_error(train_true, train_pred)\n",
    "            val_ndcg = ndcg_scale(val_true, val_pred)\n",
    "            val_rho = spearmanr(val_true, val_pred)[0]\n",
    "\n",
    "            # update the model if it has lower train_mse and higher val_ndcg\n",
    "            if train_mse < best_mse and val_ndcg > best_ndcg:\n",
    "                best_model = model\n",
    "                best_mse = train_mse\n",
    "                best_ndcg = val_ndcg\n",
    "                best_rho = val_rho\n",
    "\n",
    "        print(f\"best model is {best_model}\")\n",
    "        return best_model\n",
    "\n",
    "    def run_ridge_layer(\n",
    "        self, embed_layer: int,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running ridge regression for a given layer of embedding\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"train\": {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"\"\"\n",
    "\n",
    "        # set up the datasets\n",
    "        if self.all_embed_layers:\n",
    "            ds_list = [self.train_ds, self.val_ds, self.test_ds]\n",
    "        else:\n",
    "            print(f\"Getting embed for {embed_layer}...\")\n",
    "            ds_list = [\n",
    "                ProtranDataset(\n",
    "                    dataset_path=self.dataset_path,\n",
    "                    subset=subset,\n",
    "                    encoder_name=self.encoder_name,\n",
    "                    reset_param=self.reset_param,\n",
    "                    resample_param=self.resample_param,\n",
    "                    embed_batch_size=self.embed_batch_size,\n",
    "                    flatten_emb=self.flatten_emb,\n",
    "                    embed_folder=self.embed_folder,\n",
    "                    embed_layer=embed_layer,\n",
    "                    seq_start_idx=self.seq_start_idx,\n",
    "                    seq_end_idx=self.seq_end_idx,\n",
    "                    if_encode_all=self.if_encode_all,\n",
    "                    **self.encoder_params,\n",
    "                )\n",
    "                for subset in [\"train\", \"val\", \"test\"]\n",
    "            ]\n",
    "\n",
    "        # train and get the best alpha\n",
    "        best_model = self.pick_model(\n",
    "            embed_layer=embed_layer, train_ds=ds_list[0], val_ds=ds_list[1]\n",
    "        )\n",
    "\n",
    "        # init dict for resulted outputs\n",
    "        result_dict = {}\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, ds in zip([\"train\", \"val\", \"test\"], ds_list):\n",
    "            pred, true = self.sk_test(best_model, ds, embed_layer=embed_layer)\n",
    "\n",
    "            result_dict[subset] = {\n",
    "                \"mse\": mean_squared_error(true, pred),\n",
    "                \"pred\": pred,\n",
    "                \"true\": true,\n",
    "                \"ndcg\": ndcg_scale(true, pred),\n",
    "                \"rho\": spearmanr(true, pred),\n",
    "            }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=self.all_result_folder,\n",
    "            dataset_path=self.dataset_path,\n",
    "            encoder_name=self.encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self.flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    @property\n",
    "    def all_ridge_results(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"layer#\": {\n",
    "                        \"train\": {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"val\":   {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"test\":  {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        }\n",
    "        \"\"\"\n",
    "        return self._all_ridge_results\n",
    "\n",
    "\n",
    "class RunSK:\n",
    "    \"\"\"\n",
    "    A class for running sklearn models \n",
    "    [NOT FULLY TESTED YET]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        encoder_name: str,\n",
    "        reset_param: bool = False,\n",
    "        resample_param: bool = False,\n",
    "        embed_batch_size: int = 128,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_folder: str | None = None,\n",
    "        seq_start_idx: bool | int = False,\n",
    "        seq_end_idx: bool | int = False,\n",
    "        alphas: np.ndarray | int = SKLEARN_ALPHAS,\n",
    "        sklearn_state: int = RAND_SEED,\n",
    "        sklearn_params: dict | None = None,\n",
    "        all_result_folder: str = \"results/sklearn\",\n",
    "        **encoder_params,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation,\n",
    "            mut_name (optional), mut_numb (optional)\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - reset_param: bool = False, if update the full model to xavier_uniform_\n",
    "        - resample_param: bool = False, if update the full model to xavier_normal_\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_folder: str = None, path to presaved embedding\n",
    "        - seq_start_idx: bool | int = False, the index for the start of the sequence\n",
    "        - seq_end_idx: bool | int = False, the index for the end of the sequence\n",
    "        - alphas: np.ndarray, arrays of alphas to be tested\n",
    "        - sklearn_state: int = RAND_SEED, seed the ridge or logistic regression\n",
    "        - sklearn_params: dict | None = None, other ridge or logistic regression args\n",
    "        - all_result_folder: str = \"results/train_val_test\", the parent folder for all results\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.encoder_name = encoder_name\n",
    "        self.reset_param = reset_param\n",
    "        self.resample_param = resample_param\n",
    "        self.flatten_emb = flatten_emb\n",
    "\n",
    "        if not isinstance(alphas, np.ndarray):\n",
    "            alphas = np.array([alphas])\n",
    "        self.alphas = alphas\n",
    "\n",
    "        self.sklearn_state = sklearn_state\n",
    "        self.sklearn_params = sklearn_params\n",
    "        self.all_result_folder = all_result_folder\n",
    "\n",
    "        if self.reset_param and \"-rand\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-rand\"\n",
    "\n",
    "        if self.resample_param and \"-stat\" not in self.all_result_folder:\n",
    "            self.all_result_folder = f\"{self.all_result_folder}-stat\"\n",
    "\n",
    "        # loader has ALL embedding layers\n",
    "        self.train_ds, self.val_ds, self.test_ds = (\n",
    "            ProtranDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                subset=subset,\n",
    "                encoder_name=encoder_name,\n",
    "                reset_param=reset_param,\n",
    "                resample_param=resample_param,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                embed_folder=embed_folder,\n",
    "                seq_start_idx=seq_start_idx,\n",
    "                seq_end_idx=seq_end_idx,\n",
    "                **encoder_params,\n",
    "            )\n",
    "            for subset in [\"train\", \"val\", \"test\"]\n",
    "        )\n",
    "\n",
    "        # pick ridge regression if y numerical\n",
    "        if self.val_ds.y.dtype.kind in \"iufc\":\n",
    "            self.sklearn_model = Ridge\n",
    "\n",
    "        # pick logistic regression if y is categorical\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            self.train_ds.y, self.val_ds.y, self.test_ds.y = [\n",
    "                le.fit_transform(y.flatten())\n",
    "                for y in [self.train_ds.y, self.val_ds.y, self.test_ds.y]\n",
    "            ]\n",
    "            self.sklearn_model = LogisticRegression\n",
    "            # convert alpha to C\n",
    "            self.alphas = 1 / self.alphas\n",
    "            # add other params\n",
    "            if self.sklearn_params is None:\n",
    "                self.sklearn_params[\"multi_class\"] = \"multinomial\"\n",
    "                self.sklearn_params[\"max_iter\"] = 1000\n",
    "\n",
    "        all_sklearn_results = {}\n",
    "\n",
    "        # TODO for easier total_emb_layer\n",
    "        if self.encoder_name in TRANSFORMER_INFO.keys():\n",
    "            total_emb_layer = TRANSFORMER_INFO[encoder_name][1] + 1\n",
    "        elif self.encoder_name in CARP_INFO.keys():\n",
    "            total_emb_layer = CARP_INFO[encoder_name][1]\n",
    "        else:\n",
    "            # for onehot\n",
    "            self.encoder_name = \"onehot\"\n",
    "            total_emb_layer = 1\n",
    "\n",
    "        for layer in range(total_emb_layer):\n",
    "            all_sklearn_results[layer] = self.run_sklearn_layer(embed_layer=layer,)\n",
    "\n",
    "        self._all_sklearn_results = all_sklearn_results\n",
    "\n",
    "    def sk_test(\n",
    "        self, model: sklearn.linear_model, ds: ProtranDataset, embed_layer: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for testing sklearn models for a specific layer of embeddings\n",
    "\n",
    "        Args:\n",
    "        - model: sklearn.linear_model, trained model\n",
    "        - ds: ProtranDataset, train, val, or test dataset\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - np.concatenate(pred): np.ndarray, 1D predicted fitness values\n",
    "        - np.concatenate(true): np.ndarry, 1D true fitness values\n",
    "        - \n",
    "        \"\"\"\n",
    "\n",
    "        if self.sklearn_model == Ridge:\n",
    "            pred_prob = None\n",
    "        else:\n",
    "            pred_prob = model.predict_proba(\n",
    "                getattr(ds, \"layer\" + str(embed_layer)).cpu().numpy()\n",
    "            ).squeeze()\n",
    "\n",
    "        return (\n",
    "            model.predict(\n",
    "                getattr(ds, \"layer\" + str(embed_layer)).cpu().numpy()\n",
    "            ).squeeze(),\n",
    "            ds.y.squeeze(),\n",
    "            pred_prob,\n",
    "        )\n",
    "\n",
    "    def pick_model(\n",
    "        self, embed_layer: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function for picking the best model for given alaphs, meaning\n",
    "        lower train_mse and higher test_ndcg\n",
    "        NOTE: alphas tuning is NOT currently optimal\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - sklearn.linear_model, the model with the best alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # init values for comparison\n",
    "        if self.sklearn_model == Ridge:\n",
    "            best_mse = np.Inf\n",
    "            best_ndcg = -1\n",
    "            best_rho = -1\n",
    "        else:\n",
    "            best_loss = np.Inf\n",
    "            best_acc = 0\n",
    "            best_auc = 0\n",
    "\n",
    "        best_model = None\n",
    "\n",
    "        # loop through all alphas\n",
    "        for alpha in self.alphas:\n",
    "\n",
    "            # init model for each alpha\n",
    "            if self.sklearn_params is None:\n",
    "                self.sklearn_params = {}\n",
    "            model = self.sklearn_model(\n",
    "                alpha=alpha, random_state=self.sklearn_state, **self.sklearn_params\n",
    "            )\n",
    "\n",
    "            # fit the model for a given layer of embedding\n",
    "            fitness_scaler = StandardScaler()\n",
    "            model.fit(\n",
    "                getattr(self.train_ds, \"layer\" + str(embed_layer)).cpu().numpy(),\n",
    "                fitness_scaler.fit_transform(self.train_ds.y),\n",
    "            )\n",
    "\n",
    "            # eval the model with train and test\n",
    "            train_pred, train_true, train_prob = self.sk_test(\n",
    "                model, self.train_ds, embed_layer=embed_layer\n",
    "            )\n",
    "            val_pred, val_true, val_prob = self.sk_test(\n",
    "                model, self.val_ds, embed_layer=embed_layer\n",
    "            )\n",
    "\n",
    "            if self.sklearn_model == Ridge:\n",
    "                # calc the metrics\n",
    "                train_mse = mean_squared_error(train_true, train_pred)\n",
    "                val_ndcg = ndcg_scale(val_true, val_pred)\n",
    "                val_rho = spearmanr(val_true, val_pred)[0]\n",
    "\n",
    "                # update the model if it has lower train_mse and higher val_ndcg\n",
    "                if train_mse < best_mse and val_ndcg > best_ndcg:\n",
    "                    best_model = model\n",
    "                    best_mse = train_mse\n",
    "                    best_ndcg = val_ndcg\n",
    "                    best_rho = val_rho\n",
    "\n",
    "            else:\n",
    "                # calc the metrics\n",
    "                train_loss = log_loss(train_true, train_prob)\n",
    "                val_acc = accuracy_score(val_true, val_pred)\n",
    "                val_auc = roc_auc_score(val_true, val_prob, multi_class=\"ovo\")\n",
    "\n",
    "                # update the model if it has lower log_loss and higher val_auc\n",
    "                if train_loss < best_loss and val_auc > best_auc:\n",
    "                    best_loss = train_loss\n",
    "                    best_acc = val_acc\n",
    "                    best_auc = val_auc\n",
    "\n",
    "        print(f\"best model is {best_model}\")\n",
    "        return best_model\n",
    "\n",
    "    def run_sklearn_layer(\n",
    "        self, embed_layer: int,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        A function for running ridge or logistics regression for a given layer of embedding\n",
    "\n",
    "        Args:\n",
    "        - embed_layer: int, specific layer of the embedding\n",
    "\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"train\": {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"val\":   {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "            \"test\":  {\"mse\": float,\n",
    "                    \"pred\": np.ndarray,\n",
    "                    \"true\": np.ndarray,\n",
    "                    \"ndcg\": float,\n",
    "                    \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "        \"\"\"\n",
    "\n",
    "        # train and get the best alpha\n",
    "        best_model = self.pick_model(embed_layer=embed_layer,)\n",
    "\n",
    "        # init dict for resulted outputs\n",
    "        result_dict = {}\n",
    "\n",
    "        # now test the model with the test data\n",
    "        for subset, ds in zip(\n",
    "            [\"train\", \"val\", \"test\"], [self.train_ds, self.val_ds, self.test_ds],\n",
    "        ):\n",
    "            pred, true, prob = self.sk_test(best_model, ds, embed_layer=embed_layer)\n",
    "\n",
    "            if self.sklearn_model == Ridge:\n",
    "                result_dict[subset] = {\n",
    "                    \"mse\": mean_squared_error(true, pred),\n",
    "                    \"pred\": pred,\n",
    "                    \"true\": true,\n",
    "                    \"ndcg\": ndcg_scale(true, pred),\n",
    "                    \"rho\": spearmanr(true, pred),\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                result_dict[subset] = {\n",
    "                    \"log\": log_loss(true, prob),\n",
    "                    \"pred\": pred,\n",
    "                    \"prob\": prob,\n",
    "                    \"true\": true,\n",
    "                    \"acc\": accuracy_score(true, pred),\n",
    "                    \"rocauc\": roc_auc_score(true, prob, multi_class=\"ovo\"),\n",
    "                }\n",
    "\n",
    "        dataset_subfolder, file_name = get_folder_file_names(\n",
    "            parent_folder=self.all_result_folder,\n",
    "            dataset_path=self.dataset_path,\n",
    "            encoder_name=self.encoder_name,\n",
    "            embed_layer=embed_layer,\n",
    "            flatten_emb=self.flatten_emb,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving results for {file_name} to: {dataset_subfolder}...\")\n",
    "        pickle_save(\n",
    "            what2save=result_dict,\n",
    "            where2save=os.path.join(dataset_subfolder, file_name + \".pkl\"),\n",
    "        )\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    @property\n",
    "    def all_sklearn_results(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - dict, with the keys and dict values\n",
    "            \"layer#\": {\n",
    "                        \"train\": {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"val\":   {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        \"test\":  {\"mse\": float,\n",
    "                                \"pred\": np.ndarray,\n",
    "                                \"true\": np.ndarray,\n",
    "                                \"ndcg\": float,\n",
    "                                \"rho\": SpearmanrResults(correlation=float, pvalue=float)}\n",
    "                        }\n",
    "        \"\"\"\n",
    "        return self._all_sklearn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embed for 0...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=5.35783e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is Ridge(alpha=0.001, random_state=42)\n",
      "Making test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S ...\n",
      "Making test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean ...\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_0 to: test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean...\n",
      "Getting embed for 1...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=6.37587e-11): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.94957e-10): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.65884e-10): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=4.54099e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=4.05554e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is Ridge(alpha=0.001, random_state=42)\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_1 to: test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean...\n",
      "Getting embed for 2...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.97021e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.0047e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.37474e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=6.22379e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=5.86424e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is Ridge(alpha=0.001, random_state=42)\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_2 to: test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean...\n",
      "Getting embed for 3...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.85371e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.8602e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.92408e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/anaconda/envs/protran/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=4.55431e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is Ridge(alpha=0.001, random_state=42)\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_3 to: test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean...\n",
      "Getting embed for 4...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is Ridge(alpha=0.001, random_state=42)\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_4 to: test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean...\n",
      "Getting embed for 5...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is Ridge(alpha=0.001, random_state=42)\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_5 to: test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean...\n",
      "Getting embed for 6...\n",
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating esm1_t6_43M_UR50S upto 6 layer embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is Ridge(alpha=0.001, random_state=42)\n",
      "Saving results for esm1_t6_43M_UR50S-mean-layer_6 to: test/sklearn/proeng/thermo/mixed_split/esm1_t6_43M_UR50S/mean...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RunRidge at 0x7fc334e50250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunRidge(\n",
    "    dataset_path=\"data/proeng/thermo/mixed_split.csv\",\n",
    "    encoder_name=\"esm1_t6_43M_UR50S\",\n",
    "    reset_param= False,\n",
    "    resample_param = False,\n",
    "    embed_batch_size = 128,\n",
    "    flatten_emb = \"mean\",\n",
    "    embed_folder = \"embeddings/proeng/thermo/mixed_split\",\n",
    "    all_embed_layers=False,\n",
    "    seq_start_idx = False,\n",
    "    seq_end_idx = False,\n",
    "    if_encode_all=False,\n",
    "    alphas = SKLEARN_ALPHAS,\n",
    "    ridge_state= RAND_SEED,\n",
    "    ridge_params= None,\n",
    "    all_result_folder = \"test/sklearn\",\n",
    "    # **encoder_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
