{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-fli/repo/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/repo/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.utils import pickle_load\n",
    "from scr.preprocess.data_process import TaskProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "      <th>split</th>\n",
       "      <th>csv_path</th>\n",
       "      <th>fasta_path</th>\n",
       "      <th>pkl_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annotation</td>\n",
       "      <td>scl</td>\n",
       "      <td>balanced</td>\n",
       "      <td>data/annotation/scl/balanced.csv</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>structure</td>\n",
       "      <td>secondary_structure</td>\n",
       "      <td>cb513</td>\n",
       "      <td>[data/structure/secondary_structure/secondary_...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>proeng</td>\n",
       "      <td>gb1</td>\n",
       "      <td>two_vs_rest</td>\n",
       "      <td>data/proeng/gb1/two_vs_rest.csv</td>\n",
       "      <td>data/proeng/gb1/5LDE_1.fasta</td>\n",
       "      <td>data/proeng/gb1/two_vs_rest.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>proeng</td>\n",
       "      <td>gb1</td>\n",
       "      <td>low_vs_high</td>\n",
       "      <td>data/proeng/gb1/low_vs_high.csv</td>\n",
       "      <td>data/proeng/gb1/5LDE_1.fasta</td>\n",
       "      <td>data/proeng/gb1/low_vs_high.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>proeng</td>\n",
       "      <td>aav</td>\n",
       "      <td>one_vs_many</td>\n",
       "      <td>data/proeng/aav/one_vs_many.csv</td>\n",
       "      <td>data/proeng/aav/P03135.fasta</td>\n",
       "      <td>data/proeng/aav/one_vs_many.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>proeng</td>\n",
       "      <td>aav</td>\n",
       "      <td>two_vs_many</td>\n",
       "      <td>data/proeng/aav/two_vs_many.csv</td>\n",
       "      <td>data/proeng/aav/P03135.fasta</td>\n",
       "      <td>data/proeng/aav/two_vs_many.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>proeng</td>\n",
       "      <td>thermo</td>\n",
       "      <td>mixed</td>\n",
       "      <td>data/proeng/thermo/mixed.csv</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         task              dataset        split  \\\n",
       "0  annotation                  scl     balanced   \n",
       "1   structure  secondary_structure        cb513   \n",
       "2      proeng                  gb1  two_vs_rest   \n",
       "3      proeng                  gb1  low_vs_high   \n",
       "4      proeng                  aav  one_vs_many   \n",
       "5      proeng                  aav  two_vs_many   \n",
       "6      proeng               thermo        mixed   \n",
       "\n",
       "                                            csv_path  \\\n",
       "0                   data/annotation/scl/balanced.csv   \n",
       "1  [data/structure/secondary_structure/secondary_...   \n",
       "2                    data/proeng/gb1/two_vs_rest.csv   \n",
       "3                    data/proeng/gb1/low_vs_high.csv   \n",
       "4                    data/proeng/aav/one_vs_many.csv   \n",
       "5                    data/proeng/aav/two_vs_many.csv   \n",
       "6                       data/proeng/thermo/mixed.csv   \n",
       "\n",
       "                     fasta_path                         pkl_path  \n",
       "0                                                                 \n",
       "1                                                                 \n",
       "2  data/proeng/gb1/5LDE_1.fasta  data/proeng/gb1/two_vs_rest.pkl  \n",
       "3  data/proeng/gb1/5LDE_1.fasta  data/proeng/gb1/low_vs_high.pkl  \n",
       "4  data/proeng/aav/P03135.fasta  data/proeng/aav/one_vs_many.pkl  \n",
       "5  data/proeng/aav/P03135.fasta  data/proeng/aav/two_vs_many.pkl  \n",
       "6                                                                 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskProcess().sum_file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.preprocess.data_process import ProtranDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43,\n",
       " ('MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVNGEWTYDDATKTFTVTELEVLFQGPLDPNSMATYEVLCEVARKLGTDDREVVLFLLNVFIPQPTLAQLIGALRALKEEGRLTFPLLAECLFRAGRRDLLRDLLHLDPRFLERHLAGTMSYFSPYQLTVLHVDGELCARDIRSLIFLSKDTIGSRSTPQTFLHWVYCMENLDLLGPTDVDALMSMLRSLSRVDLQRQVQTLMGLHLSGPSHSQHYRHTPLEHHHHHH',\n",
       "  1.80292279874,\n",
       "  'D40N',\n",
       "  1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\"\n",
    "val_dataset = ProtranDataset(dataset_path=dataset_path, subset=\"val\")\n",
    "len(val_dataset), val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVNGEWTYDDATKTFTVTELEVLFQGPLDPNSMATYEVLCEVARKLGTDDREVVLFLLNVFIPQPTLAQLIGALRALKEEGRLTFPLLAECLFRAGRRDLLRDLLHLDPRFLERHLAGTMSYFSPYQLTVLHVDGELCARDIRSLIFLSKDTIGSRSTPQTFLHWVYCMENLDLLGPTDVDALMSMLRSLSRVDLQRQVQTLMGLHLSGPSHSQHYRHTPLEHHHHHH',\n",
       " 1.80292279874,\n",
       " 'D40N',\n",
       " 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.x[0], val_dataset.y[0], val_dataset.mut_name[0], val_dataset.mut_numb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1678,\n",
       " ('MATPSAAFEALMNGVTSWDVPEDAVPCELLLIGEASFPVMVNDMGQVLIAASSYGRGRLVVVSHEDYLVEAQLTPFLLNAVGWLCSSPGAPIGVHPSLAPLAKILEGSGVDAKVEPEVKDSLGVYCIDAYNETMTEKLVKFMKCGGGLLIGGQAWDWANQGEDERVLFTFPGNLVTSVAGIYFTDNKGDTSFFKVSKKMPKIPVLVSCEDDLSDDREELLHGISELDISNSDCFPSQLLVHGALAFPLGLDSYHGCVIAAARYGRGRVVVTGHKVLFTVGKLGPFLLNAVRWLDGGRRGKVVVQTELRTLSGLLAVGGIDTSIEPNLTSDASVYCFEPVSEVGVKELQEFVAEGGGLFVGAQAWWWAFKNPGVSPLARFPGNLLLNPFGISITSQSLNPGPFRTPKAGIRTYHFRSTLAEFQVIMGRKRGNVEKGWLAKLGPDGAAFLQIPAEEIPAYMSVHRLLRKLLSRYRLPVATRENPVINDCCRGAMLSLATGLAHSGSDLSLLVPEIEDMYSSPYLRPSESPITVEVNCTNPGTRYCWMSTGLYIPGRQIIEVSLPEAAASADLKIQIGCHTDDLTRASKLFRGPLVINRCCLDKPTKSITCLWGGLLYIIVPQNSKLGSVPVTVKGAVHAPYYKLGETTLEEWKRRIQENPGPWGELATDNIILTVPTANLRTLENPEPLLRLWDEVMQAVARLGAEPFPLRLPQRIVADVQISVGWMHAGYPIMCHLESVQELINEKLIRTKGLWGPVHELGRNQQRQEWEFPPHTTEATCNLWCVYVHETVLGIPRSRANIALWPPVREKRVRIYLSKGPNVKNWNAWTALETYLQLQEAFGWEPFIRLFTEYRNQTNLPTENVDKMNLWVKMFSHQVQKNLAPFFEAWAWPIQKEVATSLAYLPEWKENIMKLYLLTQMPH',\n",
       "  'Cell membrane',\n",
       "  '',\n",
       "  nan))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"data/annotation/scl/balanced.csv\"\n",
    "val_dataset = ProtranDataset(dataset_path=dataset_path, subset=\"val\")\n",
    "len(val_dataset), val_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MATPSAAFEALMNGVTSWDVPEDAVPCELLLIGEASFPVMVNDMGQVLIAASSYGRGRLVVVSHEDYLVEAQLTPFLLNAVGWLCSSPGAPIGVHPSLAPLAKILEGSGVDAKVEPEVKDSLGVYCIDAYNETMTEKLVKFMKCGGGLLIGGQAWDWANQGEDERVLFTFPGNLVTSVAGIYFTDNKGDTSFFKVSKKMPKIPVLVSCEDDLSDDREELLHGISELDISNSDCFPSQLLVHGALAFPLGLDSYHGCVIAAARYGRGRVVVTGHKVLFTVGKLGPFLLNAVRWLDGGRRGKVVVQTELRTLSGLLAVGGIDTSIEPNLTSDASVYCFEPVSEVGVKELQEFVAEGGGLFVGAQAWWWAFKNPGVSPLARFPGNLLLNPFGISITSQSLNPGPFRTPKAGIRTYHFRSTLAEFQVIMGRKRGNVEKGWLAKLGPDGAAFLQIPAEEIPAYMSVHRLLRKLLSRYRLPVATRENPVINDCCRGAMLSLATGLAHSGSDLSLLVPEIEDMYSSPYLRPSESPITVEVNCTNPGTRYCWMSTGLYIPGRQIIEVSLPEAAASADLKIQIGCHTDDLTRASKLFRGPLVINRCCLDKPTKSITCLWGGLLYIIVPQNSKLGSVPVTVKGAVHAPYYKLGETTLEEWKRRIQENPGPWGELATDNIILTVPTANLRTLENPEPLLRLWDEVMQAVARLGAEPFPLRLPQRIVADVQISVGWMHAGYPIMCHLESVQELINEKLIRTKGLWGPVHELGRNQQRQEWEFPPHTTEATCNLWCVYVHETVLGIPRSRANIALWPPVREKRVRIYLSKGPNVKNWNAWTALETYLQLQEAFGWEPFIRLFTEYRNQTNLPTENVDKMNLWVKMFSHQVQKNLAPFFEAWAWPIQKEVATSLAYLPEWKENIMKLYLLTQMPH',\n",
       " 'Cell membrane',\n",
       " '',\n",
       " nan)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.x[0], val_dataset.y[0], val_dataset.mut_name[0], val_dataset.mut_numb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.encoding.encoding_classes import ESMEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ESMEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1b_t33_650M_UR50S using 0 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1280)\n",
      "Loading esm1b_t33_650M_UR50S using 0 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1280)\n"
     ]
    }
   ],
   "source": [
    "dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\"\n",
    "val_dataset = ProtranDataset(dataset_path=dataset_path, subset=\"val\")\n",
    "val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            worker_init_fn=42,\n",
    "        )\n",
    "\n",
    "for val_batch in val_loader:\n",
    "    mean_flat_encoder = encoder(\n",
    "        encoder_name=\"esm1b_t33_650M_UR50S\",\n",
    "        embed_layer=0,\n",
    "    ).encode(mut_seqs=val_batch[0], mut_names=val_batch[2],flatten_emb=\"mean\")\n",
    "    one_mean_emb = next(mean_flat_encoder)\n",
    "    print(one_mean_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"data/annotation/scl/balanced.csv\"\n",
    "val_dataset = ProtranDataset(dataset_path=dataset_path, subset=\"val\")\n",
    "val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            worker_init_fn=42,\n",
    "        )\n",
    "for val_batch in val_loader:\n",
    "    mean_flat_encoder = ESMEncoder(\n",
    "        encoder_name=\"esm1b_t33_650M_UR50S\",\n",
    "        embed_layer=0,\n",
    "    ).encode(mut_seqs=val_batch[0], mut_names=val_batch[2],flatten_emb=\"mean\")\n",
    "    one_mean_emb = next(mean_flat_encoder)\n",
    "    print(one_mean_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-processing the dataset\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Sequence\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scr.utils import pickle_save, pickle_load, replace_ext\n",
    "from scr.preprocess.seq_loader import SeqLoader\n",
    "from scr.encoding.encoding_classes import AbstractEncoder\n",
    "\n",
    "\n",
    "def get_mut_name(mut_seq: str, parent_seq: str) -> str:\n",
    "    \"\"\"\n",
    "    A function for returning the mutant name\n",
    "\n",
    "    Args:\n",
    "    - mut_seq: str, the full mutant sequence\n",
    "    - parent_seq: str, the full parent sequence\n",
    "\n",
    "    Returns:\n",
    "    - str, parent, indel, or mutant name in the format of\n",
    "        ParentAAMutLocMutAA:ParentAAMutLocMutAA:..., ie. W39W:D40G:G41C:V54Q\n",
    "    \"\"\"\n",
    "\n",
    "    mut_list = []\n",
    "    if parent_seq == mut_seq:\n",
    "        return \"parent\"\n",
    "    elif len(parent_seq) == len(mut_seq):\n",
    "        for i, (p, m) in enumerate(zip(list(parent_seq), list(mut_seq))):\n",
    "            if p != m:\n",
    "                mut_list.append(f\"{p}{i+1}{m}\")\n",
    "        return \":\".join(mut_list)\n",
    "    else:\n",
    "        return \"indel\"\n",
    "\n",
    "\n",
    "class AddMutInfo:\n",
    "    \"\"\"A class for appending mutation info for mainly protein engineering tasks\"\"\"\n",
    "\n",
    "    def __init__(self, parent_seq_path: str, csv_path: str):\n",
    "\n",
    "        # Load the parent sequence from the fasta file\n",
    "        self._parent_seq = SeqLoader(parent_seq_path=parent_seq_path)\n",
    "\n",
    "        # load the dataframe\n",
    "        self._init_df = pd.read_csv(csv_path)\n",
    "\n",
    "        self._df = self._init_df.copy()\n",
    "        # add a column with the mutant names\n",
    "        self._df[\"mut_name\"] = self._init_df[\"sequence\"].apply(\n",
    "            get_mut_name, parent_seq=self._parent_seq\n",
    "        )\n",
    "        # add a column with the number of mutations\n",
    "        self._df[\"mut_numb\"] = (\n",
    "            self._df[\"mut_name\"].str.split(\":\").map(len, na_action=\"ignore\")\n",
    "        )\n",
    "\n",
    "        # get the pickle file path\n",
    "        self._pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "\n",
    "        pickle_save(what2save=self._df, where2save=self._pkl_path)\n",
    "\n",
    "    @property\n",
    "    def parent_seq(self) -> str:\n",
    "        \"\"\"Return the parent sequence\"\"\"\n",
    "        return self._parent_seq\n",
    "\n",
    "    @property\n",
    "    def pkl_path(self) -> str:\n",
    "        \"\"\"Return the pkl file path for the processed dataframe\"\"\"\n",
    "        return self._pkl_path\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the processed dataframe\"\"\"\n",
    "        return self._df\n",
    "\n",
    "\n",
    "class TaskProcess:\n",
    "    \"\"\"A class for handling different downstream tasks\"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_folder: str, a folder path with all the tasks as subfolders where\n",
    "            all the subfolders have datasets as the subsubfolders, ie\n",
    "\n",
    "            {data_folder}/\n",
    "                proeng/\n",
    "                    aav/\n",
    "                        one_vs_many.csv\n",
    "                        two_vs_many.csv\n",
    "                        P03135.fasta\n",
    "                    thermo/\n",
    "                        mixed.csv\n",
    "        \"\"\"\n",
    "\n",
    "        if data_folder[-1] == \"/\":\n",
    "            self._data_folder = data_folder\n",
    "        else:\n",
    "            self._data_folder = data_folder + \"/\"\n",
    "\n",
    "        # sumamarize all files i nthe data folder\n",
    "        self._sum_file_df = self.sum_files()\n",
    "\n",
    "    def sum_files(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Summarize all files in the data folder\n",
    "\n",
    "        Returns:\n",
    "        - A dataframe with \"task\", \"dataset\", \"split\", \"csv_path\", \"fasta_path\", \"pkl_path\" as columns, ie.\n",
    "            (proeng, gb1, low_vs_high, data/proeng/gb1/low_vs_high.csv, data/proeng/gb1/5LDE_1.fasta)\n",
    "            note that csv_path is the list of lmdb files for the structure task\n",
    "        \"\"\"\n",
    "        dataset_folders = glob(f\"{self._data_folder}*/*\")\n",
    "        # need a list of tuples in the order of:\n",
    "        # (task, dataset, split, csv_path, fasta_path)\n",
    "        list_for_df = []\n",
    "        for dataset_folder in dataset_folders:\n",
    "            _, task, dataset = dataset_folder.split(\"/\")\n",
    "            if task == \"structure\":\n",
    "                structure_file_list = [\n",
    "                    file_path\n",
    "                    for file_path in glob(f\"{dataset_folder}/*.*\")\n",
    "                    if os.path.basename(os.path.splitext(file_path)[0]).split(\"_\")[-1]\n",
    "                    in [\"train\", \"valid\", \"cb513\"]\n",
    "                ]\n",
    "                list_for_df.append(\n",
    "                    tuple([task, dataset, \"cb513\", structure_file_list, \"\", \"\"])\n",
    "                )\n",
    "            else:\n",
    "                csv_paths = glob(f\"{dataset_folder}/*.csv\")\n",
    "                fasta_paths = glob(f\"{dataset_folder}/*.fasta\")\n",
    "                pkl_paths = glob(f\"{dataset_folder}/*.pkl\")\n",
    "\n",
    "                assert len(csv_paths) >= 1, \"Less than one csv\"\n",
    "                assert len(fasta_paths) <= 1, \"More than one fasta\"\n",
    "\n",
    "                for csv_path in csv_paths:\n",
    "                    # if parent seq fasta exists\n",
    "                    if len(fasta_paths) == 1:\n",
    "                        fasta_path = fasta_paths[0]\n",
    "\n",
    "                        # if no existing pkl file, generate and save\n",
    "                        if len(pkl_paths) == 0:\n",
    "                            print(f\"Adding mutation info to {csv_path}...\")\n",
    "                            pkl_path = AddMutInfo(\n",
    "                                parent_seq_path=fasta_path, csv_path=csv_path\n",
    "                            ).pkl_path\n",
    "                        # pkl file exits\n",
    "                        else:\n",
    "                            pkl_path = replace_ext(input_path=csv_path, ext=\".pkl\")\n",
    "                    # no parent fasta no pkl file\n",
    "                    else:\n",
    "                        fasta_path = \"\"\n",
    "                        pkl_path = \"\"\n",
    "\n",
    "                    list_for_df.append(\n",
    "                        tuple(\n",
    "                            [\n",
    "                                task,\n",
    "                                dataset,\n",
    "                                os.path.basename(os.path.splitext(csv_path)[0]),\n",
    "                                csv_path,\n",
    "                                fasta_path,\n",
    "                                pkl_path,\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            list_for_df,\n",
    "            columns=[\"task\", \"dataset\", \"split\", \"csv_path\", \"fasta_path\", \"pkl_path\"],\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def sum_file_df(self) -> pd.DataFrame:\n",
    "        \"\"\"A summary table for all files in the data folder\"\"\"\n",
    "        return self._sum_file_df\n",
    "\n",
    "\n",
    "class ProtranDataset(Dataset):\n",
    "\n",
    "    \"\"\"A dataset class for processing protein transfer data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        subset: str,\n",
    "        encoder_class: AbstractEncoder,\n",
    "        encoder_name: str,\n",
    "        embed_layer: int,\n",
    "        embed_batch_size: int = 0,\n",
    "        flatten_emb: bool | str = False,\n",
    "        embed_path: str = None,\n",
    "        **encoder_params,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - dataset_path: str, full path to the dataset, in pkl or panda readable format\n",
    "            columns include: sequence, target, set, validation, mut_name (optional), mut_numb (optional)\n",
    "        - subset: str, train, val, test\n",
    "        - encoder_class: AbstractEncoder, the encoder class\n",
    "        - encoder_name: str, the name of the encoder\n",
    "        - embed_layer: int, the layer number of the embedding\n",
    "        - embed_batch_size: int, set to 0 to encode all in a single batch\n",
    "        - flatten_emb: bool or str, if and how (one of [\"max\", \"mean\"]) to flatten the embedding\n",
    "        - embed_path: str = None, path to presaved embedding\n",
    "        - encoder_params: kwarg, additional parameters for encoding\n",
    "        \"\"\"\n",
    "\n",
    "        # with additional info mut_name, mut_numb\n",
    "        if os.path.splitext(dataset_path)[-1] in [\".pkl\", \".PKL\", \"\"]:\n",
    "            self._df = pickle_load(dataset_path)\n",
    "            self._add_mut_info = True\n",
    "        # without such info\n",
    "        else:\n",
    "            self._df = pd.read_csv(dataset_path)\n",
    "            self._add_mut_info = False\n",
    "\n",
    "        assert \"set\" in self._df.columns, f\"set is not a column in {dataset_path}\"\n",
    "        assert (\n",
    "            \"validation\" in self._df.columns\n",
    "        ), f\"validation is not a column in {dataset_path}\"\n",
    "\n",
    "        self._df_train = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] != True)\n",
    "        ]\n",
    "        self._df_val = self._df.loc[\n",
    "            (self._df[\"set\"] == \"train\") & (self._df[\"validation\"] == True)\n",
    "        ]\n",
    "        self._df_test = self._df.loc[(self._df[\"set\"] == \"test\")]\n",
    "\n",
    "        self._df_dict = {\n",
    "            \"train\": self._df_train,\n",
    "            \"val\": self._df_val,\n",
    "            \"test\": self._df_test,\n",
    "        }\n",
    "\n",
    "        assert subset in list(\n",
    "            self._df_dict.keys()\n",
    "        ), \"split can only be 'train', 'val', or 'test'\"\n",
    "        self._subset = subset\n",
    "\n",
    "        self._subdf_len = len(self._df_dict[self._subset])\n",
    "\n",
    "        # get unencoded string of input sequence\n",
    "        # will need to convert data type\n",
    "        self.sequence = self._get_column_value(\"sequence\")\n",
    "\n",
    "        # check if pregenerated embedding\n",
    "        if embed_path is not None:\n",
    "            print(f\"Loading pregenerated embeddings from {embed_path}\")\n",
    "            encoded_sequences = pickle_load(embed_path)\n",
    "\n",
    "        # encode the sequences without the mut_name\n",
    "        else:\n",
    "            encoded_sequences = []\n",
    "\n",
    "            for x in encoder_class(\n",
    "                encoder_name=encoder_name, embed_layer=embed_layer\n",
    "            ).encode(\n",
    "                mut_seqs=self.sequence,\n",
    "                batch_size=embed_batch_size,\n",
    "                flatten_emb=flatten_emb,\n",
    "                **encoder_params,\n",
    "            ):\n",
    "                encoded_sequences.append(x)\n",
    "\n",
    "        self.x = torch.tensor(np.vstack(encoded_sequences), dtype=torch.float32)\n",
    "\n",
    "        # get and format the fitness or secondary structure values\n",
    "        # can be numbers or string\n",
    "        # will need to convert data type\n",
    "        self.y = self._get_column_value(\"target\")\n",
    "\n",
    "        # add mut_name and mut_numb for relevant proeng datasets\n",
    "        if self._add_mut_info:\n",
    "            self.mut_name = self._get_column_value(\"mut_name\")\n",
    "            self.mut_numb = self._get_column_value(\"mut_numb\")\n",
    "        else:\n",
    "            self.mut_name = [\"\"] * self._subdf_len\n",
    "            self.mut_numb = [np.nan] * self._subdf_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the selected subset of the dataframe\"\"\"\n",
    "        return self._subdf_len\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        \"\"\"\n",
    "        Return the item in the order of\n",
    "        encoded sequence (x), target (y), sequence, mut_name (optional), mut_numb (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        return (\n",
    "            self.x[idx],\n",
    "            self.y[idx],\n",
    "            self.sequence[idx],\n",
    "            self.mut_name[idx],\n",
    "            self.mut_numb[idx],\n",
    "        )\n",
    "\n",
    "    def _get_column_value(self, column_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Check and return the column values of the selected dataframe subset\n",
    "        \"\"\"\n",
    "        if column_name in self._df.columns:\n",
    "            return self._df_dict[self._subset][column_name].values\n",
    "\n",
    "    @property\n",
    "    def df_full(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the full loaded dataset\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def df_train(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_train\n",
    "\n",
    "    @property\n",
    "    def df_val(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for validation only\"\"\"\n",
    "        return self._df_val\n",
    "\n",
    "    @property\n",
    "    def df_test(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the dataset for training only\"\"\"\n",
    "        return self._df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading esm1b_t33_650M_UR50S using 0 layer embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t-fli/.cache/torch/hub/facebookresearch_esm_main\n",
      "100%|██████████| 2/2 [00:29<00:00, 14.94s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_path=\"data/proeng/gb1/two_vs_rest.pkl\"\n",
    "val_dataset = ProtranDataset(dataset_path=dataset_path, \n",
    "                             subset=\"val\",\n",
    "                             encoder_class=ESMEncoder,\n",
    "                             encoder_name=\"esm1b_t33_650M_UR50S\",\n",
    "                             embed_layer=0,\n",
    "                             embed_batch_size=32,\n",
    "                             flatten_emb=\"mean\",\n",
    "                             embed_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43,\n",
       " (tensor([-0.1774, -0.0007,  0.0167,  ...,  0.0066,  0.0361,  0.0965]),\n",
       "  1.80292279874,\n",
       "  'MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVNGEWTYDDATKTFTVTELEVLFQGPLDPNSMATYEVLCEVARKLGTDDREVVLFLLNVFIPQPTLAQLIGALRALKEEGRLTFPLLAECLFRAGRRDLLRDLLHLDPRFLERHLAGTMSYFSPYQLTVLHVDGELCARDIRSLIFLSKDTIGSRSTPQTFLHWVYCMENLDLLGPTDVDALMSMLRSLSRVDLQRQVQTLMGLHLSGPSHSQHYRHTPLEHHHHHH',\n",
       "  'D40N',\n",
       "  1),\n",
       " torch.Size([1280]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset), val_dataset[0], val_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('protran')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019692f042c79b6731ff84413c6b50d6174007f2b51f86eed1fb032dbd4a337e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
