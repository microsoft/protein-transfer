{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/francesca/protein-transfer\n"
     ]
    }
   ],
   "source": [
    "%cd ~/protein-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.utils import pickle_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyzing per layer output\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from scr.encoding.encoding_classes import get_emb_info\n",
    "from scr.params.emb import TRANSFORMER_INFO, CARP_INFO\n",
    "from scr.params.vis import CHECKPOINT_COLOR\n",
    "from scr.utils import pickle_load, get_filename, checkNgen_folder\n",
    "\n",
    "\n",
    "class LayerLoss:\n",
    "    \"\"\"A class for handling layer analysis\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        add_checkpoint: bool = True,\n",
    "        checkpoint_list: list = [0.5, 0.25, 0.125],\n",
    "        input_path: str = \"results/sklearn\",\n",
    "        output_path: str = \"results/sklearn_layer\",\n",
    "        metric_dict: dict[list[str]] = {\n",
    "            \"proeng\": [\"train_mse\", \"val_mse\", \"test_mse\", \"test_ndcg\", \"test_rho\"],\n",
    "            \"annotation\": [\n",
    "                \"train_cross-entropy\",\n",
    "                \"val_cross-entropy\",\n",
    "                \"test_cross-entropy\",\n",
    "                \"test_acc\",\n",
    "                \"test_rocauc\",\n",
    "            ],\n",
    "            \"structure\": [\n",
    "                \"train_cross-entropy\",\n",
    "                \"val_cross-entropy\",\n",
    "                \"casp12_acc\",\n",
    "                \"casp12_rocauc\",\n",
    "                \"cb513_acc\",\n",
    "                \"cb513_rocauc\",\n",
    "                \"ts115_acc\",\n",
    "                \"ts115_rocauc\",\n",
    "            ],\n",
    "        },\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - add_checkpoint: bool = True, if add checkpoint for carp\n",
    "        - checkpoint_list: list = [0.5, 0.25, 0.125],\n",
    "        - input_path: str = \"results/sklearn\",\n",
    "        - output_path: str = \"results/sklearn_layer\"\n",
    "        - metric_dict: list[str] = [\"train_mse\", \"test_ndcg\", \"test_rho\"]\n",
    "        \"\"\"\n",
    "        self._add_checkpoint = add_checkpoint\n",
    "        self._checkpoint_list = checkpoint_list\n",
    "        # get rid of the last \"/\" if any\n",
    "        self._input_path = os.path.normpath(input_path)\n",
    "        # get the list of subfolders for each dataset\n",
    "        self._dataset_folders = glob(f\"{self._input_path}/*/*/*/*/*\")\n",
    "        # glob(\"results/train_val_test/*/*/*/*/*\")\n",
    "\n",
    "        # get rid of the last \"/\" if any\n",
    "        self._output_path = os.path.normpath(output_path)\n",
    "        self._metric_dict = metric_dict\n",
    "\n",
    "        # init a dictionary for recording outputs\n",
    "        self._onehot_baseline_dict = defaultdict(dict)\n",
    "        self._layer_analysis_dict = defaultdict(dict)\n",
    "        self._rand_layer_analysis_dict = defaultdict(dict)\n",
    "        self._stat_layer_analysis_dict = defaultdict(dict)\n",
    "\n",
    "        # init a dict for metric params\n",
    "        self._metric_numb = defaultdict(dict)\n",
    "\n",
    "        # init\n",
    "        self._checkpoint_analysis_dict = defaultdict(dict)\n",
    "        if self._add_checkpoint:\n",
    "            for checkpoint in self._checkpoint_list:\n",
    "                self._checkpoint_analysis_dict[checkpoint] = defaultdict(dict)\n",
    "\n",
    "        for dataset_folder in self._dataset_folders:\n",
    "            # dataset_folder = \"results/train_val_test/proeng/gb1/two_vs_rest/esm1b_t33_650M_UR50S/max\"\n",
    "            # get the details for the dataset such as proeng/gb1/two_vs_rest\n",
    "            task_subfolder = dataset_folder.split(self._input_path + \"/\")[-1]\n",
    "            # task_subfolder = \"proeng/gb1/two_vs_rest/esm1b_t33_650M_UR50S/max\"\n",
    "            task, dataset, split, encoder_name, flatten_emb = task_subfolder.split(\"/\")\n",
    "            # get collage_name\n",
    "            collage_name = f\"{task}_{dataset}_{split}_{flatten_emb}\"\n",
    "\n",
    "            # get number of metircs\n",
    "            self._metric_numb[collage_name] = len(self._metric_dict[task])\n",
    "\n",
    "            # parse results for plotting the collage and onehot\n",
    "            self._layer_analysis_dict[collage_name][\n",
    "                encoder_name\n",
    "            ] = self.parse_result_dicts(\n",
    "                dataset_folder, task, dataset, split, encoder_name, flatten_emb\n",
    "            )\n",
    "\n",
    "            # init\n",
    "            # check if check points exists\n",
    "            if self._add_checkpoint:\n",
    "                for checkpoint in self._checkpoint_list:\n",
    "                    checkpoint_path = f\"{self._input_path}-{str(checkpoint)}\"\n",
    "\n",
    "                    if os.path.exists(checkpoint_path):\n",
    "                        self._checkpoint_analysis_dict[checkpoint][\n",
    "                            f\"{task}_{dataset}_{split}_{flatten_emb}\"\n",
    "                        ][encoder_name] = self.parse_result_dicts(\n",
    "                            dataset_folder.replace(self._input_path, checkpoint_path),\n",
    "                            task,\n",
    "                            dataset,\n",
    "                            split,\n",
    "                            encoder_name,\n",
    "                            flatten_emb,\n",
    "                        )\n",
    "\n",
    "            # check if reset param experimental results exist\n",
    "            reset_param_path = f\"{self._input_path}-rand\"\n",
    "\n",
    "            if os.path.exists(reset_param_path):\n",
    "                self._rand_layer_analysis_dict[\n",
    "                    f\"{task}_{dataset}_{split}_{flatten_emb}\"\n",
    "                ][encoder_name] = self.parse_result_dicts(\n",
    "                    dataset_folder.replace(self._input_path, reset_param_path),\n",
    "                    task,\n",
    "                    dataset,\n",
    "                    split,\n",
    "                    encoder_name,\n",
    "                    flatten_emb,\n",
    "                )\n",
    "                add_rand = True\n",
    "            else:\n",
    "                add_rand = False\n",
    "\n",
    "            # check if resample param experimental results exist\n",
    "            resample_param_path = f\"{self._input_path}-stat\"\n",
    "\n",
    "            if os.path.exists(resample_param_path):\n",
    "                self._stat_layer_analysis_dict[\n",
    "                    f\"{task}_{dataset}_{split}_{flatten_emb}\"\n",
    "                ][encoder_name] = self.parse_result_dicts(\n",
    "                    dataset_folder.replace(self._input_path, resample_param_path),\n",
    "                    task,\n",
    "                    dataset,\n",
    "                    split,\n",
    "                    encoder_name,\n",
    "                    flatten_emb,\n",
    "                )\n",
    "                add_stat = True\n",
    "            else:\n",
    "                add_stat = False\n",
    "\n",
    "            # check if onehot experimental results exist\n",
    "            onehot_path = f\"{self._input_path}-onehot\"\n",
    "\n",
    "            if os.path.exists(onehot_path):\n",
    "                if task == \"structure\":\n",
    "                    onehot_flatten_emb_name = \"noflatten\"\n",
    "                else:\n",
    "                    onehot_flatten_emb_name = \"flatten\"\n",
    "                self._onehot_baseline_dict[\n",
    "                    f\"{task}_{dataset}_{split}\"\n",
    "                ] = self.parse_result_dicts(\n",
    "                    dataset_folder.replace(self._input_path, onehot_path)\n",
    "                    .replace(encoder_name, \"onehot\")\n",
    "                    .replace(flatten_emb, onehot_flatten_emb_name),\n",
    "                    task,\n",
    "                    dataset,\n",
    "                    split,\n",
    "                    \"onehot\",\n",
    "                    onehot_flatten_emb_name,\n",
    "                )\n",
    "                add_onehot = True\n",
    "            else:\n",
    "                add_onehot = False\n",
    "\n",
    "        # combine different model into one big plot with different encoders\n",
    "        collage_folder = os.path.join(self._output_path, \"collage\")\n",
    "        checkNgen_folder(collage_folder)\n",
    "\n",
    "        for collage_name, encoder_dict in self._layer_analysis_dict.items():\n",
    "\n",
    "            print(f\"Plotting collage_name {collage_name}...\")\n",
    "\n",
    "            onehot_name = \"_\".join(collage_name.split(\"_\")[:-1])\n",
    "\n",
    "            if set(list(TRANSFORMER_INFO.keys())) == set(encoder_dict.keys()):\n",
    "                # set the key rankings to default\n",
    "                encoder_names = list(TRANSFORMER_INFO.keys())\n",
    "                encoder_label = \"esm\"\n",
    "            elif set(list(CARP_INFO.keys())) == set(encoder_dict.keys()):\n",
    "                # set the key rankings to default\n",
    "                encoder_names = list(CARP_INFO.keys())\n",
    "                encoder_label = \"carp\"\n",
    "            else:\n",
    "                encoder_names = list(set(encoder_dict.keys()))\n",
    "                encoder_label = \"pretrained\"\n",
    "\n",
    "            fig, axs = plt.subplots(\n",
    "                self._metric_numb[collage_name],\n",
    "                len(encoder_names),\n",
    "                sharey=\"row\",\n",
    "                sharex=\"col\",\n",
    "                figsize=(20, 2 * self._metric_numb[collage_name]),\n",
    "                squeeze=False # not get rid off the extra dim if 1D\n",
    "            )\n",
    "\n",
    "            for m, metric in enumerate(self._metric_dict[collage_name.split(\"_\")[0]]):\n",
    "\n",
    "                for n, encoder_name in enumerate(encoder_names):\n",
    "                    axs[m, n].plot(\n",
    "                        encoder_dict[encoder_name][metric],\n",
    "                        label=encoder_label,\n",
    "                        color=\"#f79646ff\",  # orange\n",
    "                    )\n",
    "\n",
    "                    # add checkpoints\n",
    "                    if self._add_checkpoint:\n",
    "                        for checkpoint in self._checkpoint_list:\n",
    "\n",
    "                            checkpoint_vals = self._checkpoint_analysis_dict[\n",
    "                                checkpoint\n",
    "                            ][collage_name][encoder_name][metric]\n",
    "\n",
    "                            if not np.all(checkpoint_vals == 0):\n",
    "                                axs[m, n].plot(\n",
    "                                    checkpoint_vals,\n",
    "                                    label=f\"{encoder_label}-{checkpoint}\",\n",
    "                                    color=CHECKPOINT_COLOR[\n",
    "                                        checkpoint\n",
    "                                    ],  # darker oranges\n",
    "                                    linestyle=\"dashed\",\n",
    "                                )\n",
    "\n",
    "                    # overlay random init\n",
    "                    if add_rand:\n",
    "                        axs[m, n].plot(\n",
    "                            self._rand_layer_analysis_dict[collage_name][encoder_name][\n",
    "                                metric\n",
    "                            ],\n",
    "                            label=\"random init\",\n",
    "                            color=\"#4bacc6\",  # blue\n",
    "                            linestyle=\"dashed\"\n",
    "                            # color=\"#D3D3D3\",  # light grey\n",
    "                        )\n",
    "\n",
    "                    # overlay stat init\n",
    "                    if add_stat:\n",
    "                        axs[m, n].plot(\n",
    "                            self._stat_layer_analysis_dict[collage_name][encoder_name][\n",
    "                                metric\n",
    "                            ],\n",
    "                            label=\"stat transfer\",\n",
    "                            color=\"#9bbb59\",  # green\n",
    "                            linestyle=\"dashed\"\n",
    "                            # color=\"#A9A9A9\",  # dark grey\n",
    "                            # linestyle=\"dotted\",\n",
    "                        )\n",
    "\n",
    "                    # overlay onehot baseline\n",
    "                    if add_onehot:\n",
    "                        axs[m, n].axhline(\n",
    "                            self._onehot_baseline_dict[onehot_name][metric],\n",
    "                            label=\"onehot\",\n",
    "                            color=\"#000000\",  # black or #D3D3D3 light grey\n",
    "                            linestyle=\"dotted\",\n",
    "                        )\n",
    "\n",
    "            # add xlabels\n",
    "            for ax in axs[self._metric_numb[collage_name] - 1]:\n",
    "                ax.set_xlabel(\"layers\", fontsize=16)\n",
    "                ax.tick_params(axis=\"x\", labelsize=16)\n",
    "\n",
    "            # add column names\n",
    "            for ax, col in zip(axs[0], encoder_names):\n",
    "                ax.set_title(col, fontsize=16)\n",
    "\n",
    "            # add row names\n",
    "            for ax, row in zip(\n",
    "                axs[:, 0], self._metric_dict[collage_name.split(\"_\")[0]]\n",
    "            ):\n",
    "                ax.set_ylabel(\n",
    "                    row.replace(\"_\", \" \").replace(\"cross-entropy\", \"ce\"), fontsize=16\n",
    "                )\n",
    "                ax.tick_params(\n",
    "                    axis=\"y\",\n",
    "                    which=\"major\",\n",
    "                    reset=True,\n",
    "                    labelsize=16,\n",
    "                    left=True,\n",
    "                    right=False,  # no right side tick on the plot\n",
    "                    labelleft=True,\n",
    "                    labelright=False,\n",
    "                )\n",
    "                ax.relim()  # make sure all the data fits\n",
    "                ax.autoscale()\n",
    "\n",
    "            # set the plot yticks\n",
    "            plt.gca().yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n",
    "            plt.gca().yaxis.tick_left()\n",
    "            plt.gca().autoscale()\n",
    "\n",
    "            # add legend\n",
    "            handles, labels = axs[0, 0].get_legend_handles_labels()\n",
    "\n",
    "            if len(labels) == 7:\n",
    "\n",
    "                # Add two empty dummy legend items\n",
    "                # using the first label info\n",
    "\n",
    "                axs[0, 0].axhline(\n",
    "                    self._onehot_baseline_dict[onehot_name][\n",
    "                        self._metric_dict[collage_name.split(\"_\")[0]][0]\n",
    "                    ],\n",
    "                    label=\" \",\n",
    "                    color=\"w\",\n",
    "                    alpha=0,\n",
    "                )\n",
    "\n",
    "                axs[0, 0].axhline(\n",
    "                    self._onehot_baseline_dict[onehot_name][\n",
    "                        self._metric_dict[collage_name.split(\"_\")[0]][0]\n",
    "                    ],\n",
    "                    label=\" \",\n",
    "                    color=\"w\",\n",
    "                    alpha=0,\n",
    "                )\n",
    "\n",
    "                adjusted_handles, adjusted_labels = axs[\n",
    "                    0, 0\n",
    "                ].get_legend_handles_labels()\n",
    "                adjusted_y = 1.045\n",
    "                ncol = 3\n",
    "                legend_params = {\n",
    "                    \"labelspacing\": 0.1,  # vertical space between the legend entries, default 0.5\n",
    "                    \"handletextpad\": 0.2,  # space between the legend the text, default 0.8\n",
    "                    \"handlelength\": 0.95,  # length of the legend handles, default 2.0\n",
    "                    \"columnspacing\": 1,  # spacing between columns, default 2.0\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                adjusted_handles, adjusted_labels = handles, labels\n",
    "                adjusted_y = 1.025\n",
    "                ncol = 2\n",
    "                legend_params = {}\n",
    "\n",
    "            fig.legend(\n",
    "                adjusted_handles,\n",
    "                adjusted_labels,\n",
    "                loc=\"upper left\",\n",
    "                bbox_to_anchor=[0.05, adjusted_y],\n",
    "                fontsize=16,\n",
    "                frameon=False,\n",
    "                ncol=ncol,\n",
    "                **legend_params,\n",
    "            )\n",
    "\n",
    "            # add whole plot level title\n",
    "            fig.suptitle(\n",
    "                collage_name.replace(\"_\", \" \").replace(\"cross-entropy\", \"ce\"),\n",
    "                y=1.0025,\n",
    "                fontsize=24,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            fig.align_labels()\n",
    "            fig.tight_layout()\n",
    "\n",
    "            for plot_ext in [\".svg\", \".png\"]:\n",
    "                plt.savefig(\n",
    "                    os.path.join(collage_folder, collage_name + plot_ext),\n",
    "                    bbox_inches=\"tight\",\n",
    "                )\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    def parse_result_dicts(\n",
    "        self,\n",
    "        folder_path: str,\n",
    "        task: str,\n",
    "        dataset: str,\n",
    "        split: str,\n",
    "        encoder_name: str,\n",
    "        flatten_emb: bool | str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parse the output result dictionaries for plotting\n",
    "\n",
    "        Args:\n",
    "        - folder_path: str, the folder path for the datasets\n",
    "\n",
    "        Returns:\n",
    "        - dict, encode name as key with a dict as its value\n",
    "            where metric name as keys and the array of losses as values\n",
    "        - str, details for collage plot\n",
    "        \"\"\"\n",
    "\n",
    "        # get the list of output pickle files\n",
    "        pkl_list = glob(f\"{folder_path}/*.pkl\")\n",
    "\n",
    "        _, _, max_layer_numb = get_emb_info(encoder_name)\n",
    "\n",
    "        # init the ouput dict\n",
    "        output_numb_dict = {\n",
    "            metric: np.zeros([max_layer_numb]) for metric in self._metric_dict[task]\n",
    "        }\n",
    "\n",
    "        # loop through the list of the pickle files\n",
    "        for pkl_file in pkl_list:\n",
    "            # get the layer number\n",
    "            layer_numb = int(get_filename(pkl_file).split(\"-\")[-1].split(\"_\")[-1])\n",
    "            # load the result dictionary\n",
    "            try:\n",
    "                result_dict = pickle_load(pkl_file)\n",
    "            except Exception as e:\n",
    "                print(f\"{pkl_file} with err: \", e)\n",
    "\n",
    "            # populate the processed dictionary\n",
    "            for metric in self._metric_dict[task]:\n",
    "                subset, kind = metric.split(\"_\")\n",
    "                if kind == \"rho\":\n",
    "                    output_numb_dict[metric][layer_numb] = result_dict[subset][kind][0]\n",
    "                else:\n",
    "                    output_numb_dict[metric][layer_numb] = result_dict[subset][kind]\n",
    "\n",
    "        # get some details for plotting and saving\n",
    "        output_subfolder = checkNgen_folder(\n",
    "            folder_path.replace(self._input_path, self._output_path)\n",
    "        )\n",
    "\n",
    "        for metric in output_numb_dict.keys():\n",
    "\n",
    "            plot_name = f\"{encoder_name}_{flatten_emb}_{metric}\"\n",
    "            plot_prefix = f\"{task}_{dataset}_{split}\"\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(output_numb_dict[metric])\n",
    "            plt.title(f\"{plot_prefix} \\n {plot_name}\")\n",
    "            plt.xlabel(\"layers\")\n",
    "            plt.ylabel(\"loss\")\n",
    "\n",
    "            for plot_ext in [\".svg\", \".png\"]:\n",
    "                plt.savefig(\n",
    "                    os.path.join(output_subfolder, plot_name + plot_ext),\n",
    "                    bbox_inches=\"tight\",\n",
    "                )\n",
    "            plt.close()\n",
    "\n",
    "        return output_numb_dict\n",
    "\n",
    "    @property\n",
    "    def layer_analysis_dict(self) -> dict:\n",
    "        \"\"\"Return a dict with dataset name as the key\"\"\"\n",
    "        return self._layer_analysis_dict\n",
    "\n",
    "    @property\n",
    "    def rand_layer_analysis_dict(self) -> dict:\n",
    "        \"\"\"Return a dict with dataset name as the key for rand\"\"\"\n",
    "        return self._rand_layer_analysis_dict\n",
    "\n",
    "    @property\n",
    "    def stat_layer_analysis_dict(self) -> dict:\n",
    "        \"\"\"Return a dict with dataset name as the key for stat\"\"\"\n",
    "        return self._stat_layer_analysis_dict\n",
    "\n",
    "    @property\n",
    "    def onehot_baseline_dict(self) -> dict:\n",
    "        \"\"\"Return a dict with dataset name as the key for onehot\"\"\"\n",
    "        return self._onehot_baseline_dict\n",
    "\n",
    "    @property\n",
    "    def checkpoint_analysis_dict(self) -> dict:\n",
    "        \"\"\"Return a dict with dataset name as the key for checkpoints\"\"\"\n",
    "        return self._checkpoint_analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting collage_name annotation_scl_balanced_mean...\n",
      "Plotting collage_name structure_ss3_tape_processed_noflatten...\n"
     ]
    }
   ],
   "source": [
    "pytorch_carp = LayerLoss(\n",
    "    input_path=\"results/pytorch-carp\",\n",
    "    output_path=\"results/pytorch-carp_layer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'annotation_scl_balanced': {'train_cross-entropy': array([0.35247527]),\n",
       "              'val_cross-entropy': array([1.37385221]),\n",
       "              'test_cross-entropy': array([2.01599532]),\n",
       "              'test_acc': array([0.37402597]),\n",
       "              'test_rocauc': array([0.66190611])},\n",
       "             'structure_ss3_tape_processed': {'train_cross-entropy': array([1.00965377]),\n",
       "              'val_cross-entropy': array([1.00609217]),\n",
       "              'casp12_acc': array([0.48194598]),\n",
       "              'casp12_rocauc': array([0.62021253]),\n",
       "              'cb513_acc': array([0.48816847]),\n",
       "              'cb513_rocauc': array([0.64051902]),\n",
       "              'ts115_acc': array([0.50855104]),\n",
       "              'ts115_rocauc': array([0.64197196])}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_carp.onehot_baseline_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_scl_balanced_mean.carp_600k.train_cross-entropy</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_600k.val_cross-entropy</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_600k.test_cross-entropy</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_600k.test_acc</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_600k.test_rocauc</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_76M.train_cross-entropy</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_76M.val_cross-entropy</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_76M.test_cross-entropy</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_76M.test_acc</th>\n",
       "      <th>annotation_scl_balanced_mean.carp_76M.test_rocauc</th>\n",
       "      <th>...</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_38M.ts115_acc</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_38M.ts115_rocauc</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.train_cross-entropy</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.val_cross-entropy</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.casp12_acc</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.casp12_rocauc</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.cb513_acc</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.cb513_rocauc</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.ts115_acc</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten.carp_600k.ts115_rocauc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.8769907888613249, 1.7527372648841457, 1.695...</td>\n",
       "      <td>[1.9061075278690882, 1.7826685564858573, 1.727...</td>\n",
       "      <td>[1.9521732330322266, 1.9269767999649048, 1.910...</td>\n",
       "      <td>[0.2623376623376623, 0.2753246753246753, 0.316...</td>\n",
       "      <td>[0.6495972923926321, 0.6703754074198381, 0.673...</td>\n",
       "      <td>[1.71919468202089, 1.4959022904697217, 1.44056...</td>\n",
       "      <td>[1.7575252737317766, 1.534557512828282, 1.4600...</td>\n",
       "      <td>[2.0076744556427, 2.0575806498527527, 1.995046...</td>\n",
       "      <td>[0.3038961038961039, 0.35324675324675325, 0.35...</td>\n",
       "      <td>[0.6315851180553764, 0.7013079524965977, 0.710...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.46471855642337734, 0.6232830595206033, 0.69...</td>\n",
       "      <td>[0.5852224245003864, 0.7806285091979067, 0.844...</td>\n",
       "      <td>[1.0358617109795139, 0.89385723659437, 0.81611...</td>\n",
       "      <td>[1.0326182277579057, 0.8892227097561485, 0.810...</td>\n",
       "      <td>[0.45355567805953695, 0.5525082690187431, 0.57...</td>\n",
       "      <td>[0.5753563391975582, 0.6990188518563044, 0.746...</td>\n",
       "      <td>[0.44878082571349404, 0.5898309781102798, 0.64...</td>\n",
       "      <td>[0.593458300550087, 0.7515188960364423, 0.8002...</td>\n",
       "      <td>[0.4690614058712631, 0.5999192028009696, 0.655...</td>\n",
       "      <td>[0.5998865729309671, 0.7559258255636626, 0.809...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_scl_balanced_mean.carp_600k.train_cross-entropy  \\\n",
       "0  [1.8769907888613249, 1.7527372648841457, 1.695...           \n",
       "\n",
       "  annotation_scl_balanced_mean.carp_600k.val_cross-entropy  \\\n",
       "0  [1.9061075278690882, 1.7826685564858573, 1.727...         \n",
       "\n",
       "  annotation_scl_balanced_mean.carp_600k.test_cross-entropy  \\\n",
       "0  [1.9521732330322266, 1.9269767999649048, 1.910...          \n",
       "\n",
       "     annotation_scl_balanced_mean.carp_600k.test_acc  \\\n",
       "0  [0.2623376623376623, 0.2753246753246753, 0.316...   \n",
       "\n",
       "  annotation_scl_balanced_mean.carp_600k.test_rocauc  \\\n",
       "0  [0.6495972923926321, 0.6703754074198381, 0.673...   \n",
       "\n",
       "  annotation_scl_balanced_mean.carp_76M.train_cross-entropy  \\\n",
       "0  [1.71919468202089, 1.4959022904697217, 1.44056...          \n",
       "\n",
       "  annotation_scl_balanced_mean.carp_76M.val_cross-entropy  \\\n",
       "0  [1.7575252737317766, 1.534557512828282, 1.4600...        \n",
       "\n",
       "  annotation_scl_balanced_mean.carp_76M.test_cross-entropy  \\\n",
       "0  [2.0076744556427, 2.0575806498527527, 1.995046...         \n",
       "\n",
       "      annotation_scl_balanced_mean.carp_76M.test_acc  \\\n",
       "0  [0.3038961038961039, 0.35324675324675325, 0.35...   \n",
       "\n",
       "   annotation_scl_balanced_mean.carp_76M.test_rocauc  ...  \\\n",
       "0  [0.6315851180553764, 0.7013079524965977, 0.710...  ...   \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_38M.ts115_acc  \\\n",
       "0  [0.46471855642337734, 0.6232830595206033, 0.69...          \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_38M.ts115_rocauc  \\\n",
       "0  [0.5852224245003864, 0.7806285091979067, 0.844...             \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.train_cross-entropy  \\\n",
       "0  [1.0358617109795139, 0.89385723659437, 0.81611...                     \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.val_cross-entropy  \\\n",
       "0  [1.0326182277579057, 0.8892227097561485, 0.810...                   \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.casp12_acc  \\\n",
       "0  [0.45355567805953695, 0.5525082690187431, 0.57...            \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.casp12_rocauc  \\\n",
       "0  [0.5753563391975582, 0.6990188518563044, 0.746...               \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.cb513_acc  \\\n",
       "0  [0.44878082571349404, 0.5898309781102798, 0.64...           \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.cb513_rocauc  \\\n",
       "0  [0.593458300550087, 0.7515188960364423, 0.8002...              \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.ts115_acc  \\\n",
       "0  [0.4690614058712631, 0.5999192028009696, 0.655...           \n",
       "\n",
       "  structure_ss3_tape_processed_noflatten.carp_600k.ts115_rocauc  \n",
       "0  [0.5998865729309671, 0.7559258255636626, 0.809...             \n",
       "\n",
       "[1 rows x 52 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_normalize(pytorch_carp.layer_analysis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.87699079, 1.75273726, 1.69514218, 1.60551146, 1.53652087,\n",
       "       1.46996107, 1.40448275, 1.33026753, 1.30898465, 1.26282578,\n",
       "       1.26654993, 1.21502749, 1.19079669, 1.183124  , 1.19900847,\n",
       "       1.17011334, 1.18096242])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_carp.layer_analysis_dict[\"annotation_scl_balanced_mean\"][\"carp_600k\"][\"train_cross-entropy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_scl_balanced</th>\n",
       "      <th>structure_ss3_tape_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_cross-entropy</th>\n",
       "      <td>[0.3524752729817441]</td>\n",
       "      <td>[1.0096537658613023]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_cross-entropy</th>\n",
       "      <td>[1.373852210385459]</td>\n",
       "      <td>[1.0060921719199734]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_cross-entropy</th>\n",
       "      <td>[2.0159953236579895]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_acc</th>\n",
       "      <td>[0.37402597402597404]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_rocauc</th>\n",
       "      <td>[0.6619061060529179]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casp12_acc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.48194597574421166]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casp12_rocauc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.6202125329637339]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb513_acc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.48816846771958994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb513_rocauc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.6405190154371231]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts115_acc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.5085510368973876]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts115_rocauc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.6419719552524553]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    annotation_scl_balanced structure_ss3_tape_processed\n",
       "train_cross-entropy    [0.3524752729817441]         [1.0096537658613023]\n",
       "val_cross-entropy       [1.373852210385459]         [1.0060921719199734]\n",
       "test_cross-entropy     [2.0159953236579895]                          NaN\n",
       "test_acc              [0.37402597402597404]                          NaN\n",
       "test_rocauc            [0.6619061060529179]                          NaN\n",
       "casp12_acc                              NaN        [0.48194597574421166]\n",
       "casp12_rocauc                           NaN         [0.6202125329637339]\n",
       "cb513_acc                               NaN        [0.48816846771958994]\n",
       "cb513_rocauc                            NaN         [0.6405190154371231]\n",
       "ts115_acc                               NaN         [0.5085510368973876]\n",
       "ts115_rocauc                            NaN         [0.6419719552524553]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(pytorch_carp.onehot_baseline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proeng_aav_one_vs_many</th>\n",
       "      <th>proeng_aav_two_vs_many</th>\n",
       "      <th>proeng_gb1_low_vs_high</th>\n",
       "      <th>proeng_gb1_two_vs_rest</th>\n",
       "      <th>proeng_gb1_sampled</th>\n",
       "      <th>proeng_thermo_mixed_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_mse</th>\n",
       "      <td>[6.606138986229867]</td>\n",
       "      <td>[16.67168599756062]</td>\n",
       "      <td>[0.1770679055989013]</td>\n",
       "      <td>[2.090415559759884]</td>\n",
       "      <td>[2.0079257240201596]</td>\n",
       "      <td>[2760.1282354723244]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_mse</th>\n",
       "      <td>[8.361448368900476]</td>\n",
       "      <td>[16.951583715058266]</td>\n",
       "      <td>[0.17003734650743707]</td>\n",
       "      <td>[2.0447116896087842]</td>\n",
       "      <td>[1.9726564028720246]</td>\n",
       "      <td>[2778.184268114801]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_mse</th>\n",
       "      <td>[66.71775739461496]</td>\n",
       "      <td>[24.814891839771693]</td>\n",
       "      <td>[4.028476326030474]</td>\n",
       "      <td>[5.335454660039244]</td>\n",
       "      <td>[2.025209681370326]</td>\n",
       "      <td>[2838.811461721218]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_ndcg</th>\n",
       "      <td>[0.960145115831988]</td>\n",
       "      <td>[0.9629290404042454]</td>\n",
       "      <td>[0.9178409557926928]</td>\n",
       "      <td>[0.8905737288756033]</td>\n",
       "      <td>[0.9342600571731149]</td>\n",
       "      <td>[0.9554623184824335]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_rho</th>\n",
       "      <td>[0.1903477426748507]</td>\n",
       "      <td>[-0.0015626678198874598]</td>\n",
       "      <td>[0.32173083298452543]</td>\n",
       "      <td>[0.5428396414906184]</td>\n",
       "      <td>[0.7885047525700145]</td>\n",
       "      <td>[0.1226657473529901]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          proeng_aav_one_vs_many    proeng_aav_two_vs_many  \\\n",
       "train_mse    [6.606138986229867]       [16.67168599756062]   \n",
       "val_mse      [8.361448368900476]      [16.951583715058266]   \n",
       "test_mse     [66.71775739461496]      [24.814891839771693]   \n",
       "test_ndcg    [0.960145115831988]      [0.9629290404042454]   \n",
       "test_rho    [0.1903477426748507]  [-0.0015626678198874598]   \n",
       "\n",
       "          proeng_gb1_low_vs_high proeng_gb1_two_vs_rest    proeng_gb1_sampled  \\\n",
       "train_mse   [0.1770679055989013]    [2.090415559759884]  [2.0079257240201596]   \n",
       "val_mse    [0.17003734650743707]   [2.0447116896087842]  [1.9726564028720246]   \n",
       "test_mse     [4.028476326030474]    [5.335454660039244]   [2.025209681370326]   \n",
       "test_ndcg   [0.9178409557926928]   [0.8905737288756033]  [0.9342600571731149]   \n",
       "test_rho   [0.32173083298452543]   [0.5428396414906184]  [0.7885047525700145]   \n",
       "\n",
       "          proeng_thermo_mixed_split  \n",
       "train_mse      [2760.1282354723244]  \n",
       "val_mse         [2778.184268114801]  \n",
       "test_mse        [2838.811461721218]  \n",
       "test_ndcg      [0.9554623184824335]  \n",
       "test_rho       [0.1226657473529901]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(sklearn_carp.onehot_baseline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_scl_balanced_mean</th>\n",
       "      <th>structure_ss3_tape_processed_noflatten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>carp_600k</th>\n",
       "      <td>{'train_cross-entropy': [1.8769907888613249, 1...</td>\n",
       "      <td>{'train_cross-entropy': [1.0358617109795139, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carp_76M</th>\n",
       "      <td>{'train_cross-entropy': [1.71919468202089, 1.4...</td>\n",
       "      <td>{'train_cross-entropy': [1.0396245995612994, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carp_38M</th>\n",
       "      <td>{'train_cross-entropy': [1.7385239789360447, 1...</td>\n",
       "      <td>{'train_cross-entropy': [1.0429124097301536, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carp_640M</th>\n",
       "      <td>{'train_cross-entropy': [1.7193063999477185, 1...</td>\n",
       "      <td>{'train_cross-entropy': [1.0420560608171436, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                annotation_scl_balanced_mean  \\\n",
       "carp_600k  {'train_cross-entropy': [1.8769907888613249, 1...   \n",
       "carp_76M   {'train_cross-entropy': [1.71919468202089, 1.4...   \n",
       "carp_38M   {'train_cross-entropy': [1.7385239789360447, 1...   \n",
       "carp_640M  {'train_cross-entropy': [1.7193063999477185, 1...   \n",
       "\n",
       "                      structure_ss3_tape_processed_noflatten  \n",
       "carp_600k  {'train_cross-entropy': [1.0358617109795139, 0...  \n",
       "carp_76M   {'train_cross-entropy': [1.0396245995612994, 0...  \n",
       "carp_38M   {'train_cross-entropy': [1.0429124097301536, 0...  \n",
       "carp_640M  {'train_cross-entropy': [1.0420560608171436, 0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(pytorch_carp.layer_analysis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_cross-entropy</th>\n",
       "      <th>val_cross-entropy</th>\n",
       "      <th>test_cross-entropy</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_rocauc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.8769907888613249, 1.7527372648841457, 1.695...</td>\n",
       "      <td>[1.9061075278690882, 1.7826685564858573, 1.727...</td>\n",
       "      <td>[1.9521732330322266, 1.9269767999649048, 1.910...</td>\n",
       "      <td>[0.2623376623376623, 0.2753246753246753, 0.316...</td>\n",
       "      <td>[0.6495972923926321, 0.6703754074198381, 0.673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.71919468202089, 1.4959022904697217, 1.44056...</td>\n",
       "      <td>[1.7575252737317766, 1.534557512828282, 1.4600...</td>\n",
       "      <td>[2.0076744556427, 2.0575806498527527, 1.995046...</td>\n",
       "      <td>[0.3038961038961039, 0.35324675324675325, 0.35...</td>\n",
       "      <td>[0.6315851180553764, 0.7013079524965977, 0.710...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.7385239789360447, 1.4665500490288985, 1.395...</td>\n",
       "      <td>[1.7737009525299072, 1.5041547673089164, 1.417...</td>\n",
       "      <td>[2.035592496395111, 2.0621028542518616, 2.0096...</td>\n",
       "      <td>[0.2857142857142857, 0.36883116883116884, 0.37...</td>\n",
       "      <td>[0.6189812048918136, 0.7025105662414319, 0.713...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.7193063999477185, 1.5111642197558754, 1.437...</td>\n",
       "      <td>[1.7644925117492676, 1.5379446830068315, 1.462...</td>\n",
       "      <td>[2.0392618775367737, 2.05305552482605, 2.01585...</td>\n",
       "      <td>[0.2961038961038961, 0.34805194805194806, 0.36...</td>\n",
       "      <td>[0.6273352139525914, 0.7005424935091661, 0.706...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 train_cross-entropy  \\\n",
       "0  [1.8769907888613249, 1.7527372648841457, 1.695...   \n",
       "1  [1.71919468202089, 1.4959022904697217, 1.44056...   \n",
       "2  [1.7385239789360447, 1.4665500490288985, 1.395...   \n",
       "3  [1.7193063999477185, 1.5111642197558754, 1.437...   \n",
       "\n",
       "                                   val_cross-entropy  \\\n",
       "0  [1.9061075278690882, 1.7826685564858573, 1.727...   \n",
       "1  [1.7575252737317766, 1.534557512828282, 1.4600...   \n",
       "2  [1.7737009525299072, 1.5041547673089164, 1.417...   \n",
       "3  [1.7644925117492676, 1.5379446830068315, 1.462...   \n",
       "\n",
       "                                  test_cross-entropy  \\\n",
       "0  [1.9521732330322266, 1.9269767999649048, 1.910...   \n",
       "1  [2.0076744556427, 2.0575806498527527, 1.995046...   \n",
       "2  [2.035592496395111, 2.0621028542518616, 2.0096...   \n",
       "3  [2.0392618775367737, 2.05305552482605, 2.01585...   \n",
       "\n",
       "                                            test_acc  \\\n",
       "0  [0.2623376623376623, 0.2753246753246753, 0.316...   \n",
       "1  [0.3038961038961039, 0.35324675324675325, 0.35...   \n",
       "2  [0.2857142857142857, 0.36883116883116884, 0.37...   \n",
       "3  [0.2961038961038961, 0.34805194805194806, 0.36...   \n",
       "\n",
       "                                         test_rocauc  \n",
       "0  [0.6495972923926321, 0.6703754074198381, 0.673...  \n",
       "1  [0.6315851180553764, 0.7013079524965977, 0.710...  \n",
       "2  [0.6189812048918136, 0.7025105662414319, 0.713...  \n",
       "3  [0.6273352139525914, 0.7005424935091661, 0.706...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_normalize(pd.DataFrame.from_dict(pytorch_carp.layer_analysis_dict)['annotation_scl_balanced_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(pd.DataFrame.from_dict(pytorch_carp.layer_analysis_dict)['annotation_scl_balanced_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting collage_name proeng_aav_one_vs_many_mean...\n",
      "Plotting collage_name proeng_aav_two_vs_many_mean...\n",
      "Plotting collage_name proeng_gb1_low_vs_high_mean...\n",
      "Plotting collage_name proeng_gb1_two_vs_rest_mean...\n",
      "Plotting collage_name proeng_gb1_sampled_mean...\n",
      "Plotting collage_name proeng_thermo_mixed_split_mean...\n"
     ]
    }
   ],
   "source": [
    "sklearn_carp = LayerLoss(\n",
    "    input_path=\"results/sklearn-carp\",\n",
    "    output_path=\"results/sklearn-carp_layer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mse</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_ndcg</th>\n",
       "      <th>test_rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[9.59526940116963, 6.894266263718891, 6.377686...</td>\n",
       "      <td>[11.18654191999164, 8.622644328345816, 8.33617...</td>\n",
       "      <td>[13.582533778622887, 53.712009437112215, 74.04...</td>\n",
       "      <td>[0.9704321391244177, 0.9629028051372428, 0.962...</td>\n",
       "      <td>[0.19544242309285326, 0.3282696048440418, 0.31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[9.582026132905252, 6.934608975116892, 6.40550...</td>\n",
       "      <td>[11.20643614220332, 8.899376014238195, 8.20574...</td>\n",
       "      <td>[13.697288684643041, 48.961780681241656, 69.96...</td>\n",
       "      <td>[0.9703413636783854, 0.9633589617754629, 0.962...</td>\n",
       "      <td>[0.2116112794657769, 0.3396909252934983, 0.320...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[9.468360495727067, 8.190111410830674, 7.61644...</td>\n",
       "      <td>[10.9554842436175, 9.782055506244191, 9.209345...</td>\n",
       "      <td>[14.20679802985108, 24.53599377907496, 23.3161...</td>\n",
       "      <td>[0.9698271299324172, 0.96692411292824, 0.96943...</td>\n",
       "      <td>[0.23682032708767906, 0.3808350241317865, 0.43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9.574777226192223, 6.690699098364959, 6.35389...</td>\n",
       "      <td>[11.175331793579538, 8.655740728556914, 8.5822...</td>\n",
       "      <td>[13.968634660845268, 65.68949789195031, 78.864...</td>\n",
       "      <td>[0.969236054842432, 0.9626272649406287, 0.9625...</td>\n",
       "      <td>[0.18915018144628126, 0.3242017861013232, 0.31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           train_mse  \\\n",
       "0  [9.59526940116963, 6.894266263718891, 6.377686...   \n",
       "1  [9.582026132905252, 6.934608975116892, 6.40550...   \n",
       "2  [9.468360495727067, 8.190111410830674, 7.61644...   \n",
       "3  [9.574777226192223, 6.690699098364959, 6.35389...   \n",
       "\n",
       "                                             val_mse  \\\n",
       "0  [11.18654191999164, 8.622644328345816, 8.33617...   \n",
       "1  [11.20643614220332, 8.899376014238195, 8.20574...   \n",
       "2  [10.9554842436175, 9.782055506244191, 9.209345...   \n",
       "3  [11.175331793579538, 8.655740728556914, 8.5822...   \n",
       "\n",
       "                                            test_mse  \\\n",
       "0  [13.582533778622887, 53.712009437112215, 74.04...   \n",
       "1  [13.697288684643041, 48.961780681241656, 69.96...   \n",
       "2  [14.20679802985108, 24.53599377907496, 23.3161...   \n",
       "3  [13.968634660845268, 65.68949789195031, 78.864...   \n",
       "\n",
       "                                           test_ndcg  \\\n",
       "0  [0.9704321391244177, 0.9629028051372428, 0.962...   \n",
       "1  [0.9703413636783854, 0.9633589617754629, 0.962...   \n",
       "2  [0.9698271299324172, 0.96692411292824, 0.96943...   \n",
       "3  [0.969236054842432, 0.9626272649406287, 0.9625...   \n",
       "\n",
       "                                            test_rho  \n",
       "0  [0.19544242309285326, 0.3282696048440418, 0.31...  \n",
       "1  [0.2116112794657769, 0.3396909252934983, 0.320...  \n",
       "2  [0.23682032708767906, 0.3808350241317865, 0.43...  \n",
       "3  [0.18915018144628126, 0.3242017861013232, 0.31...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_normalize(pd.DataFrame.from_dict(sklearn_carp.layer_analysis_dict)['proeng_aav_one_vs_many_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proeng_aav_one_vs_many_mean</th>\n",
       "      <th>proeng_aav_two_vs_many_mean</th>\n",
       "      <th>proeng_gb1_low_vs_high_mean</th>\n",
       "      <th>proeng_gb1_two_vs_rest_mean</th>\n",
       "      <th>proeng_gb1_sampled_mean</th>\n",
       "      <th>proeng_thermo_mixed_split_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>carp_640M</th>\n",
       "      <td>{'train_mse': [9.59526940116963, 6.89426626371...</td>\n",
       "      <td>{'train_mse': [20.068343175878088, 16.92011420...</td>\n",
       "      <td>{'train_mse': [0.2772588428285639, 0.408520416...</td>\n",
       "      <td>{'train_mse': [2.725838148323684, 1.9707604225...</td>\n",
       "      <td>{'train_mse': [2.5043217133381526, 1.876857337...</td>\n",
       "      <td>{'train_mse': [2772.749931574079, 2768.7387720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carp_76M</th>\n",
       "      <td>{'train_mse': [9.582026132905252, 6.9346089751...</td>\n",
       "      <td>{'train_mse': [19.986434661076075, 16.91308441...</td>\n",
       "      <td>{'train_mse': [0.2853286156211369, 0.411029697...</td>\n",
       "      <td>{'train_mse': [2.7027404465889115, 1.969533968...</td>\n",
       "      <td>{'train_mse': [2.4793497727220144, 1.871709821...</td>\n",
       "      <td>{'train_mse': [2772.772707139839, 2768.7354513...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carp_600k</th>\n",
       "      <td>{'train_mse': [9.468360495727067, 8.1901114108...</td>\n",
       "      <td>{'train_mse': [19.808120463435134, 17.93898064...</td>\n",
       "      <td>{'train_mse': [0.280335127217654, 0.3869098418...</td>\n",
       "      <td>{'train_mse': [2.674702299445244, 2.1214922598...</td>\n",
       "      <td>{'train_mse': [2.4329909760979573, 2.039620513...</td>\n",
       "      <td>{'train_mse': [2772.624627474502, 2770.3332577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carp_38M</th>\n",
       "      <td>{'train_mse': [9.574777226192223, 6.6906990983...</td>\n",
       "      <td>{'train_mse': [20.04262815315973, 16.802693836...</td>\n",
       "      <td>{'train_mse': [0.27452013074623355, 0.40600096...</td>\n",
       "      <td>{'train_mse': [2.7247057205531227, 1.959347953...</td>\n",
       "      <td>{'train_mse': [2.507311420931766, 1.8454394133...</td>\n",
       "      <td>{'train_mse': [2772.7840732976197, 2768.293532...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 proeng_aav_one_vs_many_mean  \\\n",
       "carp_640M  {'train_mse': [9.59526940116963, 6.89426626371...   \n",
       "carp_76M   {'train_mse': [9.582026132905252, 6.9346089751...   \n",
       "carp_600k  {'train_mse': [9.468360495727067, 8.1901114108...   \n",
       "carp_38M   {'train_mse': [9.574777226192223, 6.6906990983...   \n",
       "\n",
       "                                 proeng_aav_two_vs_many_mean  \\\n",
       "carp_640M  {'train_mse': [20.068343175878088, 16.92011420...   \n",
       "carp_76M   {'train_mse': [19.986434661076075, 16.91308441...   \n",
       "carp_600k  {'train_mse': [19.808120463435134, 17.93898064...   \n",
       "carp_38M   {'train_mse': [20.04262815315973, 16.802693836...   \n",
       "\n",
       "                                 proeng_gb1_low_vs_high_mean  \\\n",
       "carp_640M  {'train_mse': [0.2772588428285639, 0.408520416...   \n",
       "carp_76M   {'train_mse': [0.2853286156211369, 0.411029697...   \n",
       "carp_600k  {'train_mse': [0.280335127217654, 0.3869098418...   \n",
       "carp_38M   {'train_mse': [0.27452013074623355, 0.40600096...   \n",
       "\n",
       "                                 proeng_gb1_two_vs_rest_mean  \\\n",
       "carp_640M  {'train_mse': [2.725838148323684, 1.9707604225...   \n",
       "carp_76M   {'train_mse': [2.7027404465889115, 1.969533968...   \n",
       "carp_600k  {'train_mse': [2.674702299445244, 2.1214922598...   \n",
       "carp_38M   {'train_mse': [2.7247057205531227, 1.959347953...   \n",
       "\n",
       "                                     proeng_gb1_sampled_mean  \\\n",
       "carp_640M  {'train_mse': [2.5043217133381526, 1.876857337...   \n",
       "carp_76M   {'train_mse': [2.4793497727220144, 1.871709821...   \n",
       "carp_600k  {'train_mse': [2.4329909760979573, 2.039620513...   \n",
       "carp_38M   {'train_mse': [2.507311420931766, 1.8454394133...   \n",
       "\n",
       "                              proeng_thermo_mixed_split_mean  \n",
       "carp_640M  {'train_mse': [2772.749931574079, 2768.7387720...  \n",
       "carp_76M   {'train_mse': [2772.772707139839, 2768.7354513...  \n",
       "carp_600k  {'train_mse': [2772.624627474502, 2770.3332577...  \n",
       "carp_38M   {'train_mse': [2772.7840732976197, 2768.293532...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(sklearn_carp.layer_analysis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_carp.layer_analysis_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_carp.layer_analysis_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_carp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_carp.rand_layer_analysis_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_carp.rand_layer_analysis_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_carp_rand.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_carp.stat_layer_analysis_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_carp.stat_layer_analysis_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_carp_stat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_carp.onehot_baseline_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_carp.onehot_baseline_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_carp_oh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(sklearn_carp.onehot_baseline_dict).merge(\n",
    "    pd.DataFrame.from_dict(pytorch_carp.onehot_baseline_dict),\n",
    "    how=\"outer\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").fillna(False).to_csv(\"results/summary/all_carp_oh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# pd.merge(\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#     pd.DataFrame.from_dict(sklearn_carp.checkpoint_analysis_dict),\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#     pd.DataFrame.from_dict(pytorch_carp.checkpoint_analysis_dict),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#     left_index=True,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#     right_index=True,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# ).to_csv(\"results/summary/all_carp_cp.csv\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mfrom_dict(sklearn_carp\u001b[39m.\u001b[39;49mcheckpoint_analysis_dict)\u001b[39m.\u001b[39;49mmerge(\n\u001b[1;32m      9\u001b[0m     pd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mfrom_dict(pytorch_carp\u001b[39m.\u001b[39;49mcheckpoint_analysis_dict), how\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mouter\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m )\u001b[39m.\u001b[39mfillna(\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mresults/summary/all_carp_cp.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/frame.py:9345\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9326\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   9327\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   9328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9341\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   9342\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   9343\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[0;32m-> 9345\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[1;32m   9346\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   9347\u001b[0m         right,\n\u001b[1;32m   9348\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m   9349\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m   9350\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m   9351\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m   9352\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m   9353\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m   9354\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   9355\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m   9356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   9357\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m   9358\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m   9359\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/reshape/merge.py:122\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    107\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    108\u001b[0m         left,\n\u001b[1;32m    109\u001b[0m         right,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m         validate\u001b[39m=\u001b[39mvalidate,\n\u001b[1;32m    121\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/reshape/merge.py:716\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindicator:\n\u001b[1;32m    714\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indicator_pre_merge(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright)\n\u001b[0;32m--> 716\u001b[0m join_index, left_indexer, right_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_info()\n\u001b[1;32m    718\u001b[0m llabels, rlabels \u001b[39m=\u001b[39m _items_overlap_with_suffix(\n\u001b[1;32m    719\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft\u001b[39m.\u001b[39m_info_axis, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright\u001b[39m.\u001b[39m_info_axis, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuffixes\n\u001b[1;32m    720\u001b[0m )\n\u001b[1;32m    722\u001b[0m lindexers \u001b[39m=\u001b[39m {\u001b[39m1\u001b[39m: left_indexer} \u001b[39mif\u001b[39;00m left_indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/reshape/merge.py:967\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    963\u001b[0m     join_index, right_indexer, left_indexer \u001b[39m=\u001b[39m _left_join_on_index(\n\u001b[1;32m    964\u001b[0m         right_ax, left_ax, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort\n\u001b[1;32m    965\u001b[0m     )\n\u001b[1;32m    966\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m     (left_indexer, right_indexer) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_indexers()\n\u001b[1;32m    969\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_index:\n\u001b[1;32m    970\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/reshape/merge.py:941\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_join_indexers\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[1;32m    940\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m get_join_indexers(\n\u001b[1;32m    942\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mleft_join_keys, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mright_join_keys, sort\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msort, how\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhow\n\u001b[1;32m    943\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1484\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[39m# get left & right join labels and num. of levels at each location\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m mapped \u001b[39m=\u001b[39m (\n\u001b[1;32m   1481\u001b[0m     _factorize_keys(left_keys[n], right_keys[n], sort\u001b[39m=\u001b[39msort, how\u001b[39m=\u001b[39mhow)\n\u001b[1;32m   1482\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(left_keys))\n\u001b[1;32m   1483\u001b[0m )\n\u001b[0;32m-> 1484\u001b[0m zipped \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mmapped)\n\u001b[1;32m   1485\u001b[0m llab, rlab, shape \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m zipped)\n\u001b[1;32m   1487\u001b[0m \u001b[39m# get flat i8 keys from label lists\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1481\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(left_keys) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m   1476\u001b[0m     right_keys\n\u001b[1;32m   1477\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mleft_key and right_keys must be the same length\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1479\u001b[0m \u001b[39m# get left & right join labels and num. of levels at each location\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m mapped \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1481\u001b[0m     _factorize_keys(left_keys[n], right_keys[n], sort\u001b[39m=\u001b[39;49msort, how\u001b[39m=\u001b[39;49mhow)\n\u001b[1;32m   1482\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(left_keys))\n\u001b[1;32m   1483\u001b[0m )\n\u001b[1;32m   1484\u001b[0m zipped \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmapped)\n\u001b[1;32m   1485\u001b[0m llab, rlab, shape \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m zipped)\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/core/reshape/merge.py:2183\u001b[0m, in \u001b[0;36m_factorize_keys\u001b[0;34m(lk, rk, sort, how)\u001b[0m\n\u001b[1;32m   2178\u001b[0m rizer \u001b[39m=\u001b[39m klass(\u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(lk), \u001b[39mlen\u001b[39m(rk)))\n\u001b[1;32m   2180\u001b[0m \u001b[39m# Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[39m# \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \u001b[39m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[39;00m\n\u001b[0;32m-> 2183\u001b[0m llab \u001b[39m=\u001b[39m rizer\u001b[39m.\u001b[39;49mfactorize(lk)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m \u001b[39m# Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   2185\u001b[0m \u001b[39m# \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\u001b[39;00m\n\u001b[1;32m   2186\u001b[0m \u001b[39m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[39;00m\n\u001b[1;32m   2187\u001b[0m rlab \u001b[39m=\u001b[39m rizer\u001b[39m.\u001b[39mfactorize(rk)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/protran/lib/python3.9/site-packages/pandas/_libs/hashtable.pyx:124\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.ObjectFactorizer.factorize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5402\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_labels\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5310\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "# pd.merge(\n",
    "#     pd.DataFrame.from_dict(sklearn_carp.checkpoint_analysis_dict),\n",
    "#     pd.DataFrame.from_dict(pytorch_carp.checkpoint_analysis_dict),\n",
    "#     left_index=True,\n",
    "#     right_index=True,\n",
    "# ).to_csv(\"results/summary/all_carp_cp.csv\")\n",
    "\n",
    "pd.DataFrame.from_dict(sklearn_carp.checkpoint_analysis_dict).merge(\n",
    "    pd.DataFrame.from_dict(pytorch_carp.checkpoint_analysis_dict), how=\"outer\"\n",
    ").fillna(False).to_csv(\"results/summary/all_carp_cp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting collage_name structure_ss3_tape_processed_noflatten...\n",
      "Plotting collage_name annotation_scl_balanced_mean...\n"
     ]
    }
   ],
   "source": [
    "pytorch_esm = LayerLoss(\n",
    "    input_path=\"results/pytorch-esm\",\n",
    "    output_path=\"results/pytorch-esm_layer\",\n",
    "    add_checkpoint=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting collage_name proeng_thermo_mixed_split_mean...\n",
      "Plotting collage_name proeng_gb1_two_vs_rest_mean...\n",
      "Plotting collage_name proeng_gb1_low_vs_high_mean...\n",
      "Plotting collage_name proeng_gb1_sampled_mean...\n",
      "Plotting collage_name proeng_aav_one_vs_many_mean...\n",
      "Plotting collage_name proeng_aav_two_vs_many_mean...\n"
     ]
    }
   ],
   "source": [
    "sklearn_esm = LayerLoss(\n",
    "    input_path=\"results/sklearn-esm\",\n",
    "    output_path=\"results/sklearn-esm_layer\",\n",
    "    add_checkpoint=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_esm.layer_analysis_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_esm.layer_analysis_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_esm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_esm.rand_layer_analysis_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_esm.rand_layer_analysis_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_esm_rand.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_esm.stat_layer_analysis_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_esm.stat_layer_analysis_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_esm_stat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.DataFrame.from_dict(sklearn_esm.onehot_baseline_dict),\n",
    "    pd.DataFrame.from_dict(pytorch_esm.onehot_baseline_dict),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").to_csv(\"results/summary/all_esm_oh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerLoss2CSV(LayerLoss):\n",
    "    \"\"\"A class for reorganizing layer by layer results into one csv\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        add_checkpoint: bool = True,\n",
    "        checkpoint_list: list = [0.5, 0.25, 0.125],\n",
    "        input_path: str = \"results/sklearn\",\n",
    "        output_path: str = \"results/sklearn_layer\",\n",
    "        metric_dict: dict[list[str]] = {\n",
    "            \"proeng\": [\"train_mse\", \"val_mse\", \"test_mse\", \"test_ndcg\", \"test_rho\"],\n",
    "            \"annotation\": [\n",
    "                \"train_cross-entropy\",\n",
    "                \"val_cross-entropy\",\n",
    "                \"test_cross-entropy\",\n",
    "                \"test_acc\",\n",
    "                \"test_rocauc\",\n",
    "            ],\n",
    "            \"structure\": [\n",
    "                \"train_cross-entropy\",\n",
    "                \"val_cross-entropy\",\n",
    "                \"casp12_acc\",\n",
    "                \"casp12_rocauc\",\n",
    "                \"cb513_acc\",\n",
    "                \"cb513_rocauc\",\n",
    "                \"ts115_acc\",\n",
    "                \"ts115_rocauc\",\n",
    "            ],\n",
    "        },\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - add_checkpoint: bool = True, if add checkpoint for carp\n",
    "        - checkpoint_list: list = [0.5, 0.25, 0.125],\n",
    "        - input_path: str = \"results/sklearn\",\n",
    "        - output_path: str = \"results/sklearn_layer\"\n",
    "        - metric_dict: list[str] = [\"train_mse\", \"test_ndcg\", \"test_rho\"]\n",
    "        \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protran",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
