from scr.params.sys import DEVICE, RAND_SEED
from scr.params.emb import CARP_INFO, TRANSFORMER_INFO
from scr.model.run_pytorch import Run_Pytorch

"""
TRANSFORMER_INFO = {
    "esm1_t6_43M_UR50S": (768, 6, 2),
    "esm1_t12_85M_UR50S": (768, 12, 2),
    "esm1_t34_670M_UR50S": (1280, 34, 2),
    "esm1b_t33_650M_UR50S": (1280, 33, 2),
}

"""
"""

Run_Pytorch(
    dataset_path="data/annotation/scl/balanced.csv",
    encoder_name="onehot",
    reset_param = False,
    resample_param = False,
    embed_batch_size = 128,
    flatten_emb= "flatten",
    embed_folder= "embeddings/annotation/scl/balanced",
    seq_start_idx= False,
    seq_end_idx = False,
    loader_batch_size = 256,
    worker_seed = RAND_SEED,
    if_encode_all = False,
    if_multiprocess = False,
    learning_rate = 1e-4,
    lr_decay = 0.1,
    epochs = 100,
    early_stop = True,
    tolerance = 10,
    min_epoch = 5,
    device = DEVICE,
    all_plot_folder = "test/learning_curves_fixembpool",
    all_result_folder = "test/pytorch_fixembpool",
    # **encoder_params,
)"""

for encoder_name in CARP_INFO.keys():
    Run_Pytorch(
        dataset_path="data/annotation/scl/balanced.csv",
        encoder_name=encoder_name,
        reset_param = False,
        # reset_param = True,
        resample_param = False,
        # resample_param = True,
        embed_batch_size = 128,
        flatten_emb= "mean",
        # embed_folder= "embeddings-rand/annotation/scl/balanced",
        # embed_folder= "embeddings-stat/annotation/scl/balanced",
        embed_folder = f"/home/t-fli/amlt/carp_emb_cuda_individual/scl-{encoder_name}-mean/embeddings/annotation/scl/balanced",
        seq_start_idx= False,
        seq_end_idx = False,
        loader_batch_size = 256,
        worker_seed = RAND_SEED,
        if_encode_all = False,
        if_multiprocess = True,
        learning_rate = 1e-4,
        lr_decay = 0.1,
        epochs = 100,
        early_stop = True,
        tolerance = 10,
        min_epoch = 5,
        device = DEVICE,
        all_plot_folder = "test/learning_curves_fixembpool",
        all_result_folder = "test/pytorch_fixembpool",
        # **encoder_params,
)

# for encoder_name in TRANSFORMER_INFO.keys():
# for encoder_name in ["esm1_t12_85M_UR50S", "esm1_t34_670M_UR50S", "esm1b_t33_650M_UR50S"]:
"""for encoder_name in ["esm1b_t33_650M_UR50S"]:
    Run_Pytorch(
        dataset_path="data/annotation/scl/balanced.csv",
        encoder_name=encoder_name,
        reset_param = False,
        # reset_param = True,
        # resample_param = False,
        resample_param = True,
        embed_batch_size = 128,
        flatten_emb= "mean",
        # embed_folder= "embeddings-rand/annotation/scl/balanced",
        # embed_folder= "embeddings-stat/annotation/scl/balanced",
        embed_folder = "/home/t-fli/amlt/genemb-fixembmean/scl-genemb-stat/embeddings-stat/annotation/scl/balanced",
        seq_start_idx= False,
        seq_end_idx = False,
        loader_batch_size = 256,
        worker_seed = RAND_SEED,
        if_encode_all = False,
        if_multiprocess = True,
        learning_rate = 1e-4,
        lr_decay = 0.1,
        epochs = 50,
        early_stop = True,
        tolerance = 10,
        min_epoch = 5,
        device = DEVICE,
        all_plot_folder = "test/learning_curves_fixembpool",
        all_result_folder = "test/pytorch_fixembpool",
        # **encoder_params,
)"""

"""Run_Pytorch(
    dataset_path="data/proeng/thermo/mixed_split.csv",
    encoder_name="esm1b_t33_650M_UR50S",
    reset_param = False,
    resample_param = False,
    embed_batch_size = 128,
    flatten_emb= "mean",
    embed_folder= "embeddings/proeng/thermo/mixed_split",
    seq_start_idx= False,
    seq_end_idx = False,
    loader_batch_size = 64,
    worker_seed = RAND_SEED,
    if_encode_all = False,
    if_multiprocess = True,
    learning_rate = 1e-4,
    lr_decay = 0.1,
    epochs = 100,
    early_stop = True,
    tolerance = 10,
    min_epoch = 5,
    device = DEVICE,
    all_plot_folder = "results/learning_curves_fixembpool",
    all_result_folder = "test/pytorch_fixembpool",
    # **encoder_params,
    )"""
"""
Run_Pytorch(
    dataset_path="data/proeng/thermo/mixed_split.csv",
    encoder_name="",
    reset_param = False,
    resample_param = False,
    embed_batch_size = 128,
    flatten_emb= "flatten",
    embed_folder= "embeddings/proeng/thermo/mixed_split",
    seq_start_idx= False,
    seq_end_idx = False,
    loader_batch_size = 64,
    worker_seed = RAND_SEED,
    if_encode_all = False,
    if_multiprocess = True,
    learning_rate = 1e-4,
    lr_decay = 0.1,
    epochs = 50,
    early_stop = True,
    tolerance = 10,
    min_epoch = 5,
    device = DEVICE,
    all_plot_folder = "results/learning_curves_fixembpool",
    all_result_folder = "results/pytorch_fixembpool",
    # **encoder_params,
    )"""

"""
Run_Pytorch(
    dataset_path="data/structure/secondary_structure/casp12.csv",
    encoder_name="",
    reset_param = False,
    resample_param = False,
    embed_batch_size = 0,
    flatten_emb= False,
    embed_folder=None,
    seq_start_idx= False,
    seq_end_idx = False,
    loader_batch_size = 64,
    worker_seed = RAND_SEED,
    if_encode_all = True,
    learning_rate = 1e-4,
    lr_decay = 0.1,
    epochs = 2,
    early_stop = True,
    tolerance = 10,
    min_epoch = 5,
    device = DEVICE,
    all_plot_folder = "test/learning_curves",
    all_result_folder = "test/train_val_test",
    # **encoder_params,
    )"""
"""
Run_Pytorch(
    dataset_path="data/structure/secondary_structure/casp12.csv",
    encoder_name="esm1b_t33_650M_UR50S",
    reset_param = False,
    resample_param = False,
    embed_batch_size = 128,
    flatten_emb= False,
    embed_folder=None,
    seq_start_idx= False,
    seq_end_idx = False,
    loader_batch_size = 64,
    worker_seed = RAND_SEED,
    if_encode_all = True,
    learning_rate = 1e-4,
    lr_decay = 0.1,
    epochs = 2,
    early_stop = True,
    tolerance = 10,
    min_epoch = 5,
    device = DEVICE,
    all_plot_folder = "test/learning_curves",
    all_result_folder = "test/train_val_test",
    # **encoder_params,
    )"""